[2018-04-23 20:38:20]  Get docker image uhub.service.ucloud.cn/ming_mirror/caffe_tinymind:res26
[2018-04-23 20:39:34]  Finish get docker image uhub.service.ucloud.cn/ming_mirror/caffe_tinymind:res26
[2018-04-23 20:39:34]  Downloading Files
[2018-04-23 20:39:36]  [Path]mxnet_mnist/, 4 files
[2018-04-23 20:39:36]  Total Data Size 11594722 Bytes
[2018-04-23 20:39:36]  Downloading mxnet_mnist/, total 4.
[2018-04-23 20:39:37]  Downloading files,finish 4/4
[2018-04-23 20:39:37]  mxnet_mnist/ download finish
[2018-04-23 20:39:38]  Data prepare finished
[2018-04-23 20:39:38]  Ready to start training job
[2018-04-23 20:39:39]  /bin/bash: tensorboard: command not found
[2018-04-23 20:39:41]  I0423 12:39:41.135025     8 _caffe.cpp:68] Using devices [0]
[2018-04-23 20:39:41]  False
[2018-04-23 20:39:43]  I0423 12:39:43.340909    21 solver.cpp:44] Initializing solver from parameters: 
[2018-04-23 20:39:43]  test_iter: 156
[2018-04-23 20:39:43]  test_interval: 100
[2018-04-23 20:39:43]  base_lr: 0.1
[2018-04-23 20:39:43]  display: 100
[2018-04-23 20:39:43]  max_iter: 10000
[2018-04-23 20:39:43]  lr_policy: "multistep"
[2018-04-23 20:39:43]  gamma: 0.1
[2018-04-23 20:39:43]  momentum: 0.9
[2018-04-23 20:39:43]  weight_decay: 0.0005
[2018-04-23 20:39:43]  snapshot: 2000
[2018-04-23 20:39:43]  snapshot_prefix: "/data/output/myres20_"
[2018-04-23 20:39:43]  solver_mode: GPU
[2018-04-23 20:39:43]  net: "/data/train_val_myres26.prototxt"
[2018-04-23 20:39:43]  stepvalue: 3000
[2018-04-23 20:39:43]  stepvalue: 5000
[2018-04-23 20:39:43]  stepvalue: 7000
[2018-04-23 20:39:43]  I0423 12:39:43.342144    21 solver.cpp:87] Creating training net from net file: /data/train_val_myres26.prototxt
[2018-04-23 20:39:43]  I0423 12:39:43.344147    21 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /data/train_val_myres26.prototxt
[2018-04-23 20:39:43]  I0423 12:39:43.344323    21 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
[2018-04-23 20:39:43]  I0423 12:39:43.345214    21 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
[2018-04-23 20:39:43]  I0423 12:39:43.345463    21 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Accuracy_test
[2018-04-23 20:39:43]  I0423 12:39:43.346603    21 net.cpp:51] Initializing net from parameters: 
[2018-04-23 20:39:43]  name: "resnet_26"
[2018-04-23 20:39:43]  state {
[2018-04-23 20:39:43]    phase: TRAIN
[2018-04-23 20:39:43]  }
[2018-04-23 20:39:43]  layer {
[2018-04-23 20:39:43]    name: "data"
[2018-04-23 20:39:43]    type: "ImageData"
[2018-04-23 20:39:43]    top: "data"
[2018-04-23 20:39:43]    top: "label"
[2018-04-23 20:39:43]    include {
[2018-04-23 20:39:43]      phase: TRAIN
[2018-04-23 20:39:43]    }
[2018-04-23 20:39:43]    transform_param {
[2018-04-23 20:39:43]      scale: 0.00390625
[2018-04-23 20:39:43]      mirror: false
[2018-04-23 20:39:43]    }
[2018-04-23 20:39:43]    image_data_param {
[2018-04-23 20:39:43]      source: "./mydata/train.txt"
[2018-04-23 20:39:43]      batch_size: 64
[2018-04-23 20:39:43]      shuffle: true
[2018-04-23 20:39:43]      new_height: 64
[2018-04-23 20:39:43]      new_width: 64
[2018-04-23 20:39:43]      root_folder: "/data/mydata/"
[2018-04-23 20:39:43]    }
[2018-04-23 20:39:43]  }
[2018-04-23 20:39:43]  layer {
[2018-04-23 20:39:43]    name: "Convolution1"
[2018-04-23 20:39:43]    type: "Convolution"
[2018-04-23 20:39:43]    bottom: "data"
[2018-04-23 20:39:43]    top: "Convolution1"
[2018-04-23 20:39:43]    param {
[2018-04-23 20:39:43]      lr_mult: 1
[2018-04-23 20:39:43]      decay_mult: 1
[2018-04-23 20:39:43]    }
[2018-04-23 20:39:43]    param {
[2018-04-23 20:39:43]      lr_mult: 2
[2018-04-23 20:39:44]      decay_mult: 0
[2018-04-23 20:39:44]    }
[2018-04-23 20:39:44]    convolution_param {
[2018-04-23 20:39:44]      num_output: 32
[2018-04-23 20:39:44]      pad: 2
[2018-04-23 20:39:44]      kernel_size: 5
[2018-04-23 20:39:44]      stride: 2
[2018-04-23 20:39:44]      weight_filler {
[2018-04-23 20:39:44]        type: "xavier"
[2018-04-23 20:39:44]      }
[2018-04-23 20:39:44]      bias_filler {
[2018-04-23 20:39:44]        type: "constant"
[2018-04-23 20:39:44]        value: 0
[2018-04-23 20:39:44]      }
[2018-04-23 20:39:44]    }
[2018-04-23 20:39:44]  }
[2018-04-23 20:39:44]  layer {
[2018-04-23 20:39:44]    name: "BatchNorm1"
[2018-04-23 20:39:44]    type: "BatchNorm"
[2018-04-23 20:39:44]    bottom: "Convolution1"
[2018-04-23 20:39:44]    top: "Convolution1"
[2018-04-23 20:39:44]    param {
[2018-04-23 20:39:44]      lr_mult: 0
[2018-04-23 20:39:44]      decay_mult: 0
[2018-04-23 20:39:44]    }
[2018-04-23 20:39:44]    param {
[2018-04-23 20:39:44]      lr_mult: 0
[2018-04-23 20:39:44]      decay_mult: 0
[2018-04-23 20:39:44]    }
[2018-04-23 20:39:44]    param {
[2018-04-23 20:39:44]      lr_mult: 0
[2018-04-23 20:39:44]      decay_mult: 0
[2018-04-23 20:39:44]    }
[2018-04-23 20:39:44]  }
[2018-04-23 20:39:44]  layer {
[2018-04-23 20:39:44]    name: "Scale1"
[2018-04-23 20:39:44]    type: "Scale"
[2018-04-23 20:39:44]    bottom: "Convolution1"
[2018-04-23 20:39:44]    top: "Convolution1"
[2018-04-23 20:39:44]    scale_param {
[2018-04-23 20:39:44]      bias_term: true
[2018-04-23 20:39:44]    }
[2018-04-23 20:39:44]  }
[2018-04-23 20:39:44]  layer {
[2018-04-23 20:39:44]    name: "ReLU1"
[2018-04-23 20:39:44]    type: "ReLU"
[2018-04-23 20:39:44]    bottom: "Convolution1"
[2018-04-23 20:39:44]    top: "Convolution1"
[2018-04-23 20:39:44]  }
[2018-04-23 20:39:44]  layer {
[2018-04-23 20:39:44]    name: "Convolution2"
[2018-04-23 20:39:44]    type: "Convolution"
[2018-04-23 20:39:44]    bottom: "Convolution1"
[2018-04-23 20:39:44]    top: "Convolution2"
[2018-04-23 20:39:44]    param {
[2018-04-23 20:39:44]      lr_mult: 1
[2018-04-23 20:39:44]      decay_mult: 1
[2018-04-23 20:39:44]    }
[2018-04-23 20:39:44]    param {
[2018-04-23 20:39:44]      lr_mult: 2
[2018-04-23 20:39:44]      decay_mult: 0
[2018-04-23 20:39:44]    }
[2018-04-23 20:39:44]    convolution_param {
[2018-04-23 20:39:44]      num_output: 32
[2018-04-23 20:39:44]      pad: 1
[2018-04-23 20:39:44]      kernel_size: 3
[2018-04-23 20:39:44]      stride: 1
[2018-04-23 20:39:44]      weight_filler {
[2018-04-23 20:39:44]        type: "xavier"
[2018-04-23 20:39:44]      }
[2018-04-23 20:39:44]      bias_filler {
[2018-04-23 20:39:44]        type: "constant"
[2018-04-23 20:39:44]        value: 0
[2018-04-23 20:39:44]      }
[2018-04-23 20:39:44]    }
[2018-04-23 20:39:44]  }
[2018-04-23 20:39:44]  layer {
[2018-04-23 20:39:44]    name: "BatchNorm2"
[2018-04-23 20:39:44]    type: "BatchNorm"
[2018-04-23 20:39:45]    bottom: "Convolution2"
[2018-04-23 20:39:45]    top: "Convolution2"
[2018-04-23 20:39:45]    param {
[2018-04-23 20:39:45]      lr_mult: 0
[2018-04-23 20:39:45]      decay_mult: 0
[2018-04-23 20:39:45]    }
[2018-04-23 20:39:45]    param {
[2018-04-23 20:39:45]      lr_mult: 0
[2018-04-23 20:39:45]      decay_mult: 0
[2018-04-23 20:39:45]    }
[2018-04-23 20:39:45]    param {
[2018-04-23 20:39:45]      lr_mult: 0
[2018-04-23 20:39:45]      decay_mult: 0
[2018-04-23 20:39:45]    }
[2018-04-23 20:39:45]  }
[2018-04-23 20:39:45]  layer {
[2018-04-23 20:39:45]    name: "Scale2"
[2018-04-23 20:39:45]    type: "Scale"
[2018-04-23 20:39:45]    bottom: "Convolution2"
[2018-04-23 20:39:45]    top: "Convolution2"
[2018-04-23 20:39:45]    scale_param {
[2018-04-23 20:39:45]      bias_term: true
[2018-04-23 20:39:45]    }
[2018-04-23 20:39:45]  }
[2018-04-23 20:39:45]  layer {
[2018-04-23 20:39:45]    name: "ReLU2"
[2018-04-23 20:39:45]    type: "ReLU"
[2018-04-23 20:39:45]    bottom: "Convolution2"
[2018-04-23 20:39:45]    top: "Convolution2"
[2018-04-23 20:39:45]  }
[2018-04-23 20:39:45]  layer {
[2018-04-23 20:39:45]    name: "Convolution3"
[2018-04-23 20:39:45]    type: "Convolution"
[2018-04-23 20:39:45]    bottom: "Convolution2"
[2018-04-23 20:39:45]    top: "Convolution3"
[2018-04-23 20:39:45]    param {
[2018-04-23 20:39:45]      lr_mult: 1
[2018-04-23 20:39:45]      decay_mult: 1
[2018-04-23 20:39:45]    }
[2018-04-23 20:39:45]    param {
[2018-04-23 20:39:45]      lr_mult: 2
[2018-04-23 20:39:45]      decay_mult: 0
[2018-04-23 20:39:45]    }
[2018-04-23 20:39:45]    convolution_param {
[2018-04-23 20:39:45]      num_output: 32
[2018-04-23 20:39:45]      pad: 1
[2018-04-23 20:39:45]      kernel_size: 3
[2018-04-23 20:39:45]      stride: 1
[2018-04-23 20:39:45]      weight_filler {
[2018-04-23 20:39:45]        type: "xavier"
[2018-04-23 20:39:45]      }
[2018-04-23 20:39:45]      bias_filler {
[2018-04-23 20:39:45]        type: "constant"
[2018-04-23 20:39:45]        value: 0
[2018-04-23 20:39:45]      }
[2018-04-23 20:39:45]    }
[2018-04-23 20:39:45]  }
[2018-04-23 20:39:45]  layer {
[2018-04-23 20:39:45]    name: "BatchNorm3"
[2018-04-23 20:39:45]    type: "BatchNorm"
[2018-04-23 20:39:45]    bottom: "Convolution3"
[2018-04-23 20:39:45]    top: "Convolution3"
[2018-04-23 20:39:45]    param {
[2018-04-23 20:39:45]      lr_mult: 0
[2018-04-23 20:39:45]      decay_mult: 0
[2018-04-23 20:39:45]    }
[2018-04-23 20:39:45]    param {
[2018-04-23 20:39:45]      lr_mult: 0
[2018-04-23 20:39:45]      decay_mult: 0
[2018-04-23 20:39:45]    }
[2018-04-23 20:39:45]    param {
[2018-04-23 20:39:45]      lr_mult: 0
[2018-04-23 20:39:45]      decay_mult: 0
[2018-04-23 20:39:45]    }
[2018-04-23 20:39:46]  }
[2018-04-23 20:39:46]  layer {
[2018-04-23 20:39:46]    name: "Scale3"
[2018-04-23 20:39:46]    type: "Scale"
[2018-04-23 20:39:46]    bottom: "Convolution3"
[2018-04-23 20:39:46]    top: "Convolution3"
[2018-04-23 20:39:46]    scale_param {
[2018-04-23 20:39:46]      bias_term: true
[2018-04-23 20:39:46]    }
[2018-04-23 20:39:46]  }
[2018-04-23 20:39:46]  layer {
[2018-04-23 20:39:46]    name: "Eltwise1"
[2018-04-23 20:39:46]    type: "Eltwise"
[2018-04-23 20:39:46]    bottom: "Convolution1"
[2018-04-23 20:39:46]    bottom: "Convolution3"
[2018-04-23 20:39:46]    top: "Eltwise1"
[2018-04-23 20:39:46]    eltwise_param {
[2018-04-23 20:39:46]      operation: SUM
[2018-04-23 20:39:46]    }
[2018-04-23 20:39:46]  }
[2018-04-23 20:39:46]  layer {
[2018-04-23 20:39:46]    name: "ReLU3"
[2018-04-23 20:39:46]    type: "ReLU"
[2018-04-23 20:39:46]    bottom: "Eltwise1"
[2018-04-23 20:39:46]    top: "Eltwise1"
[2018-04-23 20:39:46]  }
[2018-04-23 20:39:46]  layer {
[2018-04-23 20:39:46]    name: "Convolution4"
[2018-04-23 20:39:46]    type: "Convolution"
[2018-04-23 20:39:46]    bottom: "Eltwise1"
[2018-04-23 20:39:46]    top: "Convolution4"
[2018-04-23 20:39:46]    param {
[2018-04-23 20:39:46]      lr_mult: 1
[2018-04-23 20:39:46]      decay_mult: 1
[2018-04-23 20:39:46]    }
[2018-04-23 20:39:46]    param {
[2018-04-23 20:39:46]      lr_mult: 2
[2018-04-23 20:39:46]      decay_mult: 0
[2018-04-23 20:39:46]    }
[2018-04-23 20:39:46]    convolution_param {
[2018-04-23 20:39:46]      num_output: 32
[2018-04-23 20:39:46]      pad: 1
[2018-04-23 20:39:46]      kernel_size: 3
[2018-04-23 20:39:46]      stride: 1
[2018-04-23 20:39:46]      weight_filler {
[2018-04-23 20:39:46]        type: "xavier"
[2018-04-23 20:39:46]      }
[2018-04-23 20:39:46]      bias_filler {
[2018-04-23 20:39:46]        type: "constant"
[2018-04-23 20:39:46]        value: 0
[2018-04-23 20:39:46]      }
[2018-04-23 20:39:46]    }
[2018-04-23 20:39:46]  }
[2018-04-23 20:39:46]  layer {
[2018-04-23 20:39:46]    name: "BatchNorm4"
[2018-04-23 20:39:46]    type: "BatchNorm"
[2018-04-23 20:39:46]    bottom: "Convolution4"
[2018-04-23 20:39:46]    top: "Convolution4"
[2018-04-23 20:39:46]    param {
[2018-04-23 20:39:46]      lr_mult: 0
[2018-04-23 20:39:46]      decay_mult: 0
[2018-04-23 20:39:46]    }
[2018-04-23 20:39:46]    param {
[2018-04-23 20:39:46]      lr_mult: 0
[2018-04-23 20:39:46]      decay_mult: 0
[2018-04-23 20:39:46]    }
[2018-04-23 20:39:46]    param {
[2018-04-23 20:39:46]      lr_mult: 0
[2018-04-23 20:39:46]      decay_mult: 0
[2018-04-23 20:39:46]    }
[2018-04-23 20:39:46]  }
[2018-04-23 20:39:46]  layer {
[2018-04-23 20:39:46]    name: "Scale4"
[2018-04-23 20:39:46]    type: "Scale"
[2018-04-23 20:39:46]    bottom: "Convolution4"
[2018-04-23 20:39:46]    top: "Convolution4"
[2018-04-23 20:39:46]    scale_param {
[2018-04-23 20:39:47]      bias_term: true
[2018-04-23 20:39:47]    }
[2018-04-23 20:39:47]  }
[2018-04-23 20:39:47]  layer {
[2018-04-23 20:39:47]    name: "ReLU4"
[2018-04-23 20:39:47]    type: "ReLU"
[2018-04-23 20:39:47]    bottom: "Convolution4"
[2018-04-23 20:39:47]    top: "Convolution4"
[2018-04-23 20:39:47]  }
[2018-04-23 20:39:47]  layer {
[2018-04-23 20:39:47]    name: "Convolution5"
[2018-04-23 20:39:47]    type: "Convolution"
[2018-04-23 20:39:47]    bottom: "Convolution4"
[2018-04-23 20:39:47]    top: "Convolution5"
[2018-04-23 20:39:47]    param {
[2018-04-23 20:39:47]      lr_mult: 1
[2018-04-23 20:39:47]      decay_mult: 1
[2018-04-23 20:39:47]    }
[2018-04-23 20:39:47]    param {
[2018-04-23 20:39:47]      lr_mult: 2
[2018-04-23 20:39:47]      decay_mult: 0
[2018-04-23 20:39:47]    }
[2018-04-23 20:39:47]    convolution_param {
[2018-04-23 20:39:47]      num_output: 32
[2018-04-23 20:39:47]      pad: 1
[2018-04-23 20:39:47]      kernel_size: 3
[2018-04-23 20:39:47]      stride: 1
[2018-04-23 20:39:47]      weight_filler {
[2018-04-23 20:39:47]        type: "xavier"
[2018-04-23 20:39:47]      }
[2018-04-23 20:39:47]      bias_filler {
[2018-04-23 20:39:47]        type: "constant"
[2018-04-23 20:39:47]        value: 0
[2018-04-23 20:39:47]      }
[2018-04-23 20:39:47]    }
[2018-04-23 20:39:47]  }
[2018-04-23 20:39:47]  layer {
[2018-04-23 20:39:47]    name: "BatchNorm5"
[2018-04-23 20:39:47]    type: "BatchNorm"
[2018-04-23 20:39:47]    bottom: "Convolution5"
[2018-04-23 20:39:47]    top: "Convolution5"
[2018-04-23 20:39:47]    param {
[2018-04-23 20:39:47]      lr_mult: 0
[2018-04-23 20:39:47]      decay_mult: 0
[2018-04-23 20:39:47]    }
[2018-04-23 20:39:47]    param {
[2018-04-23 20:39:47]      lr_mult: 0
[2018-04-23 20:39:47]      decay_mult: 0
[2018-04-23 20:39:47]    }
[2018-04-23 20:39:47]    param {
[2018-04-23 20:39:47]      lr_mult: 0
[2018-04-23 20:39:47]      decay_mult: 0
[2018-04-23 20:39:47]    }
[2018-04-23 20:39:47]  }
[2018-04-23 20:39:47]  layer {
[2018-04-23 20:39:47]    name: "Scale5"
[2018-04-23 20:39:47]    type: "Scale"
[2018-04-23 20:39:47]    bottom: "Convolution5"
[2018-04-23 20:39:47]    top: "Convolution5"
[2018-04-23 20:39:47]    scale_param {
[2018-04-23 20:39:47]      bias_term: true
[2018-04-23 20:39:47]    }
[2018-04-23 20:39:47]  }
[2018-04-23 20:39:47]  layer {
[2018-04-23 20:39:47]    name: "Eltwise2"
[2018-04-23 20:39:47]    type: "Eltwise"
[2018-04-23 20:39:47]    bottom: "Eltwise1"
[2018-04-23 20:39:47]    bottom: "Convolution5"
[2018-04-23 20:39:47]    top: "Eltwise2"
[2018-04-23 20:39:47]    eltwise_param {
[2018-04-23 20:39:47]      operation: SUM
[2018-04-23 20:39:47]    }
[2018-04-23 20:39:47]  }
[2018-04-23 20:39:47]  layer {
[2018-04-23 20:39:47]    name: "ReLU5"
[2018-04-23 20:39:47]    type: "ReLU"
[2018-04-23 20:39:48]    bottom: "Eltwise2"
[2018-04-23 20:39:48]    top: "Eltwise2"
[2018-04-23 20:39:48]  }
[2018-04-23 20:39:48]  layer {
[2018-04-23 20:39:48]    name: "Convolution6"
[2018-04-23 20:39:48]    type: "Convolution"
[2018-04-23 20:39:48]    bottom: "Eltwise2"
[2018-04-23 20:39:48]    top: "Convolution6"
[2018-04-23 20:39:48]    param {
[2018-04-23 20:39:48]      lr_mult: 1
[2018-04-23 20:39:48]      decay_mult: 1
[2018-04-23 20:39:48]    }
[2018-04-23 20:39:48]    param {
[2018-04-23 20:39:48]      lr_mult: 2
[2018-04-23 20:39:48]      decay_mult: 0
[2018-04-23 20:39:48]    }
[2018-04-23 20:39:48]    convolution_param {
[2018-04-23 20:39:48]      num_output: 32
[2018-04-23 20:39:48]      pad: 1
[2018-04-23 20:39:48]      kernel_size: 3
[2018-04-23 20:39:48]      stride: 1
[2018-04-23 20:39:48]      weight_filler {
[2018-04-23 20:39:48]        type: "xavier"
[2018-04-23 20:39:48]      }
[2018-04-23 20:39:48]      bias_filler {
[2018-04-23 20:39:48]        type: "constant"
[2018-04-23 20:39:48]        value: 0
[2018-04-23 20:39:48]      }
[2018-04-23 20:39:48]    }
[2018-04-23 20:39:48]  }
[2018-04-23 20:39:48]  layer {
[2018-04-23 20:39:48]    name: "BatchNorm6"
[2018-04-23 20:39:48]    type: "BatchNorm"
[2018-04-23 20:39:48]    bottom: "Convolution6"
[2018-04-23 20:39:48]    top: "Convolution6"
[2018-04-23 20:39:48]    param {
[2018-04-23 20:39:48]      lr_mult: 0
[2018-04-23 20:39:48]      decay_mult: 0
[2018-04-23 20:39:48]    }
[2018-04-23 20:39:48]    param {
[2018-04-23 20:39:48]      lr_mult: 0
[2018-04-23 20:39:48]      decay_mult: 0
[2018-04-23 20:39:48]    }
[2018-04-23 20:39:48]    param {
[2018-04-23 20:39:48]      lr_mult: 0
[2018-04-23 20:39:48]      decay_mult: 0
[2018-04-23 20:39:48]    }
[2018-04-23 20:39:48]  }
[2018-04-23 20:39:48]  layer {
[2018-04-23 20:39:48]    name: "Scale6"
[2018-04-23 20:39:48]    type: "Scale"
[2018-04-23 20:39:48]    bottom: "Convolution6"
[2018-04-23 20:39:48]    top: "Convolution6"
[2018-04-23 20:39:48]    scale_param {
[2018-04-23 20:39:48]      bias_term: true
[2018-04-23 20:39:48]    }
[2018-04-23 20:39:48]  }
[2018-04-23 20:39:48]  layer {
[2018-04-23 20:39:48]    name: "ReLU6"
[2018-04-23 20:39:48]    type: "ReLU"
[2018-04-23 20:39:48]    bottom: "Convolution6"
[2018-04-23 20:39:48]    top: "Convolution6"
[2018-04-23 20:39:48]  }
[2018-04-23 20:39:48]  layer {
[2018-04-23 20:39:48]    name: "Convolution7"
[2018-04-23 20:39:48]    type: "Convolution"
[2018-04-23 20:39:48]    bottom: "Convolution6"
[2018-04-23 20:39:48]    top: "Convolution7"
[2018-04-23 20:39:48]    param {
[2018-04-23 20:39:48]      lr_mult: 1
[2018-04-23 20:39:48]      decay_mult: 1
[2018-04-23 20:39:48]    }
[2018-04-23 20:39:48]    param {
[2018-04-23 20:39:48]      lr_mult: 2
[2018-04-23 20:39:48]      decay_mult: 0
[2018-04-23 20:39:48]    }
[2018-04-23 20:39:48]    convolution_param {
[2018-04-23 20:39:48]      num_output: 32
[2018-04-23 20:39:48]      pad: 1
[2018-04-23 20:39:48]      kernel_size: 3
[2018-04-23 20:39:49]      stride: 1
[2018-04-23 20:39:49]      weight_filler {
[2018-04-23 20:39:49]        type: "xavier"
[2018-04-23 20:39:49]      }
[2018-04-23 20:39:49]      bias_filler {
[2018-04-23 20:39:49]        type: "constant"
[2018-04-23 20:39:49]        value: 0
[2018-04-23 20:39:49]      }
[2018-04-23 20:39:49]    }
[2018-04-23 20:39:49]  }
[2018-04-23 20:39:49]  layer {
[2018-04-23 20:39:49]    name: "BatchNorm7"
[2018-04-23 20:39:49]    type: "BatchNorm"
[2018-04-23 20:39:49]    bottom: "Convolution7"
[2018-04-23 20:39:49]    top: "Convolution7"
[2018-04-23 20:39:49]    param {
[2018-04-23 20:39:49]      lr_mult: 0
[2018-04-23 20:39:49]      decay_mult: 0
[2018-04-23 20:39:49]    }
[2018-04-23 20:39:49]    param {
[2018-04-23 20:39:49]      lr_mult: 0
[2018-04-23 20:39:49]      decay_mult: 0
[2018-04-23 20:39:49]    }
[2018-04-23 20:39:49]    param {
[2018-04-23 20:39:49]      lr_mult: 0
[2018-04-23 20:39:49]      decay_mult: 0
[2018-04-23 20:39:49]    }
[2018-04-23 20:39:49]  }
[2018-04-23 20:39:49]  layer {
[2018-04-23 20:39:49]    name: "Scale7"
[2018-04-23 20:39:49]    type: "Scale"
[2018-04-23 20:39:49]    bottom: "Convolution7"
[2018-04-23 20:39:49]    top: "Convolution7"
[2018-04-23 20:39:49]    scale_param {
[2018-04-23 20:39:49]      bias_term: true
[2018-04-23 20:39:49]    }
[2018-04-23 20:39:49]  }
[2018-04-23 20:39:49]  layer {
[2018-04-23 20:39:49]    name: "Eltwise3"
[2018-04-23 20:39:49]    type: "Eltwise"
[2018-04-23 20:39:49]    bottom: "Eltwise2"
[2018-04-23 20:39:49]    bottom: "Convolution7"
[2018-04-23 20:39:49]    top: "Eltwise3"
[2018-04-23 20:39:49]    eltwise_param {
[2018-04-23 20:39:49]      operation: SUM
[2018-04-23 20:39:49]    }
[2018-04-23 20:39:49]  }
[2018-04-23 20:39:49]  layer {
[2018-04-23 20:39:49]    name: "ReLU7"
[2018-04-23 20:39:49]    type: "ReLU"
[2018-04-23 20:39:49]    bottom: "Eltwise3"
[2018-04-23 20:39:49]    top: "Eltwise3"
[2018-04-23 20:39:49]  }
[2018-04-23 20:39:49]  layer {
[2018-04-23 20:39:49]    name: "Convolution8"
[2018-04-23 20:39:49]    type: "Convolution"
[2018-04-23 20:39:49]    bottom: "Eltwise3"
[2018-04-23 20:39:49]    top: "Convolution8"
[2018-04-23 20:39:49]    param {
[2018-04-23 20:39:49]      lr_mult: 1
[2018-04-23 20:39:49]      decay_mult: 1
[2018-04-23 20:39:49]    }
[2018-04-23 20:39:49]    param {
[2018-04-23 20:39:49]      lr_mult: 2
[2018-04-23 20:39:49]      decay_mult: 0
[2018-04-23 20:39:49]    }
[2018-04-23 20:39:49]    convolution_param {
[2018-04-23 20:39:49]      num_output: 64
[2018-04-23 20:39:49]      pad: 0
[2018-04-23 20:39:50]      kernel_size: 1
[2018-04-23 20:39:50]      stride: 2
[2018-04-23 20:39:50]      weight_filler {
[2018-04-23 20:39:50]        type: "xavier"
[2018-04-23 20:39:50]      }
[2018-04-23 20:39:50]      bias_filler {
[2018-04-23 20:39:50]        type: "constant"
[2018-04-23 20:39:50]        value: 0
[2018-04-23 20:39:50]      }
[2018-04-23 20:39:50]    }
[2018-04-23 20:39:50]  }
[2018-04-23 20:39:50]  layer {
[2018-04-23 20:39:50]    name: "BatchNorm8"
[2018-04-23 20:39:50]    type: "BatchNorm"
[2018-04-23 20:39:50]    bottom: "Convolution8"
[2018-04-23 20:39:50]    top: "Convolution8"
[2018-04-23 20:39:50]    param {
[2018-04-23 20:39:50]      lr_mult: 0
[2018-04-23 20:39:50]      decay_mult: 0
[2018-04-23 20:39:50]    }
[2018-04-23 20:39:50]    param {
[2018-04-23 20:39:50]      lr_mult: 0
[2018-04-23 20:39:50]      decay_mult: 0
[2018-04-23 20:39:50]    }
[2018-04-23 20:39:50]    param {
[2018-04-23 20:39:50]      lr_mult: 0
[2018-04-23 20:39:50]      decay_mult: 0
[2018-04-23 20:39:50]    }
[2018-04-23 20:39:50]  }
[2018-04-23 20:39:50]  layer {
[2018-04-23 20:39:50]    name: "Scale8"
[2018-04-23 20:39:50]    type: "Scale"
[2018-04-23 20:39:50]    bottom: "Convolution8"
[2018-04-23 20:39:50]    top: "Convolution8"
[2018-04-23 20:39:50]    scale_param {
[2018-04-23 20:39:50]      bias_term: true
[2018-04-23 20:39:50]    }
[2018-04-23 20:39:50]  }
[2018-04-23 20:39:50]  layer {
[2018-04-23 20:39:50]    name: "Convolution9"
[2018-04-23 20:39:50]    type: "Convolution"
[2018-04-23 20:39:50]    bottom: "Eltwise3"
[2018-04-23 20:39:50]    top: "Convolution9"
[2018-04-23 20:39:50]    param {
[2018-04-23 20:39:50]      lr_mult: 1
[2018-04-23 20:39:50]      decay_mult: 1
[2018-04-23 20:39:50]    }
[2018-04-23 20:39:50]    param {
[2018-04-23 20:39:50]      lr_mult: 2
[2018-04-23 20:39:50]      decay_mult: 0
[2018-04-23 20:39:50]    }
[2018-04-23 20:39:50]    convolution_param {
[2018-04-23 20:39:50]      num_output: 64
[2018-04-23 20:39:50]      pad: 1
[2018-04-23 20:39:50]      kernel_size: 3
[2018-04-23 20:39:50]      stride: 2
[2018-04-23 20:39:50]      weight_filler {
[2018-04-23 20:39:50]        type: "xavier"
[2018-04-23 20:39:50]      }
[2018-04-23 20:39:50]      bias_filler {
[2018-04-23 20:39:50]        type: "constant"
[2018-04-23 20:39:50]        value: 0
[2018-04-23 20:39:50]      }
[2018-04-23 20:39:50]    }
[2018-04-23 20:39:50]  }
[2018-04-23 20:39:50]  layer {
[2018-04-23 20:39:50]    name: "BatchNorm9"
[2018-04-23 20:39:50]    type: "BatchNorm"
[2018-04-23 20:39:50]    bottom: "Convolution9"
[2018-04-23 20:39:50]    top: "Convolution9"
[2018-04-23 20:39:50]    param {
[2018-04-23 20:39:50]      lr_mult: 0
[2018-04-23 20:39:51]      decay_mult: 0
[2018-04-23 20:39:51]    }
[2018-04-23 20:39:51]    param {
[2018-04-23 20:39:51]      lr_mult: 0
[2018-04-23 20:39:51]      decay_mult: 0
[2018-04-23 20:39:51]    }
[2018-04-23 20:39:51]    param {
[2018-04-23 20:39:51]      lr_mult: 0
[2018-04-23 20:39:51]      decay_mult: 0
[2018-04-23 20:39:51]    }
[2018-04-23 20:39:51]  }
[2018-04-23 20:39:51]  layer {
[2018-04-23 20:39:51]    name: "Scale9"
[2018-04-23 20:39:51]    type: "Scale"
[2018-04-23 20:39:51]    bottom: "Convolution9"
[2018-04-23 20:39:51]    top: "Convolution9"
[2018-04-23 20:39:51]    scale_param {
[2018-04-23 20:39:51]      bias_term: true
[2018-04-23 20:39:51]    }
[2018-04-23 20:39:51]  }
[2018-04-23 20:39:51]  layer {
[2018-04-23 20:39:51]    name: "ReLU8"
[2018-04-23 20:39:51]    type: "ReLU"
[2018-04-23 20:39:51]    bottom: "Convolution9"
[2018-04-23 20:39:51]    top: "Convolution9"
[2018-04-23 20:39:51]  }
[2018-04-23 20:39:51]  layer {
[2018-04-23 20:39:51]    name: "Convolution10"
[2018-04-23 20:39:51]    type: "Convolution"
[2018-04-23 20:39:51]    bottom: "Convolution9"
[2018-04-23 20:39:51]    top: "Convolution10"
[2018-04-23 20:39:51]    param {
[2018-04-23 20:39:51]      lr_mult: 1
[2018-04-23 20:39:51]      decay_mult: 1
[2018-04-23 20:39:51]    }
[2018-04-23 20:39:51]    param {
[2018-04-23 20:39:51]      lr_mult: 2
[2018-04-23 20:39:51]      decay_mult: 0
[2018-04-23 20:39:51]    }
[2018-04-23 20:39:51]    convolution_param {
[2018-04-23 20:39:51]      num_output: 64
[2018-04-23 20:39:51]      pad: 1
[2018-04-23 20:39:51]      kernel_size: 3
[2018-04-23 20:39:51]      stride: 1
[2018-04-23 20:39:51]      weight_filler {
[2018-04-23 20:39:51]        type: "xavier"
[2018-04-23 20:39:51]      }
[2018-04-23 20:39:51]      bias_filler {
[2018-04-23 20:39:51]        type: "constant"
[2018-04-23 20:39:51]        value: 0
[2018-04-23 20:39:51]      }
[2018-04-23 20:39:51]    }
[2018-04-23 20:39:51]  }
[2018-04-23 20:39:51]  layer {
[2018-04-23 20:39:51]    name: "BatchNorm10"
[2018-04-23 20:39:51]    type: "BatchNorm"
[2018-04-23 20:39:51]    bottom: "Convolution10"
[2018-04-23 20:39:51]    top: "Convolution10"
[2018-04-23 20:39:51]    param {
[2018-04-23 20:39:51]      lr_mult: 0
[2018-04-23 20:39:51]      decay_mult: 0
[2018-04-23 20:39:51]    }
[2018-04-23 20:39:51]    param {
[2018-04-23 20:39:51]      lr_mult: 0
[2018-04-23 20:39:51]      decay_mult: 0
[2018-04-23 20:39:51]    }
[2018-04-23 20:39:51]    param {
[2018-04-23 20:39:51]      lr_mult: 0
[2018-04-23 20:39:51]      decay_mult: 0
[2018-04-23 20:39:51]    }
[2018-04-23 20:39:51]  }
[2018-04-23 20:39:51]  layer {
[2018-04-23 20:39:51]    name: "Scale10"
[2018-04-23 20:39:51]    type: "Scale"
[2018-04-23 20:39:51]    bottom: "Convolution10"
[2018-04-23 20:39:51]    top: "Convolution10"
[2018-04-23 20:39:52]    scale_param {
[2018-04-23 20:39:52]      bias_term: true
[2018-04-23 20:39:52]    }
[2018-04-23 20:39:52]  }
[2018-04-23 20:39:52]  layer {
[2018-04-23 20:39:52]    name: "Eltwise4"
[2018-04-23 20:39:52]    type: "Eltwise"
[2018-04-23 20:39:52]    bottom: "Convolution8"
[2018-04-23 20:39:52]    bottom: "Convolution10"
[2018-04-23 20:39:52]    top: "Eltwise4"
[2018-04-23 20:39:52]    eltwise_param {
[2018-04-23 20:39:52]      operation: SUM
[2018-04-23 20:39:52]    }
[2018-04-23 20:39:52]  }
[2018-04-23 20:39:52]  layer {
[2018-04-23 20:39:52]    name: "ReLU9"
[2018-04-23 20:39:52]    type: "ReLU"
[2018-04-23 20:39:52]    bottom: "Eltwise4"
[2018-04-23 20:39:52]    top: "Eltwise4"
[2018-04-23 20:39:52]  }
[2018-04-23 20:39:52]  layer {
[2018-04-23 20:39:52]    name: "Convolution11"
[2018-04-23 20:39:52]    type: "Convolution"
[2018-04-23 20:39:52]    bottom: "Eltwise4"
[2018-04-23 20:39:52]    top: "Convolution11"
[2018-04-23 20:39:52]    param {
[2018-04-23 20:39:52]      lr_mult: 1
[2018-04-23 20:39:52]      decay_mult: 1
[2018-04-23 20:39:52]    }
[2018-04-23 20:39:52]    param {
[2018-04-23 20:39:52]      lr_mult: 2
[2018-04-23 20:39:52]      decay_mult: 0
[2018-04-23 20:39:52]    }
[2018-04-23 20:39:52]    convolution_param {
[2018-04-23 20:39:52]      num_output: 64
[2018-04-23 20:39:52]      pad: 1
[2018-04-23 20:39:52]      kernel_size: 3
[2018-04-23 20:39:52]      stride: 1
[2018-04-23 20:39:52]      weight_filler {
[2018-04-23 20:39:52]        type: "xavier"
[2018-04-23 20:39:52]      }
[2018-04-23 20:39:52]      bias_filler {
[2018-04-23 20:39:52]        type: "constant"
[2018-04-23 20:39:52]        value: 0
[2018-04-23 20:39:52]      }
[2018-04-23 20:39:52]    }
[2018-04-23 20:39:52]  }
[2018-04-23 20:39:52]  layer {
[2018-04-23 20:39:52]    name: "BatchNorm11"
[2018-04-23 20:39:52]    type: "BatchNorm"
[2018-04-23 20:39:52]    bottom: "Convolution11"
[2018-04-23 20:39:52]    top: "Convolution11"
[2018-04-23 20:39:52]    param {
[2018-04-23 20:39:52]      lr_mult: 0
[2018-04-23 20:39:52]      decay_mult: 0
[2018-04-23 20:39:52]    }
[2018-04-23 20:39:52]    param {
[2018-04-23 20:39:52]      lr_mult: 0
[2018-04-23 20:39:52]      decay_mult: 0
[2018-04-23 20:39:52]    }
[2018-04-23 20:39:52]    param {
[2018-04-23 20:39:52]      lr_mult: 0
[2018-04-23 20:39:52]      decay_mult: 0
[2018-04-23 20:39:52]    }
[2018-04-23 20:39:52]  }
[2018-04-23 20:39:52]  layer {
[2018-04-23 20:39:52]    name: "Scale11"
[2018-04-23 20:39:52]    type: "Scale"
[2018-04-23 20:39:52]    bottom: "Convolution11"
[2018-04-23 20:39:52]    top: "Convolution11"
[2018-04-23 20:39:52]    scale_param {
[2018-04-23 20:39:52]      bias_term: true
[2018-04-23 20:39:52]    }
[2018-04-23 20:39:52]  }
[2018-04-23 20:39:53]  layer {
[2018-04-23 20:39:53]    name: "ReLU10"
[2018-04-23 20:39:53]    type: "ReLU"
[2018-04-23 20:39:53]    bottom: "Convolution11"
[2018-04-23 20:39:53]    top: "Convolution11"
[2018-04-23 20:39:53]  }
[2018-04-23 20:39:53]  layer {
[2018-04-23 20:39:53]    name: "Convolution12"
[2018-04-23 20:39:53]    type: "Convolution"
[2018-04-23 20:39:53]    bottom: "Convolution11"
[2018-04-23 20:39:53]    top: "Convolution12"
[2018-04-23 20:39:53]    param {
[2018-04-23 20:39:53]      lr_mult: 1
[2018-04-23 20:39:53]      decay_mult: 1
[2018-04-23 20:39:53]    }
[2018-04-23 20:39:53]    param {
[2018-04-23 20:39:53]      lr_mult: 2
[2018-04-23 20:39:53]      decay_mult: 0
[2018-04-23 20:39:53]    }
[2018-04-23 20:39:53]    convolution_param {
[2018-04-23 20:39:53]      num_output: 64
[2018-04-23 20:39:53]      pad: 1
[2018-04-23 20:39:53]      kernel_size: 3
[2018-04-23 20:39:53]      stride: 1
[2018-04-23 20:39:53]      weight_filler {
[2018-04-23 20:39:53]        type: "xavier"
[2018-04-23 20:39:53]      }
[2018-04-23 20:39:53]      bias_filler {
[2018-04-23 20:39:53]        type: "constant"
[2018-04-23 20:39:53]        value: 0
[2018-04-23 20:39:53]      }
[2018-04-23 20:39:53]    }
[2018-04-23 20:39:53]  }
[2018-04-23 20:39:53]  layer {
[2018-04-23 20:39:53]    name: "BatchNorm12"
[2018-04-23 20:39:53]    type: "BatchNorm"
[2018-04-23 20:39:53]    bottom: "Convolution12"
[2018-04-23 20:39:53]    top: "Convolution12"
[2018-04-23 20:39:53]    param {
[2018-04-23 20:39:53]      lr_mult: 0
[2018-04-23 20:39:53]      decay_mult: 0
[2018-04-23 20:39:53]    }
[2018-04-23 20:39:53]    param {
[2018-04-23 20:39:53]      lr_mult: 0
[2018-04-23 20:39:53]      decay_mult: 0
[2018-04-23 20:39:53]    }
[2018-04-23 20:39:53]    param {
[2018-04-23 20:39:53]      lr_mult: 0
[2018-04-23 20:39:53]      decay_mult: 0
[2018-04-23 20:39:53]    }
[2018-04-23 20:39:53]  }
[2018-04-23 20:39:53]  layer {
[2018-04-23 20:39:53]    name: "Scale12"
[2018-04-23 20:39:53]    type: "Scale"
[2018-04-23 20:39:53]    bottom: "Convolution12"
[2018-04-23 20:39:53]    top: "Convolution12"
[2018-04-23 20:39:53]    scale_param {
[2018-04-23 20:39:53]      bias_term: true
[2018-04-23 20:39:53]    }
[2018-04-23 20:39:53]  }
[2018-04-23 20:39:53]  layer {
[2018-04-23 20:39:53]    name: "Eltwise5"
[2018-04-23 20:39:53]    type: "Eltwise"
[2018-04-23 20:39:53]    bottom: "Eltwise4"
[2018-04-23 20:39:53]    bottom: "Convolution12"
[2018-04-23 20:39:54]    top: "Eltwise5"
[2018-04-23 20:39:54]    eltwise_param {
[2018-04-23 20:39:54]      operation: SUM
[2018-04-23 20:39:54]    }
[2018-04-23 20:39:54]  }
[2018-04-23 20:39:54]  layer {
[2018-04-23 20:39:54]    name: "ReLU11"
[2018-04-23 20:39:54]    type: "ReLU"
[2018-04-23 20:39:54]    bottom: "Eltwise5"
[2018-04-23 20:39:54]    top: "Eltwise5"
[2018-04-23 20:39:54]  }
[2018-04-23 20:39:54]  layer {
[2018-04-23 20:39:54]    name: "Convolution13"
[2018-04-23 20:39:54]    type: "Convolution"
[2018-04-23 20:39:54]    bottom: "Eltwise5"
[2018-04-23 20:39:54]    top: "Convolution13"
[2018-04-23 20:39:54]    param {
[2018-04-23 20:39:54]      lr_mult: 1
[2018-04-23 20:39:54]      decay_mult: 1
[2018-04-23 20:39:54]    }
[2018-04-23 20:39:54]    param {
[2018-04-23 20:39:54]      lr_mult: 2
[2018-04-23 20:39:54]      decay_mult: 0
[2018-04-23 20:39:54]    }
[2018-04-23 20:39:54]    convolution_param {
[2018-04-23 20:39:54]      num_output: 64
[2018-04-23 20:39:54]      pad: 1
[2018-04-23 20:39:54]      kernel_size: 3
[2018-04-23 20:39:54]      stride: 1
[2018-04-23 20:39:54]      weight_filler {
[2018-04-23 20:39:54]        type: "xavier"
[2018-04-23 20:39:54]      }
[2018-04-23 20:39:54]      bias_filler {
[2018-04-23 20:39:54]        type: "constant"
[2018-04-23 20:39:54]        value: 0
[2018-04-23 20:39:54]      }
[2018-04-23 20:39:54]    }
[2018-04-23 20:39:54]  }
[2018-04-23 20:39:54]  layer {
[2018-04-23 20:39:54]    name: "BatchNorm13"
[2018-04-23 20:39:54]    type: "BatchNorm"
[2018-04-23 20:39:54]    bottom: "Convolution13"
[2018-04-23 20:39:54]    top: "Convolution13"
[2018-04-23 20:39:54]    param {
[2018-04-23 20:39:54]      lr_mult: 0
[2018-04-23 20:39:54]      decay_mult: 0
[2018-04-23 20:39:54]    }
[2018-04-23 20:39:54]    param {
[2018-04-23 20:39:54]      lr_mult: 0
[2018-04-23 20:39:54]      decay_mult: 0
[2018-04-23 20:39:54]    }
[2018-04-23 20:39:54]    param {
[2018-04-23 20:39:54]      lr_mult: 0
[2018-04-23 20:39:54]      decay_mult: 0
[2018-04-23 20:39:54]    }
[2018-04-23 20:39:54]  }
[2018-04-23 20:39:54]  layer {
[2018-04-23 20:39:54]    name: "Scale13"
[2018-04-23 20:39:54]    type: "Scale"
[2018-04-23 20:39:54]    bottom: "Convolution13"
[2018-04-23 20:39:54]    top: "Convolution13"
[2018-04-23 20:39:54]    scale_param {
[2018-04-23 20:39:54]      bias_term: true
[2018-04-23 20:39:54]    }
[2018-04-23 20:39:54]  }
[2018-04-23 20:39:54]  layer {
[2018-04-23 20:39:54]    name: "ReLU12"
[2018-04-23 20:39:54]    type: "ReLU"
[2018-04-23 20:39:54]    bottom: "Convolution13"
[2018-04-23 20:39:54]    top: "Convolution13"
[2018-04-23 20:39:54]  }
[2018-04-23 20:39:54]  layer {
[2018-04-23 20:39:54]    name: "Convolution14"
[2018-04-23 20:39:54]    type: "Convolution"
[2018-04-23 20:39:54]    bottom: "Convolution13"
[2018-04-23 20:39:54]    top: "Convolution14"
[2018-04-23 20:39:55]    param {
[2018-04-23 20:39:55]      lr_mult: 1
[2018-04-23 20:39:55]      decay_mult: 1
[2018-04-23 20:39:55]    }
[2018-04-23 20:39:55]    param {
[2018-04-23 20:39:55]      lr_mult: 2
[2018-04-23 20:39:55]      decay_mult: 0
[2018-04-23 20:39:55]    }
[2018-04-23 20:39:55]    convolution_param {
[2018-04-23 20:39:55]      num_output: 64
[2018-04-23 20:39:55]      pad: 1
[2018-04-23 20:39:55]      kernel_size: 3
[2018-04-23 20:39:55]      stride: 1
[2018-04-23 20:39:55]      weight_filler {
[2018-04-23 20:39:55]        type: "xavier"
[2018-04-23 20:39:55]      }
[2018-04-23 20:39:55]      bias_filler {
[2018-04-23 20:39:55]        type: "constant"
[2018-04-23 20:39:55]        value: 0
[2018-04-23 20:39:55]      }
[2018-04-23 20:39:55]    }
[2018-04-23 20:39:55]  }
[2018-04-23 20:39:55]  layer {
[2018-04-23 20:39:55]    name: "BatchNorm14"
[2018-04-23 20:39:55]    type: "BatchNorm"
[2018-04-23 20:39:55]    bottom: "Convolution14"
[2018-04-23 20:39:55]    top: "Convolution14"
[2018-04-23 20:39:55]    param {
[2018-04-23 20:39:55]      lr_mult: 0
[2018-04-23 20:39:55]      decay_mult: 0
[2018-04-23 20:39:55]    }
[2018-04-23 20:39:55]    param {
[2018-04-23 20:39:55]      lr_mult: 0
[2018-04-23 20:39:55]      decay_mult: 0
[2018-04-23 20:39:55]    }
[2018-04-23 20:39:55]    param {
[2018-04-23 20:39:55]      lr_mult: 0
[2018-04-23 20:39:55]      decay_mult: 0
[2018-04-23 20:39:55]    }
[2018-04-23 20:39:55]  }
[2018-04-23 20:39:55]  layer {
[2018-04-23 20:39:55]    name: "Scale14"
[2018-04-23 20:39:55]    type: "Scale"
[2018-04-23 20:39:55]    bottom: "Convolution14"
[2018-04-23 20:39:55]    top: "Convolution14"
[2018-04-23 20:39:55]    scale_param {
[2018-04-23 20:39:55]      bias_term: true
[2018-04-23 20:39:55]    }
[2018-04-23 20:39:55]  }
[2018-04-23 20:39:55]  layer {
[2018-04-23 20:39:55]    name: "Eltwise6"
[2018-04-23 20:39:55]    type: "Eltwise"
[2018-04-23 20:39:55]    bottom: "Eltwise5"
[2018-04-23 20:39:55]    bottom: "Convolution14"
[2018-04-23 20:39:55]    top: "Eltwise6"
[2018-04-23 20:39:55]    eltwise_param {
[2018-04-23 20:39:55]      operation: SUM
[2018-04-23 20:39:55]    }
[2018-04-23 20:39:55]  }
[2018-04-23 20:39:55]  layer {
[2018-04-23 20:39:55]    name: "ReLU13"
[2018-04-23 20:39:55]    type: "ReLU"
[2018-04-23 20:39:55]    bottom: "Eltwise6"
[2018-04-23 20:39:55]    top: "Eltwise6"
[2018-04-23 20:39:55]  }
[2018-04-23 20:39:55]  layer {
[2018-04-23 20:39:55]    name: "Convolution15"
[2018-04-23 20:39:55]    type: "Convolution"
[2018-04-23 20:39:55]    bottom: "Eltwise6"
[2018-04-23 20:39:55]    top: "Convolution15"
[2018-04-23 20:39:55]    param {
[2018-04-23 20:39:55]      lr_mult: 1
[2018-04-23 20:39:55]      decay_mult: 1
[2018-04-23 20:39:56]    }
[2018-04-23 20:39:56]    param {
[2018-04-23 20:39:56]      lr_mult: 2
[2018-04-23 20:39:56]      decay_mult: 0
[2018-04-23 20:39:56]    }
[2018-04-23 20:39:56]    convolution_param {
[2018-04-23 20:39:56]      num_output: 128
[2018-04-23 20:39:56]      pad: 0
[2018-04-23 20:39:56]      kernel_size: 1
[2018-04-23 20:39:56]      stride: 2
[2018-04-23 20:39:56]      weight_filler {
[2018-04-23 20:39:56]        type: "xavier"
[2018-04-23 20:39:56]      }
[2018-04-23 20:39:56]      bias_filler {
[2018-04-23 20:39:56]        type: "constant"
[2018-04-23 20:39:56]        value: 0
[2018-04-23 20:39:56]      }
[2018-04-23 20:39:56]    }
[2018-04-23 20:39:56]  }
[2018-04-23 20:39:56]  layer {
[2018-04-23 20:39:56]    name: "BatchNorm15"
[2018-04-23 20:39:56]    type: "BatchNorm"
[2018-04-23 20:39:56]    bottom: "Convolution15"
[2018-04-23 20:39:56]    top: "Convolution15"
[2018-04-23 20:39:56]    param {
[2018-04-23 20:39:56]      lr_mult: 0
[2018-04-23 20:39:56]      decay_mult: 0
[2018-04-23 20:39:56]    }
[2018-04-23 20:39:56]    param {
[2018-04-23 20:39:56]      lr_mult: 0
[2018-04-23 20:39:56]      decay_mult: 0
[2018-04-23 20:39:56]    }
[2018-04-23 20:39:56]    param {
[2018-04-23 20:39:56]      lr_mult: 0
[2018-04-23 20:39:56]      decay_mult: 0
[2018-04-23 20:39:56]    }
[2018-04-23 20:39:56]  }
[2018-04-23 20:39:56]  layer {
[2018-04-23 20:39:56]    name: "Scale15"
[2018-04-23 20:39:56]    type: "Scale"
[2018-04-23 20:39:56]    bottom: "Convolution15"
[2018-04-23 20:39:56]    top: "Convolution15"
[2018-04-23 20:39:56]    scale_param {
[2018-04-23 20:39:56]      bias_term: true
[2018-04-23 20:39:56]    }
[2018-04-23 20:39:56]  }
[2018-04-23 20:39:56]  layer {
[2018-04-23 20:39:56]    name: "Convolution16"
[2018-04-23 20:39:56]    type: "Convolution"
[2018-04-23 20:39:56]    bottom: "Eltwise6"
[2018-04-23 20:39:56]    top: "Convolution16"
[2018-04-23 20:39:56]    param {
[2018-04-23 20:39:56]      lr_mult: 1
[2018-04-23 20:39:56]      decay_mult: 1
[2018-04-23 20:39:56]    }
[2018-04-23 20:39:56]    param {
[2018-04-23 20:39:56]      lr_mult: 2
[2018-04-23 20:39:56]      decay_mult: 0
[2018-04-23 20:39:56]    }
[2018-04-23 20:39:56]    convolution_param {
[2018-04-23 20:39:56]      num_output: 128
[2018-04-23 20:39:56]      pad: 1
[2018-04-23 20:39:56]      kernel_size: 3
[2018-04-23 20:39:56]      stride: 2
[2018-04-23 20:39:56]      weight_filler {
[2018-04-23 20:39:56]        type: "xavier"
[2018-04-23 20:39:56]      }
[2018-04-23 20:39:56]      bias_filler {
[2018-04-23 20:39:56]        type: "constant"
[2018-04-23 20:39:56]        value: 0
[2018-04-23 20:39:56]      }
[2018-04-23 20:39:56]    }
[2018-04-23 20:39:56]  }
[2018-04-23 20:39:57]  layer {
[2018-04-23 20:39:57]    name: "BatchNorm16"
[2018-04-23 20:39:57]    type: "BatchNorm"
[2018-04-23 20:39:57]    bottom: "Convolution16"
[2018-04-23 20:39:57]    top: "Convolution16"
[2018-04-23 20:39:57]    param {
[2018-04-23 20:39:57]      lr_mult: 0
[2018-04-23 20:39:57]      decay_mult: 0
[2018-04-23 20:39:57]    }
[2018-04-23 20:39:57]    param {
[2018-04-23 20:39:57]      lr_mult: 0
[2018-04-23 20:39:57]      decay_mult: 0
[2018-04-23 20:39:57]    }
[2018-04-23 20:39:57]    param {
[2018-04-23 20:39:57]      lr_mult: 0
[2018-04-23 20:39:57]      decay_mult: 0
[2018-04-23 20:39:57]    }
[2018-04-23 20:39:57]  }
[2018-04-23 20:39:57]  layer {
[2018-04-23 20:39:57]    name: "Scale16"
[2018-04-23 20:39:57]    type: "Scale"
[2018-04-23 20:39:57]    bottom: "Convolution16"
[2018-04-23 20:39:57]    top: "Convolution16"
[2018-04-23 20:39:57]    scale_param {
[2018-04-23 20:39:57]      bias_term: true
[2018-04-23 20:39:57]    }
[2018-04-23 20:39:57]  }
[2018-04-23 20:39:57]  layer {
[2018-04-23 20:39:57]    name: "ReLU14"
[2018-04-23 20:39:57]    type: "ReLU"
[2018-04-23 20:39:57]    bottom: "Convolution16"
[2018-04-23 20:39:57]    top: "Convolution16"
[2018-04-23 20:39:57]  }
[2018-04-23 20:39:57]  layer {
[2018-04-23 20:39:57]    name: "Convolution17"
[2018-04-23 20:39:57]    type: "Convolution"
[2018-04-23 20:39:57]    bottom: "Convolution16"
[2018-04-23 20:39:57]    top: "Convolution17"
[2018-04-23 20:39:57]    param {
[2018-04-23 20:39:57]      lr_mult: 1
[2018-04-23 20:39:57]      decay_mult: 1
[2018-04-23 20:39:57]    }
[2018-04-23 20:39:57]    param {
[2018-04-23 20:39:57]      lr_mult: 2
[2018-04-23 20:39:57]      decay_mult: 0
[2018-04-23 20:39:57]    }
[2018-04-23 20:39:57]    convolution_param {
[2018-04-23 20:39:57]      num_output: 128
[2018-04-23 20:39:57]      pad: 1
[2018-04-23 20:39:57]      kernel_size: 3
[2018-04-23 20:39:57]      stride: 1
[2018-04-23 20:39:57]      weight_filler {
[2018-04-23 20:39:57]        type: "xavier"
[2018-04-23 20:39:57]      }
[2018-04-23 20:39:57]      bias_filler {
[2018-04-23 20:39:57]        type: "constant"
[2018-04-23 20:39:57]        value: 0
[2018-04-23 20:39:57]      }
[2018-04-23 20:39:57]    }
[2018-04-23 20:39:57]  }
[2018-04-23 20:39:57]  layer {
[2018-04-23 20:39:57]    name: "BatchNorm17"
[2018-04-23 20:39:57]    type: "BatchNorm"
[2018-04-23 20:39:57]    bottom: "Convolution17"
[2018-04-23 20:39:57]    top: "Convolution17"
[2018-04-23 20:39:57]    param {
[2018-04-23 20:39:57]      lr_mult: 0
[2018-04-23 20:39:57]      decay_mult: 0
[2018-04-23 20:39:57]    }
[2018-04-23 20:39:57]    param {
[2018-04-23 20:39:57]      lr_mult: 0
[2018-04-23 20:39:57]      decay_mult: 0
[2018-04-23 20:39:57]    }
[2018-04-23 20:39:57]    param {
[2018-04-23 20:39:57]      lr_mult: 0
[2018-04-23 20:39:57]      decay_mult: 0
[2018-04-23 20:39:57]    }
[2018-04-23 20:39:58]  }
[2018-04-23 20:39:58]  layer {
[2018-04-23 20:39:58]    name: "Scale17"
[2018-04-23 20:39:58]    type: "Scale"
[2018-04-23 20:39:58]    bottom: "Convolution17"
[2018-04-23 20:39:58]    top: "Convolution17"
[2018-04-23 20:39:58]    scale_param {
[2018-04-23 20:39:58]      bias_term: true
[2018-04-23 20:39:58]    }
[2018-04-23 20:39:58]  }
[2018-04-23 20:39:58]  layer {
[2018-04-23 20:39:58]    name: "Eltwise7"
[2018-04-23 20:39:58]    type: "Eltwise"
[2018-04-23 20:39:58]    bottom: "Convolution15"
[2018-04-23 20:39:58]    bottom: "Convolution17"
[2018-04-23 20:39:58]    top: "Eltwise7"
[2018-04-23 20:39:58]    eltwise_param {
[2018-04-23 20:39:58]      operation: SUM
[2018-04-23 20:39:58]    }
[2018-04-23 20:39:58]  }
[2018-04-23 20:39:58]  layer {
[2018-04-23 20:39:58]    name: "ReLU15"
[2018-04-23 20:39:58]    type: "ReLU"
[2018-04-23 20:39:58]    bottom: "Eltwise7"
[2018-04-23 20:39:58]    top: "Eltwise7"
[2018-04-23 20:39:58]  }
[2018-04-23 20:39:58]  layer {
[2018-04-23 20:39:58]    name: "Convolution18"
[2018-04-23 20:39:58]    type: "Convolution"
[2018-04-23 20:39:58]    bottom: "Eltwise7"
[2018-04-23 20:39:58]    top: "Convolution18"
[2018-04-23 20:39:58]    param {
[2018-04-23 20:39:58]      lr_mult: 1
[2018-04-23 20:39:58]      decay_mult: 1
[2018-04-23 20:39:58]    }
[2018-04-23 20:39:58]    param {
[2018-04-23 20:39:58]      lr_mult: 2
[2018-04-23 20:39:58]      decay_mult: 0
[2018-04-23 20:39:58]    }
[2018-04-23 20:39:58]    convolution_param {
[2018-04-23 20:39:58]      num_output: 128
[2018-04-23 20:39:58]      pad: 1
[2018-04-23 20:39:58]      kernel_size: 3
[2018-04-23 20:39:58]      stride: 1
[2018-04-23 20:39:58]      weight_filler {
[2018-04-23 20:39:58]        type: "xavier"
[2018-04-23 20:39:58]      }
[2018-04-23 20:39:58]      bias_filler {
[2018-04-23 20:39:58]        type: "constant"
[2018-04-23 20:39:58]        value: 0
[2018-04-23 20:39:58]      }
[2018-04-23 20:39:58]    }
[2018-04-23 20:39:58]  }
[2018-04-23 20:39:58]  layer {
[2018-04-23 20:39:58]    name: "BatchNorm18"
[2018-04-23 20:39:58]    type: "BatchNorm"
[2018-04-23 20:39:58]    bottom: "Convolution18"
[2018-04-23 20:39:58]    top: "Convolution18"
[2018-04-23 20:39:58]    param {
[2018-04-23 20:39:58]      lr_mult: 0
[2018-04-23 20:39:58]      decay_mult: 0
[2018-04-23 20:39:58]    }
[2018-04-23 20:39:58]    param {
[2018-04-23 20:39:58]      lr_mult: 0
[2018-04-23 20:39:58]      decay_mult: 0
[2018-04-23 20:39:58]    }
[2018-04-23 20:39:58]    param {
[2018-04-23 20:39:58]      lr_mult: 0
[2018-04-23 20:39:58]      decay_mult: 0
[2018-04-23 20:39:58]    }
[2018-04-23 20:39:58]  }
[2018-04-23 20:39:58]  layer {
[2018-04-23 20:39:58]    name: "Scale18"
[2018-04-23 20:39:58]    type: "Scale"
[2018-04-23 20:39:58]    bottom: "Convolution18"
[2018-04-23 20:39:58]    top: "Convolution18"
[2018-04-23 20:39:59]    scale_param {
[2018-04-23 20:39:59]      bias_term: true
[2018-04-23 20:39:59]    }
[2018-04-23 20:39:59]  }
[2018-04-23 20:39:59]  layer {
[2018-04-23 20:39:59]    name: "ReLU16"
[2018-04-23 20:39:59]    type: "ReLU"
[2018-04-23 20:39:59]    bottom: "Convolution18"
[2018-04-23 20:39:59]    top: "Convolution18"
[2018-04-23 20:39:59]  }
[2018-04-23 20:39:59]  layer {
[2018-04-23 20:39:59]    name: "Convolution19"
[2018-04-23 20:39:59]    type: "Convolution"
[2018-04-23 20:39:59]    bottom: "Convolution18"
[2018-04-23 20:39:59]    top: "Convolution19"
[2018-04-23 20:39:59]    param {
[2018-04-23 20:39:59]      lr_mult: 1
[2018-04-23 20:39:59]      decay_mult: 1
[2018-04-23 20:39:59]    }
[2018-04-23 20:39:59]    param {
[2018-04-23 20:39:59]      lr_mult: 2
[2018-04-23 20:39:59]      decay_mult: 0
[2018-04-23 20:39:59]    }
[2018-04-23 20:39:59]    convolution_param {
[2018-04-23 20:39:59]      num_output: 128
[2018-04-23 20:39:59]      pad: 1
[2018-04-23 20:39:59]      kernel_size: 3
[2018-04-23 20:39:59]      stride: 1
[2018-04-23 20:39:59]      weight_filler {
[2018-04-23 20:39:59]        type: "xavier"
[2018-04-23 20:39:59]      }
[2018-04-23 20:39:59]      bias_filler {
[2018-04-23 20:39:59]        type: "constant"
[2018-04-23 20:39:59]        value: 0
[2018-04-23 20:39:59]      }
[2018-04-23 20:39:59]    }
[2018-04-23 20:39:59]  }
[2018-04-23 20:39:59]  layer {
[2018-04-23 20:39:59]    name: "BatchNorm19"
[2018-04-23 20:39:59]    type: "BatchNorm"
[2018-04-23 20:39:59]    bottom: "Convolution19"
[2018-04-23 20:39:59]    top: "Convolution19"
[2018-04-23 20:39:59]    param {
[2018-04-23 20:39:59]      lr_mult: 0
[2018-04-23 20:39:59]      decay_mult: 0
[2018-04-23 20:39:59]    }
[2018-04-23 20:39:59]    param {
[2018-04-23 20:39:59]      lr_mult: 0
[2018-04-23 20:39:59]      decay_mult: 0
[2018-04-23 20:39:59]    }
[2018-04-23 20:39:59]    param {
[2018-04-23 20:39:59]      lr_mult: 0
[2018-04-23 20:39:59]      decay_mult: 0
[2018-04-23 20:39:59]    }
[2018-04-23 20:39:59]  }
[2018-04-23 20:39:59]  layer {
[2018-04-23 20:39:59]    name: "Scale19"
[2018-04-23 20:39:59]    type: "Scale"
[2018-04-23 20:39:59]    bottom: "Convolution19"
[2018-04-23 20:39:59]    top: "Convolution19"
[2018-04-23 20:39:59]    scale_param {
[2018-04-23 20:39:59]      bias_term: true
[2018-04-23 20:39:59]    }
[2018-04-23 20:39:59]  }
[2018-04-23 20:39:59]  layer {
[2018-04-23 20:39:59]    name: "Eltwise8"
[2018-04-23 20:39:59]    type: "Eltwise"
[2018-04-23 20:39:59]    bottom: "Eltwise7"
[2018-04-23 20:39:59]    bottom: "Convolution19"
[2018-04-23 20:39:59]    top: "Eltwise8"
[2018-04-23 20:39:59]    eltwise_param {
[2018-04-23 20:39:59]      operation: SUM
[2018-04-23 20:39:59]    }
[2018-04-23 20:39:59]  }
[2018-04-23 20:40:00]  layer {
[2018-04-23 20:40:00]    name: "ReLU17"
[2018-04-23 20:40:00]    type: "ReLU"
[2018-04-23 20:40:00]    bottom: "Eltwise8"
[2018-04-23 20:40:00]    top: "Eltwise8"
[2018-04-23 20:40:00]  }
[2018-04-23 20:40:00]  layer {
[2018-04-23 20:40:00]    name: "Convolution20"
[2018-04-23 20:40:00]    type: "Convolution"
[2018-04-23 20:40:00]    bottom: "Eltwise8"
[2018-04-23 20:40:00]    top: "Convolution20"
[2018-04-23 20:40:00]    param {
[2018-04-23 20:40:00]      lr_mult: 1
[2018-04-23 20:40:00]      decay_mult: 1
[2018-04-23 20:40:00]    }
[2018-04-23 20:40:00]    param {
[2018-04-23 20:40:00]      lr_mult: 2
[2018-04-23 20:40:00]      decay_mult: 0
[2018-04-23 20:40:00]    }
[2018-04-23 20:40:00]    convolution_param {
[2018-04-23 20:40:00]      num_output: 128
[2018-04-23 20:40:00]      pad: 1
[2018-04-23 20:40:00]      kernel_size: 3
[2018-04-23 20:40:00]      stride: 1
[2018-04-23 20:40:00]      weight_filler {
[2018-04-23 20:40:00]        type: "xavier"
[2018-04-23 20:40:00]      }
[2018-04-23 20:40:00]      bias_filler {
[2018-04-23 20:40:00]        type: "constant"
[2018-04-23 20:40:00]        value: 0
[2018-04-23 20:40:00]      }
[2018-04-23 20:40:00]    }
[2018-04-23 20:40:00]  }
[2018-04-23 20:40:00]  layer {
[2018-04-23 20:40:00]    name: "BatchNorm20"
[2018-04-23 20:40:00]    type: "BatchNorm"
[2018-04-23 20:40:00]    bottom: "Convolution20"
[2018-04-23 20:40:00]    top: "Convolution20"
[2018-04-23 20:40:00]    param {
[2018-04-23 20:40:00]      lr_mult: 0
[2018-04-23 20:40:00]      decay_mult: 0
[2018-04-23 20:40:00]    }
[2018-04-23 20:40:00]    param {
[2018-04-23 20:40:00]      lr_mult: 0
[2018-04-23 20:40:00]      decay_mult: 0
[2018-04-23 20:40:00]    }
[2018-04-23 20:40:00]    param {
[2018-04-23 20:40:00]      lr_mult: 0
[2018-04-23 20:40:00]      decay_mult: 0
[2018-04-23 20:40:00]    }
[2018-04-23 20:40:00]  }
[2018-04-23 20:40:00]  layer {
[2018-04-23 20:40:00]    name: "Scale20"
[2018-04-23 20:40:00]    type: "Scale"
[2018-04-23 20:40:00]    bottom: "Convolution20"
[2018-04-23 20:40:00]    top: "Convolution20"
[2018-04-23 20:40:00]    scale_param {
[2018-04-23 20:40:00]      bias_term: true
[2018-04-23 20:40:00]    }
[2018-04-23 20:40:00]  }
[2018-04-23 20:40:00]  layer {
[2018-04-23 20:40:00]    name: "ReLU18"
[2018-04-23 20:40:00]    type: "ReLU"
[2018-04-23 20:40:01]    bottom: "Convolution20"
[2018-04-23 20:40:01]    top: "Convolution20"
[2018-04-23 20:40:01]  }
[2018-04-23 20:40:01]  layer {
[2018-04-23 20:40:01]    name: "Convolution21"
[2018-04-23 20:40:01]    type: "Convolution"
[2018-04-23 20:40:01]    bottom: "Convolution20"
[2018-04-23 20:40:01]    top: "Convolution21"
[2018-04-23 20:40:01]    param {
[2018-04-23 20:40:01]      lr_mult: 1
[2018-04-23 20:40:01]      decay_mult: 1
[2018-04-23 20:40:01]    }
[2018-04-23 20:40:01]    param {
[2018-04-23 20:40:01]      lr_mult: 2
[2018-04-23 20:40:01]      decay_mult: 0
[2018-04-23 20:40:01]    }
[2018-04-23 20:40:01]    convolution_param {
[2018-04-23 20:40:01]      num_output: 128
[2018-04-23 20:40:01]      pad: 1
[2018-04-23 20:40:01]      kernel_size: 3
[2018-04-23 20:40:01]      stride: 1
[2018-04-23 20:40:01]      weight_filler {
[2018-04-23 20:40:01]        type: "xavier"
[2018-04-23 20:40:01]      }
[2018-04-23 20:40:01]      bias_filler {
[2018-04-23 20:40:01]        type: "constant"
[2018-04-23 20:40:01]        value: 0
[2018-04-23 20:40:01]      }
[2018-04-23 20:40:01]    }
[2018-04-23 20:40:01]  }
[2018-04-23 20:40:01]  layer {
[2018-04-23 20:40:01]    name: "BatchNorm21"
[2018-04-23 20:40:01]    type: "BatchNorm"
[2018-04-23 20:40:01]    bottom: "Convolution21"
[2018-04-23 20:40:01]    top: "Convolution21"
[2018-04-23 20:40:01]    param {
[2018-04-23 20:40:01]      lr_mult: 0
[2018-04-23 20:40:01]      decay_mult: 0
[2018-04-23 20:40:01]    }
[2018-04-23 20:40:01]    param {
[2018-04-23 20:40:01]      lr_mult: 0
[2018-04-23 20:40:01]      decay_mult: 0
[2018-04-23 20:40:01]    }
[2018-04-23 20:40:01]    param {
[2018-04-23 20:40:01]      lr_mult: 0
[2018-04-23 20:40:01]      decay_mult: 0
[2018-04-23 20:40:01]    }
[2018-04-23 20:40:01]  }
[2018-04-23 20:40:01]  layer {
[2018-04-23 20:40:01]    name: "Scale21"
[2018-04-23 20:40:01]    type: "Scale"
[2018-04-23 20:40:01]    bottom: "Convolution21"
[2018-04-23 20:40:01]    top: "Convolution21"
[2018-04-23 20:40:02]    scale_param {
[2018-04-23 20:40:02]      bias_term: true
[2018-04-23 20:40:02]    }
[2018-04-23 20:40:02]  }
[2018-04-23 20:40:02]  layer {
[2018-04-23 20:40:02]    name: "Eltwise9"
[2018-04-23 20:40:02]    type: "Eltwise"
[2018-04-23 20:40:02]    bottom: "Eltwise8"
[2018-04-23 20:40:02]    bottom: "Convolution21"
[2018-04-23 20:40:02]    top: "Eltwise9"
[2018-04-23 20:40:02]    eltwise_param {
[2018-04-23 20:40:02]      operation: SUM
[2018-04-23 20:40:02]    }
[2018-04-23 20:40:02]  }
[2018-04-23 20:40:02]  layer {
[2018-04-23 20:40:02]    name: "ReLU19"
[2018-04-23 20:40:02]    type: "ReLU"
[2018-04-23 20:40:02]    bottom: "Eltwise9"
[2018-04-23 20:40:02]    top: "Eltwise9"
[2018-04-23 20:40:02]  }
[2018-04-23 20:40:02]  layer {
[2018-04-23 20:40:02]    name: "Convolution22"
[2018-04-23 20:40:02]    type: "Convolution"
[2018-04-23 20:40:02]    bottom: "Eltwise9"
[2018-04-23 20:40:02]    top: "Convolution22"
[2018-04-23 20:40:02]    param {
[2018-04-23 20:40:02]      lr_mult: 1
[2018-04-23 20:40:02]      decay_mult: 1
[2018-04-23 20:40:02]    }
[2018-04-23 20:40:02]    param {
[2018-04-23 20:40:02]      lr_mult: 2
[2018-04-23 20:40:02]      decay_mult: 0
[2018-04-23 20:40:02]    }
[2018-04-23 20:40:02]    convolution_param {
[2018-04-23 20:40:02]      num_output: 256
[2018-04-23 20:40:02]      pad: 0
[2018-04-23 20:40:02]      kernel_size: 1
[2018-04-23 20:40:02]      stride: 2
[2018-04-23 20:40:02]      weight_filler {
[2018-04-23 20:40:02]        type: "xavier"
[2018-04-23 20:40:02]      }
[2018-04-23 20:40:02]      bias_filler {
[2018-04-23 20:40:02]        type: "constant"
[2018-04-23 20:40:02]        value: 0
[2018-04-23 20:40:02]      }
[2018-04-23 20:40:02]    }
[2018-04-23 20:40:02]  }
[2018-04-23 20:40:02]  layer {
[2018-04-23 20:40:02]    name: "BatchNorm22"
[2018-04-23 20:40:02]    type: "BatchNorm"
[2018-04-23 20:40:02]    bottom: "Convolution22"
[2018-04-23 20:40:02]    top: "Convolution22"
[2018-04-23 20:40:02]    param {
[2018-04-23 20:40:02]      lr_mult: 0
[2018-04-23 20:40:02]      decay_mult: 0
[2018-04-23 20:40:02]    }
[2018-04-23 20:40:02]    param {
[2018-04-23 20:40:02]      lr_mult: 0
[2018-04-23 20:40:02]      decay_mult: 0
[2018-04-23 20:40:03]    }
[2018-04-23 20:40:03]    param {
[2018-04-23 20:40:03]      lr_mult: 0
[2018-04-23 20:40:03]      decay_mult: 0
[2018-04-23 20:40:03]    }
[2018-04-23 20:40:03]  }
[2018-04-23 20:40:03]  layer {
[2018-04-23 20:40:03]    name: "Scale22"
[2018-04-23 20:40:03]    type: "Scale"
[2018-04-23 20:40:03]    bottom: "Convolution22"
[2018-04-23 20:40:03]    top: "Convolution22"
[2018-04-23 20:40:03]    scale_param {
[2018-04-23 20:40:03]      bias_term: true
[2018-04-23 20:40:03]    }
[2018-04-23 20:40:03]  }
[2018-04-23 20:40:03]  layer {
[2018-04-23 20:40:03]    name: "Convolution23"
[2018-04-23 20:40:03]    type: "Convolution"
[2018-04-23 20:40:03]    bottom: "Eltwise9"
[2018-04-23 20:40:03]    top: "Convolution23"
[2018-04-23 20:40:03]    param {
[2018-04-23 20:40:03]      lr_mult: 1
[2018-04-23 20:40:03]      decay_mult: 1
[2018-04-23 20:40:03]    }
[2018-04-23 20:40:03]    param {
[2018-04-23 20:40:03]      lr_mult: 2
[2018-04-23 20:40:03]      decay_mult: 0
[2018-04-23 20:40:03]    }
[2018-04-23 20:40:03]    convolution_param {
[2018-04-23 20:40:03]      num_output: 256
[2018-04-23 20:40:03]      pad: 1
[2018-04-23 20:40:03]      kernel_size: 3
[2018-04-23 20:40:03]      stride: 2
[2018-04-23 20:40:03]      weight_filler {
[2018-04-23 20:40:03]        type: "xavier"
[2018-04-23 20:40:03]      }
[2018-04-23 20:40:03]      bias_filler {
[2018-04-23 20:40:03]        type: "constant"
[2018-04-23 20:40:03]        value: 0
[2018-04-23 20:40:03]      }
[2018-04-23 20:40:03]    }
[2018-04-23 20:40:03]  }
[2018-04-23 20:40:03]  layer {
[2018-04-23 20:40:03]    name: "BatchNorm23"
[2018-04-23 20:40:03]    type: "BatchNorm"
[2018-04-23 20:40:03]    bottom: "Convolution23"
[2018-04-23 20:40:03]    top: "Convolution23"
[2018-04-23 20:40:03]    param {
[2018-04-23 20:40:04]      lr_mult: 0
[2018-04-23 20:40:04]      decay_mult: 0
[2018-04-23 20:40:04]    }
[2018-04-23 20:40:04]    param {
[2018-04-23 20:40:04]      lr_mult: 0
[2018-04-23 20:40:04]      decay_mult: 0
[2018-04-23 20:40:04]    }
[2018-04-23 20:40:04]    param {
[2018-04-23 20:40:04]      lr_mult: 0
[2018-04-23 20:40:04]      decay_mult: 0
[2018-04-23 20:40:04]    }
[2018-04-23 20:40:04]  }
[2018-04-23 20:40:04]  layer {
[2018-04-23 20:40:04]    name: "Scale23"
[2018-04-23 20:40:04]    type: "Scale"
[2018-04-23 20:40:04]    bottom: "Convolution23"
[2018-04-23 20:40:04]    top: "Convolution23"
[2018-04-23 20:40:04]    scale_param {
[2018-04-23 20:40:04]      bias_term: true
[2018-04-23 20:40:04]    }
[2018-04-23 20:40:04]  }
[2018-04-23 20:40:04]  layer {
[2018-04-23 20:40:04]    name: "ReLU20"
[2018-04-23 20:40:04]    type: "ReLU"
[2018-04-23 20:40:04]    bottom: "Convolution23"
[2018-04-23 20:40:04]    top: "Convolution23"
[2018-04-23 20:40:04]  }
[2018-04-23 20:40:04]  layer {
[2018-04-23 20:40:04]    name: "Convolution24"
[2018-04-23 20:40:04]    type: "Convolution"
[2018-04-23 20:40:04]    bottom: "Convolution23"
[2018-04-23 20:40:04]    top: "Convolution24"
[2018-04-23 20:40:04]    param {
[2018-04-23 20:40:04]      lr_mult: 1
[2018-04-23 20:40:04]      decay_mult: 1
[2018-04-23 20:40:04]    }
[2018-04-23 20:40:04]    param {
[2018-04-23 20:40:04]      lr_mult: 2
[2018-04-23 20:40:04]      decay_mult: 0
[2018-04-23 20:40:04]    }
[2018-04-23 20:40:04]    convolution_param {
[2018-04-23 20:40:04]      num_output: 256
[2018-04-23 20:40:04]      pad: 1
[2018-04-23 20:40:04]      kernel_size: 3
[2018-04-23 20:40:04]      stride: 1
[2018-04-23 20:40:04]      weight_filler {
[2018-04-23 20:40:04]        type: "xavier"
[2018-04-23 20:40:04]      }
[2018-04-23 20:40:04]      bias_filler {
[2018-04-23 20:40:04]        type: "constant"
[2018-04-23 20:40:04]        value: 0
[2018-04-23 20:40:04]      }
[2018-04-23 20:40:04]    }
[2018-04-23 20:40:04]  }
[2018-04-23 20:40:04]  layer {
[2018-04-23 20:40:04]    name: "BatchNorm24"
[2018-04-23 20:40:05]    type: "BatchNorm"
[2018-04-23 20:40:05]    bottom: "Convolution24"
[2018-04-23 20:40:05]    top: "Convolution24"
[2018-04-23 20:40:05]    param {
[2018-04-23 20:40:05]      lr_mult: 0
[2018-04-23 20:40:05]      decay_mult: 0
[2018-04-23 20:40:05]    }
[2018-04-23 20:40:05]    param {
[2018-04-23 20:40:05]      lr_mult: 0
[2018-04-23 20:40:05]      decay_mult: 0
[2018-04-23 20:40:05]    }
[2018-04-23 20:40:05]    param {
[2018-04-23 20:40:05]      lr_mult: 0
[2018-04-23 20:40:05]      decay_mult: 0
[2018-04-23 20:40:05]    }
[2018-04-23 20:40:05]  }
[2018-04-23 20:40:05]  layer {
[2018-04-23 20:40:05]    name: "Scale24"
[2018-04-23 20:40:05]    type: "Scale"
[2018-04-23 20:40:05]    bottom: "Convolution24"
[2018-04-23 20:40:05]    top: "Convolution24"
[2018-04-23 20:40:05]    scale_param {
[2018-04-23 20:40:05]      bias_term: true
[2018-04-23 20:40:05]    }
[2018-04-23 20:40:05]  }
[2018-04-23 20:40:05]  layer {
[2018-04-23 20:40:05]    name: "Eltwise10"
[2018-04-23 20:40:05]    type: "Eltwise"
[2018-04-23 20:40:05]    bottom: "Convolution22"
[2018-04-23 20:40:05]    bottom: "Convolution24"
[2018-04-23 20:40:05]    top: "Eltwise10"
[2018-04-23 20:40:05]    eltwise_param {
[2018-04-23 20:40:05]      operation: SUM
[2018-04-23 20:40:05]    }
[2018-04-23 20:40:05]  }
[2018-04-23 20:40:05]  layer {
[2018-04-23 20:40:05]    name: "ReLU21"
[2018-04-23 20:40:05]    type: "ReLU"
[2018-04-23 20:40:05]    bottom: "Eltwise10"
[2018-04-23 20:40:05]    top: "Eltwise10"
[2018-04-23 20:40:05]  }
[2018-04-23 20:40:05]  layer {
[2018-04-23 20:40:05]    name: "Convolution25"
[2018-04-23 20:40:05]    type: "Convolution"
[2018-04-23 20:40:05]    bottom: "Eltwise10"
[2018-04-23 20:40:05]    top: "Convolution25"
[2018-04-23 20:40:05]    param {
[2018-04-23 20:40:05]      lr_mult: 1
[2018-04-23 20:40:05]      decay_mult: 1
[2018-04-23 20:40:05]    }
[2018-04-23 20:40:05]    param {
[2018-04-23 20:40:05]      lr_mult: 2
[2018-04-23 20:40:05]      decay_mult: 0
[2018-04-23 20:40:05]    }
[2018-04-23 20:40:05]    convolution_param {
[2018-04-23 20:40:05]      num_output: 256
[2018-04-23 20:40:05]      pad: 1
[2018-04-23 20:40:05]      kernel_size: 3
[2018-04-23 20:40:05]      stride: 1
[2018-04-23 20:40:06]      weight_filler {
[2018-04-23 20:40:06]        type: "xavier"
[2018-04-23 20:40:06]      }
[2018-04-23 20:40:06]      bias_filler {
[2018-04-23 20:40:06]        type: "constant"
[2018-04-23 20:40:06]        value: 0
[2018-04-23 20:40:06]      }
[2018-04-23 20:40:06]    }
[2018-04-23 20:40:06]  }
[2018-04-23 20:40:06]  layer {
[2018-04-23 20:40:06]    name: "BatchNorm25"
[2018-04-23 20:40:06]    type: "BatchNorm"
[2018-04-23 20:40:06]    bottom: "Convolution25"
[2018-04-23 20:40:06]    top: "Convolution25"
[2018-04-23 20:40:06]    param {
[2018-04-23 20:40:06]      lr_mult: 0
[2018-04-23 20:40:06]      decay_mult: 0
[2018-04-23 20:40:06]    }
[2018-04-23 20:40:06]    param {
[2018-04-23 20:40:06]      lr_mult: 0
[2018-04-23 20:40:06]      decay_mult: 0
[2018-04-23 20:40:06]    }
[2018-04-23 20:40:06]    param {
[2018-04-23 20:40:06]      lr_mult: 0
[2018-04-23 20:40:06]      decay_mult: 0
[2018-04-23 20:40:06]    }
[2018-04-23 20:40:06]  }
[2018-04-23 20:40:06]  layer {
[2018-04-23 20:40:06]    name: "Scale25"
[2018-04-23 20:40:06]    type: "Scale"
[2018-04-23 20:40:06]    bottom: "Convolution25"
[2018-04-23 20:40:06]    top: "Convolution25"
[2018-04-23 20:40:06]    scale_param {
[2018-04-23 20:40:06]      bias_term: true
[2018-04-23 20:40:06]    }
[2018-04-23 20:40:06]  }
[2018-04-23 20:40:06]  layer {
[2018-04-23 20:40:06]    name: "ReLU22"
[2018-04-23 20:40:06]    type: "ReLU"
[2018-04-23 20:40:06]    bottom: "Convolution25"
[2018-04-23 20:40:06]    top: "Convolution25"
[2018-04-23 20:40:06]  }
[2018-04-23 20:40:06]  layer {
[2018-04-23 20:40:06]    name: "Convolution26"
[2018-04-23 20:40:06]    type: "Convolution"
[2018-04-23 20:40:06]    bottom: "Convolution25"
[2018-04-23 20:40:06]    top: "Convolution26"
[2018-04-23 20:40:06]    param {
[2018-04-23 20:40:06]      lr_mult: 1
[2018-04-23 20:40:06]      decay_mult: 1
[2018-04-23 20:40:06]    }
[2018-04-23 20:40:06]    param {
[2018-04-23 20:40:06]      lr_mult: 2
[2018-04-23 20:40:06]      decay_mult: 0
[2018-04-23 20:40:06]    }
[2018-04-23 20:40:07]    convolution_param {
[2018-04-23 20:40:07]      num_output: 256
[2018-04-23 20:40:07]      pad: 1
[2018-04-23 20:40:07]      kernel_size: 3
[2018-04-23 20:40:07]      stride: 1
[2018-04-23 20:40:07]      weight_filler {
[2018-04-23 20:40:07]        type: "xavier"
[2018-04-23 20:40:07]      }
[2018-04-23 20:40:07]      bias_filler {
[2018-04-23 20:40:07]        type: "constant"
[2018-04-23 20:40:07]        value: 0
[2018-04-23 20:40:07]      }
[2018-04-23 20:40:07]    }
[2018-04-23 20:40:07]  }
[2018-04-23 20:40:07]  layer {
[2018-04-23 20:40:07]    name: "BatchNorm26"
[2018-04-23 20:40:07]    type: "BatchNorm"
[2018-04-23 20:40:07]    bottom: "Convolution26"
[2018-04-23 20:40:07]    top: "Convolution26"
[2018-04-23 20:40:07]    param {
[2018-04-23 20:40:07]      lr_mult: 0
[2018-04-23 20:40:07]      decay_mult: 0
[2018-04-23 20:40:07]    }
[2018-04-23 20:40:07]    param {
[2018-04-23 20:40:07]      lr_mult: 0
[2018-04-23 20:40:07]      decay_mult: 0
[2018-04-23 20:40:07]    }
[2018-04-23 20:40:07]    param {
[2018-04-23 20:40:07]      lr_mult: 0
[2018-04-23 20:40:07]      decay_mult: 0
[2018-04-23 20:40:07]    }
[2018-04-23 20:40:07]  }
[2018-04-23 20:40:07]  layer {
[2018-04-23 20:40:07]    name: "Scale26"
[2018-04-23 20:40:07]    type: "Scale"
[2018-04-23 20:40:07]    bottom: "Convolution26"
[2018-04-23 20:40:07]    top: "Convolution26"
[2018-04-23 20:40:07]    scale_param {
[2018-04-23 20:40:07]      bias_term: true
[2018-04-23 20:40:07]    }
[2018-04-23 20:40:07]  }
[2018-04-23 20:40:07]  layer {
[2018-04-23 20:40:07]    name: "Eltwise11"
[2018-04-23 20:40:07]    type: "Eltwise"
[2018-04-23 20:40:07]    bottom: "Eltwise10"
[2018-04-23 20:40:07]    bottom: "Convolution26"
[2018-04-23 20:40:07]    top: "Eltwise11"
[2018-04-23 20:40:07]    eltwise_param {
[2018-04-23 20:40:07]      operation: SUM
[2018-04-23 20:40:07]    }
[2018-04-23 20:40:07]  }
[2018-04-23 20:40:07]  layer {
[2018-04-23 20:40:07]    name: "ReLU23"
[2018-04-23 20:40:07]    type: "ReLU"
[2018-04-23 20:40:07]    bottom: "Eltwise11"
[2018-04-23 20:40:08]    top: "Eltwise11"
[2018-04-23 20:40:08]  }
[2018-04-23 20:40:08]  layer {
[2018-04-23 20:40:08]    name: "Convolution27"
[2018-04-23 20:40:08]    type: "Convolution"
[2018-04-23 20:40:08]    bottom: "Eltwise11"
[2018-04-23 20:40:08]    top: "Convolution27"
[2018-04-23 20:40:08]    param {
[2018-04-23 20:40:08]      lr_mult: 1
[2018-04-23 20:40:08]      decay_mult: 1
[2018-04-23 20:40:08]    }
[2018-04-23 20:40:08]    param {
[2018-04-23 20:40:08]      lr_mult: 2
[2018-04-23 20:40:08]      decay_mult: 0
[2018-04-23 20:40:08]    }
[2018-04-23 20:40:08]    convolution_param {
[2018-04-23 20:40:08]      num_output: 256
[2018-04-23 20:40:08]      pad: 1
[2018-04-23 20:40:08]      kernel_size: 3
[2018-04-23 20:40:08]      stride: 1
[2018-04-23 20:40:08]      weight_filler {
[2018-04-23 20:40:08]        type: "xavier"
[2018-04-23 20:40:08]      }
[2018-04-23 20:40:08]      bias_filler {
[2018-04-23 20:40:08]        type: "constant"
[2018-04-23 20:40:08]        value: 0
[2018-04-23 20:40:08]      }
[2018-04-23 20:40:08]    }
[2018-04-23 20:40:08]  }
[2018-04-23 20:40:08]  layer {
[2018-04-23 20:40:08]    name: "BatchNorm27"
[2018-04-23 20:40:08]    type: "BatchNorm"
[2018-04-23 20:40:08]    bottom: "Convolution27"
[2018-04-23 20:40:08]    top: "Convolution27"
[2018-04-23 20:40:08]    param {
[2018-04-23 20:40:08]      lr_mult: 0
[2018-04-23 20:40:08]      decay_mult: 0
[2018-04-23 20:40:08]    }
[2018-04-23 20:40:08]    param {
[2018-04-23 20:40:08]      lr_mult: 0
[2018-04-23 20:40:08]      decay_mult: 0
[2018-04-23 20:40:08]    }
[2018-04-23 20:40:08]    param {
[2018-04-23 20:40:08]      lr_mult: 0
[2018-04-23 20:40:08]      decay_mult: 0
[2018-04-23 20:40:08]    }
[2018-04-23 20:40:08]  }
[2018-04-23 20:40:08]  layer {
[2018-04-23 20:40:08]    name: "Scale27"
[2018-04-23 20:40:08]    type: "Scale"
[2018-04-23 20:40:08]    bottom: "Convolution27"
[2018-04-23 20:40:08]    top: "Convolution27"
[2018-04-23 20:40:08]    scale_param {
[2018-04-23 20:40:08]      bias_term: true
[2018-04-23 20:40:08]    }
[2018-04-23 20:40:09]  }
[2018-04-23 20:40:09]  layer {
[2018-04-23 20:40:09]    name: "ReLU24"
[2018-04-23 20:40:09]    type: "ReLU"
[2018-04-23 20:40:09]    bottom: "Convolution27"
[2018-04-23 20:40:09]    top: "Convolution27"
[2018-04-23 20:40:09]  }
[2018-04-23 20:40:09]  layer {
[2018-04-23 20:40:09]    name: "Convolution28"
[2018-04-23 20:40:09]    type: "Convolution"
[2018-04-23 20:40:09]    bottom: "Convolution27"
[2018-04-23 20:40:09]    top: "Convolution28"
[2018-04-23 20:40:09]    param {
[2018-04-23 20:40:09]      lr_mult: 1
[2018-04-23 20:40:09]      decay_mult: 1
[2018-04-23 20:40:09]    }
[2018-04-23 20:40:09]    param {
[2018-04-23 20:40:09]      lr_mult: 2
[2018-04-23 20:40:09]      decay_mult: 0
[2018-04-23 20:40:09]    }
[2018-04-23 20:40:09]    convolution_param {
[2018-04-23 20:40:09]      num_output: 256
[2018-04-23 20:40:09]      pad: 1
[2018-04-23 20:40:09]      kernel_size: 3
[2018-04-23 20:40:09]      stride: 1
[2018-04-23 20:40:09]      weight_filler {
[2018-04-23 20:40:09]        type: "xavier"
[2018-04-23 20:40:09]      }
[2018-04-23 20:40:09]      bias_filler {
[2018-04-23 20:40:09]        type: "constant"
[2018-04-23 20:40:09]        value: 0
[2018-04-23 20:40:09]      }
[2018-04-23 20:40:09]    }
[2018-04-23 20:40:09]  }
[2018-04-23 20:40:09]  layer {
[2018-04-23 20:40:09]    name: "BatchNorm28"
[2018-04-23 20:40:09]    type: "BatchNorm"
[2018-04-23 20:40:09]    bottom: "Convolution28"
[2018-04-23 20:40:09]    top: "Convolution28"
[2018-04-23 20:40:09]    param {
[2018-04-23 20:40:09]      lr_mult: 0
[2018-04-23 20:40:09]      decay_mult: 0
[2018-04-23 20:40:09]    }
[2018-04-23 20:40:09]    param {
[2018-04-23 20:40:09]      lr_mult: 0
[2018-04-23 20:40:09]      decay_mult: 0
[2018-04-23 20:40:09]    }
[2018-04-23 20:40:09]    param {
[2018-04-23 20:40:09]      lr_mult: 0
[2018-04-23 20:40:09]      decay_mult: 0
[2018-04-23 20:40:09]    }
[2018-04-23 20:40:09]  }
[2018-04-23 20:40:09]  layer {
[2018-04-23 20:40:09]    name: "Scale28"
[2018-04-23 20:40:09]    type: "Scale"
[2018-04-23 20:40:10]    bottom: "Convolution28"
[2018-04-23 20:40:10]    top: "Convolution28"
[2018-04-23 20:40:10]    scale_param {
[2018-04-23 20:40:10]      bias_term: true
[2018-04-23 20:40:10]    }
[2018-04-23 20:40:10]  }
[2018-04-23 20:40:10]  layer {
[2018-04-23 20:40:10]    name: "Eltwise12"
[2018-04-23 20:40:10]    type: "Eltwise"
[2018-04-23 20:40:10]    bottom: "Eltwise11"
[2018-04-23 20:40:10]    bottom: "Convolution28"
[2018-04-23 20:40:10]    top: "Eltwise12"
[2018-04-23 20:40:10]    eltwise_param {
[2018-04-23 20:40:10]      operation: SUM
[2018-04-23 20:40:10]    }
[2018-04-23 20:40:10]  }
[2018-04-23 20:40:10]  layer {
[2018-04-23 20:40:10]    name: "ReLU25"
[2018-04-23 20:40:10]    type: "ReLU"
[2018-04-23 20:40:10]    bottom: "Eltwise12"
[2018-04-23 20:40:10]    top: "Eltwise12"
[2018-04-23 20:40:10]  }
[2018-04-23 20:40:10]  layer {
[2018-04-23 20:40:10]    name: "Pooling1"
[2018-04-23 20:40:10]    type: "Pooling"
[2018-04-23 20:40:10]    bottom: "Eltwise12"
[2018-04-23 20:40:10]    top: "Pooling1"
[2018-04-23 20:40:10]    pooling_param {
[2018-04-23 20:40:10]      pool: AVE
[2018-04-23 20:40:10]      global_pooling: true
[2018-04-23 20:40:10]    }
[2018-04-23 20:40:10]  }
[2018-04-23 20:40:10]  layer {
[2018-04-23 20:40:10]    name: "InnerProduct1"
[2018-04-23 20:40:10]    type: "InnerProduct"
[2018-04-23 20:40:10]    bottom: "Pooling1"
[2018-04-23 20:40:10]    top: "InnerProduct1"
[2018-04-23 20:40:10]    param {
[2018-04-23 20:40:10]      lr_mult: 1
[2018-04-23 20:40:10]      decay_mult: 1
[2018-04-23 20:40:10]    }
[2018-04-23 20:40:10]    param {
[2018-04-23 20:40:10]      lr_mult: 2
[2018-04-23 20:40:10]      decay_mult: 1
[2018-04-23 20:40:10]    }
[2018-04-23 20:40:10]    inner_product_param {
[2018-04-23 20:40:10]      num_output: 100
[2018-04-23 20:40:10]      weight_filler {
[2018-04-23 20:40:10]        type: "xavier"
[2018-04-23 20:40:10]      }
[2018-04-23 20:40:10]      bias_filler {
[2018-04-23 20:40:10]        type: "constant"
[2018-04-23 20:40:10]        value: 0
[2018-04-23 20:40:10]      }
[2018-04-23 20:40:10]    }
[2018-04-23 20:40:10]  }
[2018-04-23 20:40:10]  layer {
[2018-04-23 20:40:10]    name: "SoftmaxWithLoss1"
[2018-04-23 20:40:10]    type: "SoftmaxWithLoss"
[2018-04-23 20:40:10]    bottom: "InnerProduct1"
[2018-04-23 20:40:10]    bottom: "label"
[2018-04-23 20:40:10]    top: "SoftmaxWithLoss1"
[2018-04-23 20:40:10]  }
[2018-04-23 20:40:10]  layer {
[2018-04-23 20:40:11]    name: "Accuracy_train"
[2018-04-23 20:40:11]    type: "Accuracy"
[2018-04-23 20:40:11]    bottom: "InnerProduct1"
[2018-04-23 20:40:11]    bottom: "label"
[2018-04-23 20:40:11]    top: "Accuracy_train"
[2018-04-23 20:40:11]    include {
[2018-04-23 20:40:11]      phase: TRAIN
[2018-04-23 20:40:11]    }
[2018-04-23 20:40:11]    accuracy_param {
[2018-04-23 20:40:11]      top_k: 1
[2018-04-23 20:40:11]    }
[2018-04-23 20:40:11]  }
[2018-04-23 20:40:11]  I0423 12:39:43.348125    21 layer_factory.hpp:77] Creating layer data
[2018-04-23 20:40:11]  I0423 12:39:43.348839    21 net.cpp:84] Creating Layer data
[2018-04-23 20:40:11]  I0423 12:39:43.349023    21 net.cpp:380] data -> data
[2018-04-23 20:40:11]  I0423 12:39:43.349107    21 net.cpp:380] data -> label
[2018-04-23 20:40:11]  I0423 12:39:43.349280    21 image_data_layer.cpp:38] Opening file ./mydata/train.txt
[2018-04-23 20:40:11]  I0423 12:39:43.383114    21 image_data_layer.cpp:53] Shuffling data
[2018-04-23 20:40:11]  I0423 12:39:43.389876    21 image_data_layer.cpp:63] A total of 35000 images.
[2018-04-23 20:40:11]  I0423 12:39:43.404559    21 image_data_layer.cpp:90] output data size: 64,3,64,64
[2018-04-23 20:40:11]  I0423 12:39:43.429116    21 net.cpp:122] Setting up data
[2018-04-23 20:40:11]  I0423 12:39:43.429201    21 net.cpp:129] Top shape: 64 3 64 64 (786432)
[2018-04-23 20:40:11]  I0423 12:39:43.429250    21 net.cpp:129] Top shape: 64 (64)
[2018-04-23 20:40:11]  I0423 12:39:43.429275    21 net.cpp:137] Memory required for data: 3145984
[2018-04-23 20:40:11]  I0423 12:39:43.429316    21 layer_factory.hpp:77] Creating layer label_data_1_split
[2018-04-23 20:40:11]  I0423 12:39:43.429414    21 net.cpp:84] Creating Layer label_data_1_split
[2018-04-23 20:40:11]  I0423 12:39:43.429455    21 net.cpp:406] label_data_1_split <- label
[2018-04-23 20:40:11]  I0423 12:39:43.429545    21 net.cpp:380] label_data_1_split -> label_data_1_split_0
[2018-04-23 20:40:11]  I0423 12:39:43.429605    21 net.cpp:380] label_data_1_split -> label_data_1_split_1
[2018-04-23 20:40:11]  I0423 12:39:43.429721    21 net.cpp:122] Setting up label_data_1_split
[2018-04-23 20:40:11]  I0423 12:39:43.429770    21 net.cpp:129] Top shape: 64 (64)
[2018-04-23 20:40:11]  I0423 12:39:43.429814    21 net.cpp:129] Top shape: 64 (64)
[2018-04-23 20:40:11]  I0423 12:39:43.429847    21 net.cpp:137] Memory required for data: 3146496
[2018-04-23 20:40:11]  I0423 12:39:43.429893    21 layer_factory.hpp:77] Creating layer Convolution1
[2018-04-23 20:40:11]  I0423 12:39:43.429953    21 net.cpp:84] Creating Layer Convolution1
[2018-04-23 20:40:11]  I0423 12:39:43.430001    21 net.cpp:406] Convolution1 <- data
[2018-04-23 20:40:11]  I0423 12:39:43.430047    21 net.cpp:380] Convolution1 -> Convolution1
[2018-04-23 20:40:11]  I0423 12:39:45.698097    21 net.cpp:122] Setting up Convolution1
[2018-04-23 20:40:11]  I0423 12:39:45.698326    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:11]  I0423 12:39:45.698362    21 net.cpp:137] Memory required for data: 11535104
[2018-04-23 20:40:11]  I0423 12:39:45.698535    21 layer_factory.hpp:77] Creating layer BatchNorm1
[2018-04-23 20:40:11]  I0423 12:39:45.698629    21 net.cpp:84] Creating Layer BatchNorm1
[2018-04-23 20:40:11]  I0423 12:39:45.698676    21 net.cpp:406] BatchNorm1 <- Convolution1
[2018-04-23 20:40:11]  I0423 12:39:45.698731    21 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
[2018-04-23 20:40:11]  I0423 12:39:45.699074    21 net.cpp:122] Setting up BatchNorm1
[2018-04-23 20:40:11]  I0423 12:39:45.699131    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:11]  I0423 12:39:45.699158    21 net.cpp:137] Memory required for data: 19923712
[2018-04-23 20:40:11]  I0423 12:39:45.699204    21 layer_factory.hpp:77] Creating layer Scale1
[2018-04-23 20:40:11]  I0423 12:39:45.699324    21 net.cpp:84] Creating Layer Scale1
[2018-04-23 20:40:11]  I0423 12:39:45.699365    21 net.cpp:406] Scale1 <- Convolution1
[2018-04-23 20:40:11]  I0423 12:39:45.699409    21 net.cpp:367] Scale1 -> Convolution1 (in-place)
[2018-04-23 20:40:11]  I0423 12:39:45.699676    21 layer_factory.hpp:77] Creating layer Scale1
[2018-04-23 20:40:11]  I0423 12:39:45.700017    21 net.cpp:122] Setting up Scale1
[2018-04-23 20:40:11]  I0423 12:39:45.700074    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:11]  I0423 12:39:45.700122    21 net.cpp:137] Memory required for data: 28312320
[2018-04-23 20:40:11]  I0423 12:39:45.700163    21 layer_factory.hpp:77] Creating layer ReLU1
[2018-04-23 20:40:11]  I0423 12:39:45.700214    21 net.cpp:84] Creating Layer ReLU1
[2018-04-23 20:40:12]  I0423 12:39:45.700255    21 net.cpp:406] ReLU1 <- Convolution1
[2018-04-23 20:40:12]  I0423 12:39:45.700300    21 net.cpp:367] ReLU1 -> Convolution1 (in-place)
[2018-04-23 20:40:12]  I0423 12:39:45.700664    21 net.cpp:122] Setting up ReLU1
[2018-04-23 20:40:12]  I0423 12:39:45.700723    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:12]  I0423 12:39:45.700750    21 net.cpp:137] Memory required for data: 36700928
[2018-04-23 20:40:12]  I0423 12:39:45.700775    21 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
[2018-04-23 20:40:12]  I0423 12:39:45.700826    21 net.cpp:84] Creating Layer Convolution1_ReLU1_0_split
[2018-04-23 20:40:12]  I0423 12:39:45.700860    21 net.cpp:406] Convolution1_ReLU1_0_split <- Convolution1
[2018-04-23 20:40:12]  I0423 12:39:45.700948    21 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
[2018-04-23 20:40:12]  I0423 12:39:45.701012    21 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
[2018-04-23 20:40:12]  I0423 12:39:45.701129    21 net.cpp:122] Setting up Convolution1_ReLU1_0_split
[2018-04-23 20:40:12]  I0423 12:39:45.701180    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:12]  I0423 12:39:45.701228    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:12]  I0423 12:39:45.701263    21 net.cpp:137] Memory required for data: 53478144
[2018-04-23 20:40:12]  I0423 12:39:45.701300    21 layer_factory.hpp:77] Creating layer Convolution2
[2018-04-23 20:40:12]  I0423 12:39:45.701381    21 net.cpp:84] Creating Layer Convolution2
[2018-04-23 20:40:12]  I0423 12:39:45.701441    21 net.cpp:406] Convolution2 <- Convolution1_ReLU1_0_split_0
[2018-04-23 20:40:12]  I0423 12:39:45.701472    21 net.cpp:380] Convolution2 -> Convolution2
[2018-04-23 20:40:12]  I0423 12:39:45.705803    21 net.cpp:122] Setting up Convolution2
[2018-04-23 20:40:12]  I0423 12:39:45.705874    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:12]  I0423 12:39:45.705919    21 net.cpp:137] Memory required for data: 61866752
[2018-04-23 20:40:12]  I0423 12:39:45.705974    21 layer_factory.hpp:77] Creating layer BatchNorm2
[2018-04-23 20:40:12]  I0423 12:39:45.706019    21 net.cpp:84] Creating Layer BatchNorm2
[2018-04-23 20:40:12]  I0423 12:39:45.706086    21 net.cpp:406] BatchNorm2 <- Convolution2
[2018-04-23 20:40:12]  I0423 12:39:45.706120    21 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
[2018-04-23 20:40:12]  I0423 12:39:45.706410    21 net.cpp:122] Setting up BatchNorm2
[2018-04-23 20:40:12]  I0423 12:39:45.706465    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:12]  I0423 12:39:45.706532    21 net.cpp:137] Memory required for data: 70255360
[2018-04-23 20:40:12]  I0423 12:39:45.706590    21 layer_factory.hpp:77] Creating layer Scale2
[2018-04-23 20:40:12]  I0423 12:39:45.706622    21 net.cpp:84] Creating Layer Scale2
[2018-04-23 20:40:12]  I0423 12:39:45.706655    21 net.cpp:406] Scale2 <- Convolution2
[2018-04-23 20:40:12]  I0423 12:39:45.706703    21 net.cpp:367] Scale2 -> Convolution2 (in-place)
[2018-04-23 20:40:12]  I0423 12:39:45.706809    21 layer_factory.hpp:77] Creating layer Scale2
[2018-04-23 20:40:12]  I0423 12:39:45.707015    21 net.cpp:122] Setting up Scale2
[2018-04-23 20:40:12]  I0423 12:39:45.707069    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:12]  I0423 12:39:45.707108    21 net.cpp:137] Memory required for data: 78643968
[2018-04-23 20:40:12]  I0423 12:39:45.707147    21 layer_factory.hpp:77] Creating layer ReLU2
[2018-04-23 20:40:12]  I0423 12:39:45.707201    21 net.cpp:84] Creating Layer ReLU2
[2018-04-23 20:40:12]  I0423 12:39:45.707237    21 net.cpp:406] ReLU2 <- Convolution2
[2018-04-23 20:40:12]  I0423 12:39:45.707271    21 net.cpp:367] ReLU2 -> Convolution2 (in-place)
[2018-04-23 20:40:12]  I0423 12:39:45.707576    21 net.cpp:122] Setting up ReLU2
[2018-04-23 20:40:12]  I0423 12:39:45.707631    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:12]  I0423 12:39:45.707670    21 net.cpp:137] Memory required for data: 87032576
[2018-04-23 20:40:12]  I0423 12:39:45.707695    21 layer_factory.hpp:77] Creating layer Convolution3
[2018-04-23 20:40:12]  I0423 12:39:45.707765    21 net.cpp:84] Creating Layer Convolution3
[2018-04-23 20:40:12]  I0423 12:39:45.707793    21 net.cpp:406] Convolution3 <- Convolution2
[2018-04-23 20:40:12]  I0423 12:39:45.707829    21 net.cpp:380] Convolution3 -> Convolution3
[2018-04-23 20:40:12]  I0423 12:39:45.709153    21 net.cpp:122] Setting up Convolution3
[2018-04-23 20:40:12]  I0423 12:39:45.709235    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:12]  I0423 12:39:45.709264    21 net.cpp:137] Memory required for data: 95421184
[2018-04-23 20:40:12]  I0423 12:39:45.709307    21 layer_factory.hpp:77] Creating layer BatchNorm3
[2018-04-23 20:40:12]  I0423 12:39:45.709383    21 net.cpp:84] Creating Layer BatchNorm3
[2018-04-23 20:40:12]  I0423 12:39:45.709424    21 net.cpp:406] BatchNorm3 <- Convolution3
[2018-04-23 20:40:12]  I0423 12:39:45.709553    21 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
[2018-04-23 20:40:12]  I0423 12:39:45.709897    21 net.cpp:122] Setting up BatchNorm3
[2018-04-23 20:40:12]  I0423 12:39:45.709952    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:12]  I0423 12:39:45.709981    21 net.cpp:137] Memory required for data: 103809792
[2018-04-23 20:40:12]  I0423 12:39:45.710038    21 layer_factory.hpp:77] Creating layer Scale3
[2018-04-23 20:40:12]  I0423 12:39:45.710085    21 net.cpp:84] Creating Layer Scale3
[2018-04-23 20:40:12]  I0423 12:39:45.710114    21 net.cpp:406] Scale3 <- Convolution3
[2018-04-23 20:40:13]  I0423 12:39:45.710165    21 net.cpp:367] Scale3 -> Convolution3 (in-place)
[2018-04-23 20:40:13]  I0423 12:39:45.710304    21 layer_factory.hpp:77] Creating layer Scale3
[2018-04-23 20:40:13]  I0423 12:39:45.710568    21 net.cpp:122] Setting up Scale3
[2018-04-23 20:40:13]  I0423 12:39:45.710623    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:13]  I0423 12:39:45.710650    21 net.cpp:137] Memory required for data: 112198400
[2018-04-23 20:40:13]  I0423 12:39:45.710690    21 layer_factory.hpp:77] Creating layer Eltwise1
[2018-04-23 20:40:13]  I0423 12:39:45.710729    21 net.cpp:84] Creating Layer Eltwise1
[2018-04-23 20:40:13]  I0423 12:39:45.710757    21 net.cpp:406] Eltwise1 <- Convolution1_ReLU1_0_split_1
[2018-04-23 20:40:13]  I0423 12:39:45.710795    21 net.cpp:406] Eltwise1 <- Convolution3
[2018-04-23 20:40:13]  I0423 12:39:45.710834    21 net.cpp:380] Eltwise1 -> Eltwise1
[2018-04-23 20:40:13]  I0423 12:39:45.711002    21 net.cpp:122] Setting up Eltwise1
[2018-04-23 20:40:13]  I0423 12:39:45.711063    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:13]  I0423 12:39:45.711112    21 net.cpp:137] Memory required for data: 120587008
[2018-04-23 20:40:13]  I0423 12:39:45.711144    21 layer_factory.hpp:77] Creating layer ReLU3
[2018-04-23 20:40:13]  I0423 12:39:45.711184    21 net.cpp:84] Creating Layer ReLU3
[2018-04-23 20:40:13]  I0423 12:39:45.711221    21 net.cpp:406] ReLU3 <- Eltwise1
[2018-04-23 20:40:13]  I0423 12:39:45.711257    21 net.cpp:367] ReLU3 -> Eltwise1 (in-place)
[2018-04-23 20:40:13]  I0423 12:39:45.711961    21 net.cpp:122] Setting up ReLU3
[2018-04-23 20:40:13]  I0423 12:39:45.712025    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:13]  I0423 12:39:45.712055    21 net.cpp:137] Memory required for data: 128975616
[2018-04-23 20:40:13]  I0423 12:39:45.712093    21 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
[2018-04-23 20:40:13]  I0423 12:39:45.712158    21 net.cpp:84] Creating Layer Eltwise1_ReLU3_0_split
[2018-04-23 20:40:13]  I0423 12:39:45.712198    21 net.cpp:406] Eltwise1_ReLU3_0_split <- Eltwise1
[2018-04-23 20:40:13]  I0423 12:39:45.712261    21 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
[2018-04-23 20:40:13]  I0423 12:39:45.712303    21 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
[2018-04-23 20:40:13]  I0423 12:39:45.712406    21 net.cpp:122] Setting up Eltwise1_ReLU3_0_split
[2018-04-23 20:40:13]  I0423 12:39:45.712467    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:13]  I0423 12:39:45.712538    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:13]  I0423 12:39:45.712590    21 net.cpp:137] Memory required for data: 145752832
[2018-04-23 20:40:13]  I0423 12:39:45.712622    21 layer_factory.hpp:77] Creating layer Convolution4
[2018-04-23 20:40:13]  I0423 12:39:45.712689    21 net.cpp:84] Creating Layer Convolution4
[2018-04-23 20:40:13]  I0423 12:39:45.712733    21 net.cpp:406] Convolution4 <- Eltwise1_ReLU3_0_split_0
[2018-04-23 20:40:13]  I0423 12:39:45.712769    21 net.cpp:380] Convolution4 -> Convolution4
[2018-04-23 20:40:13]  I0423 12:39:45.714428    21 net.cpp:122] Setting up Convolution4
[2018-04-23 20:40:13]  I0423 12:39:45.714505    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:13]  I0423 12:39:45.714550    21 net.cpp:137] Memory required for data: 154141440
[2018-04-23 20:40:13]  I0423 12:39:45.714581    21 layer_factory.hpp:77] Creating layer BatchNorm4
[2018-04-23 20:40:13]  I0423 12:39:45.714643    21 net.cpp:84] Creating Layer BatchNorm4
[2018-04-23 20:40:13]  I0423 12:39:45.714679    21 net.cpp:406] BatchNorm4 <- Convolution4
[2018-04-23 20:40:13]  I0423 12:39:45.714746    21 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
[2018-04-23 20:40:13]  I0423 12:39:45.715097    21 net.cpp:122] Setting up BatchNorm4
[2018-04-23 20:40:13]  I0423 12:39:45.715152    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:13]  I0423 12:39:45.715189    21 net.cpp:137] Memory required for data: 162530048
[2018-04-23 20:40:13]  I0423 12:39:45.715241    21 layer_factory.hpp:77] Creating layer Scale4
[2018-04-23 20:40:13]  I0423 12:39:45.715297    21 net.cpp:84] Creating Layer Scale4
[2018-04-23 20:40:13]  I0423 12:39:45.715328    21 net.cpp:406] Scale4 <- Convolution4
[2018-04-23 20:40:13]  I0423 12:39:45.715358    21 net.cpp:367] Scale4 -> Convolution4 (in-place)
[2018-04-23 20:40:13]  I0423 12:39:45.715500    21 layer_factory.hpp:77] Creating layer Scale4
[2018-04-23 20:40:13]  I0423 12:39:45.715760    21 net.cpp:122] Setting up Scale4
[2018-04-23 20:40:13]  I0423 12:39:45.715816    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:13]  I0423 12:39:45.715843    21 net.cpp:137] Memory required for data: 170918656
[2018-04-23 20:40:13]  I0423 12:39:45.715881    21 layer_factory.hpp:77] Creating layer ReLU4
[2018-04-23 20:40:13]  I0423 12:39:45.715921    21 net.cpp:84] Creating Layer ReLU4
[2018-04-23 20:40:13]  I0423 12:39:45.715948    21 net.cpp:406] ReLU4 <- Convolution4
[2018-04-23 20:40:13]  I0423 12:39:45.715998    21 net.cpp:367] ReLU4 -> Convolution4 (in-place)
[2018-04-23 20:40:13]  I0423 12:39:45.716372    21 net.cpp:122] Setting up ReLU4
[2018-04-23 20:40:13]  I0423 12:39:45.716430    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:13]  I0423 12:39:45.716460    21 net.cpp:137] Memory required for data: 179307264
[2018-04-23 20:40:13]  I0423 12:39:45.716527    21 layer_factory.hpp:77] Creating layer Convolution5
[2018-04-23 20:40:13]  I0423 12:39:45.716608    21 net.cpp:84] Creating Layer Convolution5
[2018-04-23 20:40:14]  I0423 12:39:45.716657    21 net.cpp:406] Convolution5 <- Convolution4
[2018-04-23 20:40:14]  I0423 12:39:45.716717    21 net.cpp:380] Convolution5 -> Convolution5
[2018-04-23 20:40:14]  I0423 12:39:45.718386    21 net.cpp:122] Setting up Convolution5
[2018-04-23 20:40:14]  I0423 12:39:45.718456    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:14]  I0423 12:39:45.718487    21 net.cpp:137] Memory required for data: 187695872
[2018-04-23 20:40:14]  I0423 12:39:45.718544    21 layer_factory.hpp:77] Creating layer BatchNorm5
[2018-04-23 20:40:14]  I0423 12:39:45.718611    21 net.cpp:84] Creating Layer BatchNorm5
[2018-04-23 20:40:14]  I0423 12:39:45.718658    21 net.cpp:406] BatchNorm5 <- Convolution5
[2018-04-23 20:40:14]  I0423 12:39:45.718710    21 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
[2018-04-23 20:40:14]  I0423 12:39:45.719058    21 net.cpp:122] Setting up BatchNorm5
[2018-04-23 20:40:14]  I0423 12:39:45.719111    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:14]  I0423 12:39:45.719139    21 net.cpp:137] Memory required for data: 196084480
[2018-04-23 20:40:14]  I0423 12:39:45.719187    21 layer_factory.hpp:77] Creating layer Scale5
[2018-04-23 20:40:14]  I0423 12:39:45.719229    21 net.cpp:84] Creating Layer Scale5
[2018-04-23 20:40:14]  I0423 12:39:45.719276    21 net.cpp:406] Scale5 <- Convolution5
[2018-04-23 20:40:14]  I0423 12:39:45.719322    21 net.cpp:367] Scale5 -> Convolution5 (in-place)
[2018-04-23 20:40:14]  I0423 12:39:45.719449    21 layer_factory.hpp:77] Creating layer Scale5
[2018-04-23 20:40:14]  I0423 12:39:45.719715    21 net.cpp:122] Setting up Scale5
[2018-04-23 20:40:14]  I0423 12:39:45.719779    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:14]  I0423 12:39:45.719820    21 net.cpp:137] Memory required for data: 204473088
[2018-04-23 20:40:14]  I0423 12:39:45.719857    21 layer_factory.hpp:77] Creating layer Eltwise2
[2018-04-23 20:40:14]  I0423 12:39:45.719889    21 net.cpp:84] Creating Layer Eltwise2
[2018-04-23 20:40:14]  I0423 12:39:45.719916    21 net.cpp:406] Eltwise2 <- Eltwise1_ReLU3_0_split_1
[2018-04-23 20:40:14]  I0423 12:39:45.719959    21 net.cpp:406] Eltwise2 <- Convolution5
[2018-04-23 20:40:14]  I0423 12:39:45.720000    21 net.cpp:380] Eltwise2 -> Eltwise2
[2018-04-23 20:40:14]  I0423 12:39:45.720098    21 net.cpp:122] Setting up Eltwise2
[2018-04-23 20:40:14]  I0423 12:39:45.720147    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:14]  I0423 12:39:45.720183    21 net.cpp:137] Memory required for data: 212861696
[2018-04-23 20:40:14]  I0423 12:39:45.720211    21 layer_factory.hpp:77] Creating layer ReLU5
[2018-04-23 20:40:14]  I0423 12:39:45.720243    21 net.cpp:84] Creating Layer ReLU5
[2018-04-23 20:40:14]  I0423 12:39:45.720270    21 net.cpp:406] ReLU5 <- Eltwise2
[2018-04-23 20:40:14]  I0423 12:39:45.720299    21 net.cpp:367] ReLU5 -> Eltwise2 (in-place)
[2018-04-23 20:40:14]  I0423 12:39:45.720685    21 net.cpp:122] Setting up ReLU5
[2018-04-23 20:40:14]  I0423 12:39:45.720737    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:14]  I0423 12:39:45.720767    21 net.cpp:137] Memory required for data: 221250304
[2018-04-23 20:40:14]  I0423 12:39:45.720808    21 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
[2018-04-23 20:40:14]  I0423 12:39:45.720850    21 net.cpp:84] Creating Layer Eltwise2_ReLU5_0_split
[2018-04-23 20:40:14]  I0423 12:39:45.720901    21 net.cpp:406] Eltwise2_ReLU5_0_split <- Eltwise2
[2018-04-23 20:40:14]  I0423 12:39:45.720944    21 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
[2018-04-23 20:40:14]  I0423 12:39:45.720989    21 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
[2018-04-23 20:40:14]  I0423 12:39:45.721144    21 net.cpp:122] Setting up Eltwise2_ReLU5_0_split
[2018-04-23 20:40:14]  I0423 12:39:45.721195    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:14]  I0423 12:39:45.721225    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:14]  I0423 12:39:45.721262    21 net.cpp:137] Memory required for data: 238027520
[2018-04-23 20:40:14]  I0423 12:39:45.721297    21 layer_factory.hpp:77] Creating layer Convolution6
[2018-04-23 20:40:14]  I0423 12:39:45.721388    21 net.cpp:84] Creating Layer Convolution6
[2018-04-23 20:40:14]  I0423 12:39:45.721436    21 net.cpp:406] Convolution6 <- Eltwise2_ReLU5_0_split_0
[2018-04-23 20:40:14]  I0423 12:39:45.721470    21 net.cpp:380] Convolution6 -> Convolution6
[2018-04-23 20:40:14]  I0423 12:39:45.723119    21 net.cpp:122] Setting up Convolution6
[2018-04-23 20:40:14]  I0423 12:39:45.723186    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:14]  I0423 12:39:45.723217    21 net.cpp:137] Memory required for data: 246416128
[2018-04-23 20:40:14]  I0423 12:39:45.723258    21 layer_factory.hpp:77] Creating layer BatchNorm6
[2018-04-23 20:40:14]  I0423 12:39:45.723320    21 net.cpp:84] Creating Layer BatchNorm6
[2018-04-23 20:40:14]  I0423 12:39:45.723358    21 net.cpp:406] BatchNorm6 <- Convolution6
[2018-04-23 20:40:14]  I0423 12:39:45.723413    21 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
[2018-04-23 20:40:14]  I0423 12:39:45.723788    21 net.cpp:122] Setting up BatchNorm6
[2018-04-23 20:40:14]  I0423 12:39:45.723845    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:14]  I0423 12:39:45.723873    21 net.cpp:137] Memory required for data: 254804736
[2018-04-23 20:40:14]  I0423 12:39:45.723906    21 layer_factory.hpp:77] Creating layer Scale6
[2018-04-23 20:40:14]  I0423 12:39:45.723942    21 net.cpp:84] Creating Layer Scale6
[2018-04-23 20:40:14]  I0423 12:39:45.723968    21 net.cpp:406] Scale6 <- Convolution6
[2018-04-23 20:40:14]  I0423 12:39:45.724016    21 net.cpp:367] Scale6 -> Convolution6 (in-place)
[2018-04-23 20:40:14]  I0423 12:39:45.724148    21 layer_factory.hpp:77] Creating layer Scale6
[2018-04-23 20:40:14]  I0423 12:39:45.724413    21 net.cpp:122] Setting up Scale6
[2018-04-23 20:40:14]  I0423 12:39:45.724467    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:14]  I0423 12:39:45.724504    21 net.cpp:137] Memory required for data: 263193344
[2018-04-23 20:40:14]  I0423 12:39:45.724551    21 layer_factory.hpp:77] Creating layer ReLU6
[2018-04-23 20:40:14]  I0423 12:39:45.724602    21 net.cpp:84] Creating Layer ReLU6
[2018-04-23 20:40:14]  I0423 12:39:45.724630    21 net.cpp:406] ReLU6 <- Convolution6
[2018-04-23 20:40:14]  I0423 12:39:45.724679    21 net.cpp:367] ReLU6 -> Convolution6 (in-place)
[2018-04-23 20:40:14]  I0423 12:39:45.725198    21 net.cpp:122] Setting up ReLU6
[2018-04-23 20:40:14]  I0423 12:39:45.725255    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:14]  I0423 12:39:45.725296    21 net.cpp:137] Memory required for data: 271581952
[2018-04-23 20:40:14]  I0423 12:39:45.725329    21 layer_factory.hpp:77] Creating layer Convolution7
[2018-04-23 20:40:15]  I0423 12:39:45.725450    21 net.cpp:84] Creating Layer Convolution7
[2018-04-23 20:40:15]  I0423 12:39:45.725491    21 net.cpp:406] Convolution7 <- Convolution6
[2018-04-23 20:40:15]  I0423 12:39:45.725561    21 net.cpp:380] Convolution7 -> Convolution7
[2018-04-23 20:40:15]  I0423 12:39:45.727273    21 net.cpp:122] Setting up Convolution7
[2018-04-23 20:40:15]  I0423 12:39:45.727337    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:15]  I0423 12:39:45.727370    21 net.cpp:137] Memory required for data: 279970560
[2018-04-23 20:40:15]  I0423 12:39:45.727421    21 layer_factory.hpp:77] Creating layer BatchNorm7
[2018-04-23 20:40:15]  I0423 12:39:45.727531    21 net.cpp:84] Creating Layer BatchNorm7
[2018-04-23 20:40:15]  I0423 12:39:45.727577    21 net.cpp:406] BatchNorm7 <- Convolution7
[2018-04-23 20:40:15]  I0423 12:39:45.727620    21 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
[2018-04-23 20:40:15]  I0423 12:39:45.727982    21 net.cpp:122] Setting up BatchNorm7
[2018-04-23 20:40:15]  I0423 12:39:45.728034    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:15]  I0423 12:39:45.728071    21 net.cpp:137] Memory required for data: 288359168
[2018-04-23 20:40:15]  I0423 12:39:45.728113    21 layer_factory.hpp:77] Creating layer Scale7
[2018-04-23 20:40:15]  I0423 12:39:45.728168    21 net.cpp:84] Creating Layer Scale7
[2018-04-23 20:40:15]  I0423 12:39:45.728200    21 net.cpp:406] Scale7 <- Convolution7
[2018-04-23 20:40:15]  I0423 12:39:45.728230    21 net.cpp:367] Scale7 -> Convolution7 (in-place)
[2018-04-23 20:40:15]  I0423 12:39:45.728334    21 layer_factory.hpp:77] Creating layer Scale7
[2018-04-23 20:40:15]  I0423 12:39:45.731258    21 net.cpp:122] Setting up Scale7
[2018-04-23 20:40:15]  I0423 12:39:45.731320    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:15]  I0423 12:39:45.731349    21 net.cpp:137] Memory required for data: 296747776
[2018-04-23 20:40:15]  I0423 12:39:45.731377    21 layer_factory.hpp:77] Creating layer Eltwise3
[2018-04-23 20:40:15]  I0423 12:39:45.731415    21 net.cpp:84] Creating Layer Eltwise3
[2018-04-23 20:40:15]  I0423 12:39:45.731452    21 net.cpp:406] Eltwise3 <- Eltwise2_ReLU5_0_split_1
[2018-04-23 20:40:15]  I0423 12:39:45.731490    21 net.cpp:406] Eltwise3 <- Convolution7
[2018-04-23 20:40:15]  I0423 12:39:45.731566    21 net.cpp:380] Eltwise3 -> Eltwise3
[2018-04-23 20:40:15]  I0423 12:39:45.731663    21 net.cpp:122] Setting up Eltwise3
[2018-04-23 20:40:15]  I0423 12:39:45.731716    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:15]  I0423 12:39:45.731742    21 net.cpp:137] Memory required for data: 305136384
[2018-04-23 20:40:15]  I0423 12:39:45.731776    21 layer_factory.hpp:77] Creating layer ReLU7
[2018-04-23 20:40:15]  I0423 12:39:45.731822    21 net.cpp:84] Creating Layer ReLU7
[2018-04-23 20:40:15]  I0423 12:39:45.731854    21 net.cpp:406] ReLU7 <- Eltwise3
[2018-04-23 20:40:15]  I0423 12:39:45.731894    21 net.cpp:367] ReLU7 -> Eltwise3 (in-place)
[2018-04-23 20:40:15]  I0423 12:39:45.732275    21 net.cpp:122] Setting up ReLU7
[2018-04-23 20:40:15]  I0423 12:39:45.732331    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:15]  I0423 12:39:45.732362    21 net.cpp:137] Memory required for data: 313524992
[2018-04-23 20:40:15]  I0423 12:39:45.732399    21 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
[2018-04-23 20:40:15]  I0423 12:39:45.732451    21 net.cpp:84] Creating Layer Eltwise3_ReLU7_0_split
[2018-04-23 20:40:15]  I0423 12:39:45.732523    21 net.cpp:406] Eltwise3_ReLU7_0_split <- Eltwise3
[2018-04-23 20:40:15]  I0423 12:39:45.732578    21 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
[2018-04-23 20:40:15]  I0423 12:39:45.732632    21 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
[2018-04-23 20:40:15]  I0423 12:39:45.732764    21 net.cpp:122] Setting up Eltwise3_ReLU7_0_split
[2018-04-23 20:40:15]  I0423 12:39:45.732831    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:15]  I0423 12:39:45.732872    21 net.cpp:129] Top shape: 64 32 32 32 (2097152)
[2018-04-23 20:40:15]  I0423 12:39:45.732903    21 net.cpp:137] Memory required for data: 330302208
[2018-04-23 20:40:15]  I0423 12:39:45.732959    21 layer_factory.hpp:77] Creating layer Convolution8
[2018-04-23 20:40:15]  I0423 12:39:45.733027    21 net.cpp:84] Creating Layer Convolution8
[2018-04-23 20:40:15]  I0423 12:39:45.733083    21 net.cpp:406] Convolution8 <- Eltwise3_ReLU7_0_split_0
[2018-04-23 20:40:15]  I0423 12:39:45.733146    21 net.cpp:380] Convolution8 -> Convolution8
[2018-04-23 20:40:15]  I0423 12:39:45.735030    21 net.cpp:122] Setting up Convolution8
[2018-04-23 20:40:15]  I0423 12:39:45.735124    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:15]  I0423 12:39:45.735159    21 net.cpp:137] Memory required for data: 334496512
[2018-04-23 20:40:15]  I0423 12:39:45.735210    21 layer_factory.hpp:77] Creating layer BatchNorm8
[2018-04-23 20:40:15]  I0423 12:39:45.735242    21 net.cpp:84] Creating Layer BatchNorm8
[2018-04-23 20:40:15]  I0423 12:39:45.735287    21 net.cpp:406] BatchNorm8 <- Convolution8
[2018-04-23 20:40:15]  I0423 12:39:45.735332    21 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
[2018-04-23 20:40:15]  I0423 12:39:45.735705    21 net.cpp:122] Setting up BatchNorm8
[2018-04-23 20:40:15]  I0423 12:39:45.735760    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:15]  I0423 12:39:45.735792    21 net.cpp:137] Memory required for data: 338690816
[2018-04-23 20:40:15]  I0423 12:39:45.735838    21 layer_factory.hpp:77] Creating layer Scale8
[2018-04-23 20:40:15]  I0423 12:39:45.735900    21 net.cpp:84] Creating Layer Scale8
[2018-04-23 20:40:15]  I0423 12:39:45.735934    21 net.cpp:406] Scale8 <- Convolution8
[2018-04-23 20:40:15]  I0423 12:39:45.735962    21 net.cpp:367] Scale8 -> Convolution8 (in-place)
[2018-04-23 20:40:15]  I0423 12:39:45.736068    21 layer_factory.hpp:77] Creating layer Scale8
[2018-04-23 20:40:15]  I0423 12:39:45.736337    21 net.cpp:122] Setting up Scale8
[2018-04-23 20:40:15]  I0423 12:39:45.736387    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:15]  I0423 12:39:45.736413    21 net.cpp:137] Memory required for data: 342885120
[2018-04-23 20:40:15]  I0423 12:39:45.736474    21 layer_factory.hpp:77] Creating layer Convolution9
[2018-04-23 20:40:15]  I0423 12:39:45.736557    21 net.cpp:84] Creating Layer Convolution9
[2018-04-23 20:40:15]  I0423 12:39:45.736593    21 net.cpp:406] Convolution9 <- Eltwise3_ReLU7_0_split_1
[2018-04-23 20:40:15]  I0423 12:39:45.736632    21 net.cpp:380] Convolution9 -> Convolution9
[2018-04-23 20:40:15]  I0423 12:39:45.741873    21 net.cpp:122] Setting up Convolution9
[2018-04-23 20:40:15]  I0423 12:39:45.741948    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:16]  I0423 12:39:45.741981    21 net.cpp:137] Memory required for data: 347079424
[2018-04-23 20:40:16]  I0423 12:39:45.742029    21 layer_factory.hpp:77] Creating layer BatchNorm9
[2018-04-23 20:40:16]  I0423 12:39:45.742082    21 net.cpp:84] Creating Layer BatchNorm9
[2018-04-23 20:40:16]  I0423 12:39:45.742120    21 net.cpp:406] BatchNorm9 <- Convolution9
[2018-04-23 20:40:16]  I0423 12:39:45.742182    21 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
[2018-04-23 20:40:16]  I0423 12:39:45.742573    21 net.cpp:122] Setting up BatchNorm9
[2018-04-23 20:40:16]  I0423 12:39:45.742624    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:16]  I0423 12:39:45.742650    21 net.cpp:137] Memory required for data: 351273728
[2018-04-23 20:40:16]  I0423 12:39:45.742681    21 layer_factory.hpp:77] Creating layer Scale9
[2018-04-23 20:40:16]  I0423 12:39:45.742712    21 net.cpp:84] Creating Layer Scale9
[2018-04-23 20:40:16]  I0423 12:39:45.742748    21 net.cpp:406] Scale9 <- Convolution9
[2018-04-23 20:40:16]  I0423 12:39:45.742775    21 net.cpp:367] Scale9 -> Convolution9 (in-place)
[2018-04-23 20:40:16]  I0423 12:39:45.743130    21 layer_factory.hpp:77] Creating layer Scale9
[2018-04-23 20:40:16]  I0423 12:39:45.743427    21 net.cpp:122] Setting up Scale9
[2018-04-23 20:40:16]  I0423 12:39:45.743481    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:16]  I0423 12:39:45.743523    21 net.cpp:137] Memory required for data: 355468032
[2018-04-23 20:40:16]  I0423 12:39:45.743566    21 layer_factory.hpp:77] Creating layer ReLU8
[2018-04-23 20:40:16]  I0423 12:39:45.743619    21 net.cpp:84] Creating Layer ReLU8
[2018-04-23 20:40:16]  I0423 12:39:45.743669    21 net.cpp:406] ReLU8 <- Convolution9
[2018-04-23 20:40:16]  I0423 12:39:45.743719    21 net.cpp:367] ReLU8 -> Convolution9 (in-place)
[2018-04-23 20:40:16]  I0423 12:39:45.744112    21 net.cpp:122] Setting up ReLU8
[2018-04-23 20:40:16]  I0423 12:39:45.744164    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:16]  I0423 12:39:45.744192    21 net.cpp:137] Memory required for data: 359662336
[2018-04-23 20:40:16]  I0423 12:39:45.744236    21 layer_factory.hpp:77] Creating layer Convolution10
[2018-04-23 20:40:16]  I0423 12:39:45.744305    21 net.cpp:84] Creating Layer Convolution10
[2018-04-23 20:40:16]  I0423 12:39:45.744339    21 net.cpp:406] Convolution10 <- Convolution9
[2018-04-23 20:40:16]  I0423 12:39:45.744412    21 net.cpp:380] Convolution10 -> Convolution10
[2018-04-23 20:40:16]  I0423 12:39:45.746354    21 net.cpp:122] Setting up Convolution10
[2018-04-23 20:40:16]  I0423 12:39:45.746424    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:16]  I0423 12:39:45.746456    21 net.cpp:137] Memory required for data: 363856640
[2018-04-23 20:40:16]  I0423 12:39:45.746588    21 layer_factory.hpp:77] Creating layer BatchNorm10
[2018-04-23 20:40:16]  I0423 12:39:45.746645    21 net.cpp:84] Creating Layer BatchNorm10
[2018-04-23 20:40:16]  I0423 12:39:45.746675    21 net.cpp:406] BatchNorm10 <- Convolution10
[2018-04-23 20:40:16]  I0423 12:39:45.746760    21 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
[2018-04-23 20:40:16]  I0423 12:39:45.747164    21 net.cpp:122] Setting up BatchNorm10
[2018-04-23 20:40:16]  I0423 12:39:45.747217    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:16]  I0423 12:39:45.747244    21 net.cpp:137] Memory required for data: 368050944
[2018-04-23 20:40:16]  I0423 12:39:45.747285    21 layer_factory.hpp:77] Creating layer Scale10
[2018-04-23 20:40:16]  I0423 12:39:45.747325    21 net.cpp:84] Creating Layer Scale10
[2018-04-23 20:40:16]  I0423 12:39:45.747354    21 net.cpp:406] Scale10 <- Convolution10
[2018-04-23 20:40:16]  I0423 12:39:45.747392    21 net.cpp:367] Scale10 -> Convolution10 (in-place)
[2018-04-23 20:40:16]  I0423 12:39:45.747546    21 layer_factory.hpp:77] Creating layer Scale10
[2018-04-23 20:40:16]  I0423 12:39:45.747905    21 net.cpp:122] Setting up Scale10
[2018-04-23 20:40:16]  I0423 12:39:45.747959    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:16]  I0423 12:39:45.747987    21 net.cpp:137] Memory required for data: 372245248
[2018-04-23 20:40:16]  I0423 12:39:45.748025    21 layer_factory.hpp:77] Creating layer Eltwise4
[2018-04-23 20:40:16]  I0423 12:39:45.748075    21 net.cpp:84] Creating Layer Eltwise4
[2018-04-23 20:40:16]  I0423 12:39:45.748132    21 net.cpp:406] Eltwise4 <- Convolution8
[2018-04-23 20:40:16]  I0423 12:39:45.748179    21 net.cpp:406] Eltwise4 <- Convolution10
[2018-04-23 20:40:16]  I0423 12:39:45.748229    21 net.cpp:380] Eltwise4 -> Eltwise4
[2018-04-23 20:40:16]  I0423 12:39:45.748308    21 net.cpp:122] Setting up Eltwise4
[2018-04-23 20:40:16]  I0423 12:39:45.748358    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:16]  I0423 12:39:45.748385    21 net.cpp:137] Memory required for data: 376439552
[2018-04-23 20:40:16]  I0423 12:39:45.748421    21 layer_factory.hpp:77] Creating layer ReLU9
[2018-04-23 20:40:16]  I0423 12:39:45.748461    21 net.cpp:84] Creating Layer ReLU9
[2018-04-23 20:40:16]  I0423 12:39:45.748515    21 net.cpp:406] ReLU9 <- Eltwise4
[2018-04-23 20:40:16]  I0423 12:39:45.748556    21 net.cpp:367] ReLU9 -> Eltwise4 (in-place)
[2018-04-23 20:40:16]  I0423 12:39:45.748916    21 net.cpp:122] Setting up ReLU9
[2018-04-23 20:40:16]  I0423 12:39:45.748975    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:16]  I0423 12:39:45.749004    21 net.cpp:137] Memory required for data: 380633856
[2018-04-23 20:40:16]  I0423 12:39:45.749028    21 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
[2018-04-23 20:40:16]  I0423 12:39:45.749101    21 net.cpp:84] Creating Layer Eltwise4_ReLU9_0_split
[2018-04-23 20:40:16]  I0423 12:39:45.749135    21 net.cpp:406] Eltwise4_ReLU9_0_split <- Eltwise4
[2018-04-23 20:40:16]  I0423 12:39:45.749197    21 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
[2018-04-23 20:40:16]  I0423 12:39:45.749251    21 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
[2018-04-23 20:40:16]  I0423 12:39:45.749408    21 net.cpp:122] Setting up Eltwise4_ReLU9_0_split
[2018-04-23 20:40:16]  I0423 12:39:45.749464    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:16]  I0423 12:39:45.749503    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:16]  I0423 12:39:45.749547    21 net.cpp:137] Memory required for data: 389022464
[2018-04-23 20:40:17]  I0423 12:39:45.749593    21 layer_factory.hpp:77] Creating layer Convolution11
[2018-04-23 20:40:17]  I0423 12:39:45.749649    21 net.cpp:84] Creating Layer Convolution11
[2018-04-23 20:40:17]  I0423 12:39:45.749686    21 net.cpp:406] Convolution11 <- Eltwise4_ReLU9_0_split_0
[2018-04-23 20:40:17]  I0423 12:39:45.749742    21 net.cpp:380] Convolution11 -> Convolution11
[2018-04-23 20:40:17]  I0423 12:39:45.751667    21 net.cpp:122] Setting up Convolution11
[2018-04-23 20:40:17]  I0423 12:39:45.751734    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:17]  I0423 12:39:45.751765    21 net.cpp:137] Memory required for data: 393216768
[2018-04-23 20:40:17]  I0423 12:39:45.751807    21 layer_factory.hpp:77] Creating layer BatchNorm11
[2018-04-23 20:40:17]  I0423 12:39:45.751870    21 net.cpp:84] Creating Layer BatchNorm11
[2018-04-23 20:40:17]  I0423 12:39:45.751909    21 net.cpp:406] BatchNorm11 <- Convolution11
[2018-04-23 20:40:17]  I0423 12:39:45.751971    21 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
[2018-04-23 20:40:17]  I0423 12:39:45.752341    21 net.cpp:122] Setting up BatchNorm11
[2018-04-23 20:40:17]  I0423 12:39:45.752408    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:17]  I0423 12:39:45.752434    21 net.cpp:137] Memory required for data: 397411072
[2018-04-23 20:40:17]  I0423 12:39:45.752470    21 layer_factory.hpp:77] Creating layer Scale11
[2018-04-23 20:40:17]  I0423 12:39:45.752526    21 net.cpp:84] Creating Layer Scale11
[2018-04-23 20:40:17]  I0423 12:39:45.752564    21 net.cpp:406] Scale11 <- Convolution11
[2018-04-23 20:40:17]  I0423 12:39:45.752604    21 net.cpp:367] Scale11 -> Convolution11 (in-place)
[2018-04-23 20:40:17]  I0423 12:39:45.752751    21 layer_factory.hpp:77] Creating layer Scale11
[2018-04-23 20:40:17]  I0423 12:39:45.753003    21 net.cpp:122] Setting up Scale11
[2018-04-23 20:40:17]  I0423 12:39:45.753053    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:17]  I0423 12:39:45.753080    21 net.cpp:137] Memory required for data: 401605376
[2018-04-23 20:40:17]  I0423 12:39:45.753119    21 layer_factory.hpp:77] Creating layer ReLU10
[2018-04-23 20:40:17]  I0423 12:39:45.753165    21 net.cpp:84] Creating Layer ReLU10
[2018-04-23 20:40:17]  I0423 12:39:45.753195    21 net.cpp:406] ReLU10 <- Convolution11
[2018-04-23 20:40:17]  I0423 12:39:45.753248    21 net.cpp:367] ReLU10 -> Convolution11 (in-place)
[2018-04-23 20:40:17]  I0423 12:39:45.753847    21 net.cpp:122] Setting up ReLU10
[2018-04-23 20:40:17]  I0423 12:39:45.753912    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:17]  I0423 12:39:45.753954    21 net.cpp:137] Memory required for data: 405799680
[2018-04-23 20:40:17]  I0423 12:39:45.753991    21 layer_factory.hpp:77] Creating layer Convolution12
[2018-04-23 20:40:17]  I0423 12:39:45.754082    21 net.cpp:84] Creating Layer Convolution12
[2018-04-23 20:40:17]  I0423 12:39:45.754128    21 net.cpp:406] Convolution12 <- Convolution11
[2018-04-23 20:40:17]  I0423 12:39:45.754235    21 net.cpp:380] Convolution12 -> Convolution12
[2018-04-23 20:40:17]  I0423 12:39:45.756291    21 net.cpp:122] Setting up Convolution12
[2018-04-23 20:40:17]  I0423 12:39:45.756382    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:17]  I0423 12:39:45.756428    21 net.cpp:137] Memory required for data: 409993984
[2018-04-23 20:40:17]  I0423 12:39:45.756470    21 layer_factory.hpp:77] Creating layer BatchNorm12
[2018-04-23 20:40:17]  I0423 12:39:45.756531    21 net.cpp:84] Creating Layer BatchNorm12
[2018-04-23 20:40:17]  I0423 12:39:45.756572    21 net.cpp:406] BatchNorm12 <- Convolution12
[2018-04-23 20:40:17]  I0423 12:39:45.756628    21 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
[2018-04-23 20:40:17]  I0423 12:39:45.756978    21 net.cpp:122] Setting up BatchNorm12
[2018-04-23 20:40:17]  I0423 12:39:45.757031    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:17]  I0423 12:39:45.757057    21 net.cpp:137] Memory required for data: 414188288
[2018-04-23 20:40:17]  I0423 12:39:45.757087    21 layer_factory.hpp:77] Creating layer Scale12
[2018-04-23 20:40:17]  I0423 12:39:45.757117    21 net.cpp:84] Creating Layer Scale12
[2018-04-23 20:40:17]  I0423 12:39:45.757154    21 net.cpp:406] Scale12 <- Convolution12
[2018-04-23 20:40:17]  I0423 12:39:45.757181    21 net.cpp:367] Scale12 -> Convolution12 (in-place)
[2018-04-23 20:40:17]  I0423 12:39:45.757302    21 layer_factory.hpp:77] Creating layer Scale12
[2018-04-23 20:40:17]  I0423 12:39:45.757580    21 net.cpp:122] Setting up Scale12
[2018-04-23 20:40:17]  I0423 12:39:45.757637    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:17]  I0423 12:39:45.757664    21 net.cpp:137] Memory required for data: 418382592
[2018-04-23 20:40:17]  I0423 12:39:45.757709    21 layer_factory.hpp:77] Creating layer Eltwise5
[2018-04-23 20:40:17]  I0423 12:39:45.757771    21 net.cpp:84] Creating Layer Eltwise5
[2018-04-23 20:40:17]  I0423 12:39:45.757807    21 net.cpp:406] Eltwise5 <- Eltwise4_ReLU9_0_split_1
[2018-04-23 20:40:17]  I0423 12:39:45.757858    21 net.cpp:406] Eltwise5 <- Convolution12
[2018-04-23 20:40:17]  I0423 12:39:45.757905    21 net.cpp:380] Eltwise5 -> Eltwise5
[2018-04-23 20:40:17]  I0423 12:39:45.757987    21 net.cpp:122] Setting up Eltwise5
[2018-04-23 20:40:17]  I0423 12:39:45.758038    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:17]  I0423 12:39:45.758065    21 net.cpp:137] Memory required for data: 422576896
[2018-04-23 20:40:17]  I0423 12:39:45.758100    21 layer_factory.hpp:77] Creating layer ReLU11
[2018-04-23 20:40:17]  I0423 12:39:45.758147    21 net.cpp:84] Creating Layer ReLU11
[2018-04-23 20:40:17]  I0423 12:39:45.758177    21 net.cpp:406] ReLU11 <- Eltwise5
[2018-04-23 20:40:17]  I0423 12:39:45.758210    21 net.cpp:367] ReLU11 -> Eltwise5 (in-place)
[2018-04-23 20:40:17]  I0423 12:39:45.758623    21 net.cpp:122] Setting up ReLU11
[2018-04-23 20:40:17]  I0423 12:39:45.758682    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:17]  I0423 12:39:45.758709    21 net.cpp:137] Memory required for data: 426771200
[2018-04-23 20:40:17]  I0423 12:39:45.758734    21 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
[2018-04-23 20:40:17]  I0423 12:39:45.758805    21 net.cpp:84] Creating Layer Eltwise5_ReLU11_0_split
[2018-04-23 20:40:17]  I0423 12:39:45.758849    21 net.cpp:406] Eltwise5_ReLU11_0_split <- Eltwise5
[2018-04-23 20:40:17]  I0423 12:39:45.758898    21 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
[2018-04-23 20:40:17]  I0423 12:39:45.758940    21 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
[2018-04-23 20:40:17]  I0423 12:39:45.759161    21 net.cpp:122] Setting up Eltwise5_ReLU11_0_split
[2018-04-23 20:40:17]  I0423 12:39:45.759217    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:17]  I0423 12:39:45.759258    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:18]  I0423 12:39:45.759290    21 net.cpp:137] Memory required for data: 435159808
[2018-04-23 20:40:18]  I0423 12:39:45.759327    21 layer_factory.hpp:77] Creating layer Convolution13
[2018-04-23 20:40:18]  I0423 12:39:45.759469    21 net.cpp:84] Creating Layer Convolution13
[2018-04-23 20:40:18]  I0423 12:39:45.759526    21 net.cpp:406] Convolution13 <- Eltwise5_ReLU11_0_split_0
[2018-04-23 20:40:18]  I0423 12:39:45.759563    21 net.cpp:380] Convolution13 -> Convolution13
[2018-04-23 20:40:18]  I0423 12:39:45.761548    21 net.cpp:122] Setting up Convolution13
[2018-04-23 20:40:18]  I0423 12:39:45.761617    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:18]  I0423 12:39:45.761656    21 net.cpp:137] Memory required for data: 439354112
[2018-04-23 20:40:18]  I0423 12:39:45.761718    21 layer_factory.hpp:77] Creating layer BatchNorm13
[2018-04-23 20:40:18]  I0423 12:39:45.761778    21 net.cpp:84] Creating Layer BatchNorm13
[2018-04-23 20:40:18]  I0423 12:39:45.761824    21 net.cpp:406] BatchNorm13 <- Convolution13
[2018-04-23 20:40:18]  I0423 12:39:45.761884    21 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
[2018-04-23 20:40:18]  I0423 12:39:45.762245    21 net.cpp:122] Setting up BatchNorm13
[2018-04-23 20:40:18]  I0423 12:39:45.762300    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:18]  I0423 12:39:45.762338    21 net.cpp:137] Memory required for data: 443548416
[2018-04-23 20:40:18]  I0423 12:39:45.762385    21 layer_factory.hpp:77] Creating layer Scale13
[2018-04-23 20:40:18]  I0423 12:39:45.762425    21 net.cpp:84] Creating Layer Scale13
[2018-04-23 20:40:18]  I0423 12:39:45.762460    21 net.cpp:406] Scale13 <- Convolution13
[2018-04-23 20:40:18]  I0423 12:39:45.762504    21 net.cpp:367] Scale13 -> Convolution13 (in-place)
[2018-04-23 20:40:18]  I0423 12:39:45.762636    21 layer_factory.hpp:77] Creating layer Scale13
[2018-04-23 20:40:18]  I0423 12:39:45.762898    21 net.cpp:122] Setting up Scale13
[2018-04-23 20:40:18]  I0423 12:39:45.762951    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:18]  I0423 12:39:45.762979    21 net.cpp:137] Memory required for data: 447742720
[2018-04-23 20:40:18]  I0423 12:39:45.763018    21 layer_factory.hpp:77] Creating layer ReLU12
[2018-04-23 20:40:18]  I0423 12:39:45.763057    21 net.cpp:84] Creating Layer ReLU12
[2018-04-23 20:40:18]  I0423 12:39:45.763084    21 net.cpp:406] ReLU12 <- Convolution13
[2018-04-23 20:40:18]  I0423 12:39:45.763134    21 net.cpp:367] ReLU12 -> Convolution13 (in-place)
[2018-04-23 20:40:18]  I0423 12:39:45.763545    21 net.cpp:122] Setting up ReLU12
[2018-04-23 20:40:18]  I0423 12:39:45.763603    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:18]  I0423 12:39:45.763631    21 net.cpp:137] Memory required for data: 451937024
[2018-04-23 20:40:18]  I0423 12:39:45.763656    21 layer_factory.hpp:77] Creating layer Convolution14
[2018-04-23 20:40:18]  I0423 12:39:45.763761    21 net.cpp:84] Creating Layer Convolution14
[2018-04-23 20:40:18]  I0423 12:39:45.763800    21 net.cpp:406] Convolution14 <- Convolution13
[2018-04-23 20:40:18]  I0423 12:39:45.763844    21 net.cpp:380] Convolution14 -> Convolution14
[2018-04-23 20:40:18]  I0423 12:39:45.766001    21 net.cpp:122] Setting up Convolution14
[2018-04-23 20:40:18]  I0423 12:39:45.766070    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:18]  I0423 12:39:45.766100    21 net.cpp:137] Memory required for data: 456131328
[2018-04-23 20:40:18]  I0423 12:39:45.766146    21 layer_factory.hpp:77] Creating layer BatchNorm14
[2018-04-23 20:40:18]  I0423 12:39:45.766216    21 net.cpp:84] Creating Layer BatchNorm14
[2018-04-23 20:40:18]  I0423 12:39:45.766247    21 net.cpp:406] BatchNorm14 <- Convolution14
[2018-04-23 20:40:18]  I0423 12:39:45.766297    21 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
[2018-04-23 20:40:18]  I0423 12:39:45.766690    21 net.cpp:122] Setting up BatchNorm14
[2018-04-23 20:40:18]  I0423 12:39:45.766744    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:18]  I0423 12:39:45.766772    21 net.cpp:137] Memory required for data: 460325632
[2018-04-23 20:40:18]  I0423 12:39:45.766814    21 layer_factory.hpp:77] Creating layer Scale14
[2018-04-23 20:40:18]  I0423 12:39:45.766854    21 net.cpp:84] Creating Layer Scale14
[2018-04-23 20:40:18]  I0423 12:39:45.766883    21 net.cpp:406] Scale14 <- Convolution14
[2018-04-23 20:40:18]  I0423 12:39:45.766933    21 net.cpp:367] Scale14 -> Convolution14 (in-place)
[2018-04-23 20:40:18]  I0423 12:39:45.767051    21 layer_factory.hpp:77] Creating layer Scale14
[2018-04-23 20:40:18]  I0423 12:39:45.767326    21 net.cpp:122] Setting up Scale14
[2018-04-23 20:40:18]  I0423 12:39:45.767380    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:18]  I0423 12:39:45.767407    21 net.cpp:137] Memory required for data: 464519936
[2018-04-23 20:40:18]  I0423 12:39:45.767446    21 layer_factory.hpp:77] Creating layer Eltwise6
[2018-04-23 20:40:18]  I0423 12:39:45.767487    21 net.cpp:84] Creating Layer Eltwise6
[2018-04-23 20:40:18]  I0423 12:39:45.767527    21 net.cpp:406] Eltwise6 <- Eltwise5_ReLU11_0_split_1
[2018-04-23 20:40:18]  I0423 12:39:45.767556    21 net.cpp:406] Eltwise6 <- Convolution14
[2018-04-23 20:40:18]  I0423 12:39:45.767613    21 net.cpp:380] Eltwise6 -> Eltwise6
[2018-04-23 20:40:18]  I0423 12:39:45.767711    21 net.cpp:122] Setting up Eltwise6
[2018-04-23 20:40:18]  I0423 12:39:45.767771    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:18]  I0423 12:39:45.767802    21 net.cpp:137] Memory required for data: 468714240
[2018-04-23 20:40:18]  I0423 12:39:45.767843    21 layer_factory.hpp:77] Creating layer ReLU13
[2018-04-23 20:40:18]  I0423 12:39:45.767881    21 net.cpp:84] Creating Layer ReLU13
[2018-04-23 20:40:18]  I0423 12:39:45.767915    21 net.cpp:406] ReLU13 <- Eltwise6
[2018-04-23 20:40:18]  I0423 12:39:45.767963    21 net.cpp:367] ReLU13 -> Eltwise6 (in-place)
[2018-04-23 20:40:18]  I0423 12:39:45.768551    21 net.cpp:122] Setting up ReLU13
[2018-04-23 20:40:18]  I0423 12:39:45.768616    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:18]  I0423 12:39:45.768646    21 net.cpp:137] Memory required for data: 472908544
[2018-04-23 20:40:18]  I0423 12:39:45.768682    21 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
[2018-04-23 20:40:18]  I0423 12:39:45.768739    21 net.cpp:84] Creating Layer Eltwise6_ReLU13_0_split
[2018-04-23 20:40:18]  I0423 12:39:45.768798    21 net.cpp:406] Eltwise6_ReLU13_0_split <- Eltwise6
[2018-04-23 20:40:18]  I0423 12:39:45.768837    21 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
[2018-04-23 20:40:18]  I0423 12:39:45.768882    21 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
[2018-04-23 20:40:18]  I0423 12:39:45.769026    21 net.cpp:122] Setting up Eltwise6_ReLU13_0_split
[2018-04-23 20:40:18]  I0423 12:39:45.769078    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:18]  I0423 12:39:45.769107    21 net.cpp:129] Top shape: 64 64 16 16 (1048576)
[2018-04-23 20:40:18]  I0423 12:39:45.769132    21 net.cpp:137] Memory required for data: 481297152
[2018-04-23 20:40:18]  I0423 12:39:45.769165    21 layer_factory.hpp:77] Creating layer Convolution15
[2018-04-23 20:40:18]  I0423 12:39:45.769219    21 net.cpp:84] Creating Layer Convolution15
[2018-04-23 20:40:19]  I0423 12:39:45.769258    21 net.cpp:406] Convolution15 <- Eltwise6_ReLU13_0_split_0
[2018-04-23 20:40:19]  I0423 12:39:45.769304    21 net.cpp:380] Convolution15 -> Convolution15
[2018-04-23 20:40:19]  I0423 12:39:45.771296    21 net.cpp:122] Setting up Convolution15
[2018-04-23 20:40:19]  I0423 12:39:45.771364    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:19]  I0423 12:39:45.771395    21 net.cpp:137] Memory required for data: 483394304
[2018-04-23 20:40:19]  I0423 12:39:45.771425    21 layer_factory.hpp:77] Creating layer BatchNorm15
[2018-04-23 20:40:19]  I0423 12:39:45.771476    21 net.cpp:84] Creating Layer BatchNorm15
[2018-04-23 20:40:19]  I0423 12:39:45.771543    21 net.cpp:406] BatchNorm15 <- Convolution15
[2018-04-23 20:40:19]  I0423 12:39:45.771587    21 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
[2018-04-23 20:40:19]  I0423 12:39:45.771977    21 net.cpp:122] Setting up BatchNorm15
[2018-04-23 20:40:19]  I0423 12:39:45.772035    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:19]  I0423 12:39:45.772063    21 net.cpp:137] Memory required for data: 485491456
[2018-04-23 20:40:19]  I0423 12:39:45.772105    21 layer_factory.hpp:77] Creating layer Scale15
[2018-04-23 20:40:19]  I0423 12:39:45.772179    21 net.cpp:84] Creating Layer Scale15
[2018-04-23 20:40:19]  I0423 12:39:45.772217    21 net.cpp:406] Scale15 <- Convolution15
[2018-04-23 20:40:19]  I0423 12:39:45.772259    21 net.cpp:367] Scale15 -> Convolution15 (in-place)
[2018-04-23 20:40:19]  I0423 12:39:45.772392    21 layer_factory.hpp:77] Creating layer Scale15
[2018-04-23 20:40:19]  I0423 12:39:45.772688    21 net.cpp:122] Setting up Scale15
[2018-04-23 20:40:19]  I0423 12:39:45.772740    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:19]  I0423 12:39:45.772768    21 net.cpp:137] Memory required for data: 487588608
[2018-04-23 20:40:19]  I0423 12:39:45.772816    21 layer_factory.hpp:77] Creating layer Convolution16
[2018-04-23 20:40:19]  I0423 12:39:45.772891    21 net.cpp:84] Creating Layer Convolution16
[2018-04-23 20:40:19]  I0423 12:39:45.772925    21 net.cpp:406] Convolution16 <- Eltwise6_ReLU13_0_split_1
[2018-04-23 20:40:19]  I0423 12:39:45.772964    21 net.cpp:380] Convolution16 -> Convolution16
[2018-04-23 20:40:19]  I0423 12:39:45.778918    21 net.cpp:122] Setting up Convolution16
[2018-04-23 20:40:19]  I0423 12:39:45.778995    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:19]  I0423 12:39:45.779027    21 net.cpp:137] Memory required for data: 489685760
[2018-04-23 20:40:19]  I0423 12:39:45.779075    21 layer_factory.hpp:77] Creating layer BatchNorm16
[2018-04-23 20:40:19]  I0423 12:39:45.779122    21 net.cpp:84] Creating Layer BatchNorm16
[2018-04-23 20:40:19]  I0423 12:39:45.779181    21 net.cpp:406] BatchNorm16 <- Convolution16
[2018-04-23 20:40:19]  I0423 12:39:45.779239    21 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
[2018-04-23 20:40:19]  I0423 12:39:45.779623    21 net.cpp:122] Setting up BatchNorm16
[2018-04-23 20:40:19]  I0423 12:39:45.779675    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:19]  I0423 12:39:45.779702    21 net.cpp:137] Memory required for data: 491782912
[2018-04-23 20:40:19]  I0423 12:39:45.779744    21 layer_factory.hpp:77] Creating layer Scale16
[2018-04-23 20:40:19]  I0423 12:39:45.779790    21 net.cpp:84] Creating Layer Scale16
[2018-04-23 20:40:19]  I0423 12:39:45.779827    21 net.cpp:406] Scale16 <- Convolution16
[2018-04-23 20:40:19]  I0423 12:39:45.779878    21 net.cpp:367] Scale16 -> Convolution16 (in-place)
[2018-04-23 20:40:19]  I0423 12:39:45.780001    21 layer_factory.hpp:77] Creating layer Scale16
[2018-04-23 20:40:19]  I0423 12:39:45.780269    21 net.cpp:122] Setting up Scale16
[2018-04-23 20:40:19]  I0423 12:39:45.780324    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:19]  I0423 12:39:45.780352    21 net.cpp:137] Memory required for data: 493880064
[2018-04-23 20:40:19]  I0423 12:39:45.780391    21 layer_factory.hpp:77] Creating layer ReLU14
[2018-04-23 20:40:19]  I0423 12:39:45.780431    21 net.cpp:84] Creating Layer ReLU14
[2018-04-23 20:40:19]  I0423 12:39:45.780463    21 net.cpp:406] ReLU14 <- Convolution16
[2018-04-23 20:40:19]  I0423 12:39:45.780524    21 net.cpp:367] ReLU14 -> Convolution16 (in-place)
[2018-04-23 20:40:19]  I0423 12:39:45.781107    21 net.cpp:122] Setting up ReLU14
[2018-04-23 20:40:19]  I0423 12:39:45.781172    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:19]  I0423 12:39:45.781203    21 net.cpp:137] Memory required for data: 495977216
[2018-04-23 20:40:19]  I0423 12:39:45.781240    21 layer_factory.hpp:77] Creating layer Convolution17
[2018-04-23 20:40:19]  I0423 12:39:45.781301    21 net.cpp:84] Creating Layer Convolution17
[2018-04-23 20:40:19]  I0423 12:39:45.781379    21 net.cpp:406] Convolution17 <- Convolution16
[2018-04-23 20:40:19]  I0423 12:39:45.781442    21 net.cpp:380] Convolution17 -> Convolution17
[2018-04-23 20:40:19]  I0423 12:39:45.784485    21 net.cpp:122] Setting up Convolution17
[2018-04-23 20:40:19]  I0423 12:39:45.784562    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:19]  I0423 12:39:45.784593    21 net.cpp:137] Memory required for data: 498074368
[2018-04-23 20:40:19]  I0423 12:39:45.784644    21 layer_factory.hpp:77] Creating layer BatchNorm17
[2018-04-23 20:40:19]  I0423 12:39:45.784723    21 net.cpp:84] Creating Layer BatchNorm17
[2018-04-23 20:40:19]  I0423 12:39:45.784760    21 net.cpp:406] BatchNorm17 <- Convolution17
[2018-04-23 20:40:19]  I0423 12:39:45.784801    21 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
[2018-04-23 20:40:19]  I0423 12:39:45.785184    21 net.cpp:122] Setting up BatchNorm17
[2018-04-23 20:40:19]  I0423 12:39:45.785236    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:19]  I0423 12:39:45.785264    21 net.cpp:137] Memory required for data: 500171520
[2018-04-23 20:40:19]  I0423 12:39:45.785310    21 layer_factory.hpp:77] Creating layer Scale17
[2018-04-23 20:40:19]  I0423 12:39:45.785390    21 net.cpp:84] Creating Layer Scale17
[2018-04-23 20:40:19]  I0423 12:39:45.785434    21 net.cpp:406] Scale17 <- Convolution17
[2018-04-23 20:40:19]  I0423 12:39:45.785475    21 net.cpp:367] Scale17 -> Convolution17 (in-place)
[2018-04-23 20:40:19]  I0423 12:39:45.785715    21 layer_factory.hpp:77] Creating layer Scale17
[2018-04-23 20:40:19]  I0423 12:39:45.785995    21 net.cpp:122] Setting up Scale17
[2018-04-23 20:40:19]  I0423 12:39:45.786047    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:19]  I0423 12:39:45.786074    21 net.cpp:137] Memory required for data: 502268672
[2018-04-23 20:40:19]  I0423 12:39:45.786115    21 layer_factory.hpp:77] Creating layer Eltwise7
[2018-04-23 20:40:19]  I0423 12:39:45.786175    21 net.cpp:84] Creating Layer Eltwise7
[2018-04-23 20:40:19]  I0423 12:39:45.786209    21 net.cpp:406] Eltwise7 <- Convolution15
[2018-04-23 20:40:19]  I0423 12:39:45.786260    21 net.cpp:406] Eltwise7 <- Convolution17
[2018-04-23 20:40:20]  I0423 12:39:45.786298    21 net.cpp:380] Eltwise7 -> Eltwise7
[2018-04-23 20:40:20]  I0423 12:39:45.786401    21 net.cpp:122] Setting up Eltwise7
[2018-04-23 20:40:20]  I0423 12:39:45.786453    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:20]  I0423 12:39:45.786478    21 net.cpp:137] Memory required for data: 504365824
[2018-04-23 20:40:20]  I0423 12:39:45.786523    21 layer_factory.hpp:77] Creating layer ReLU15
[2018-04-23 20:40:20]  I0423 12:39:45.786576    21 net.cpp:84] Creating Layer ReLU15
[2018-04-23 20:40:20]  I0423 12:39:45.786612    21 net.cpp:406] ReLU15 <- Eltwise7
[2018-04-23 20:40:20]  I0423 12:39:45.786659    21 net.cpp:367] ReLU15 -> Eltwise7 (in-place)
[2018-04-23 20:40:20]  I0423 12:39:45.787019    21 net.cpp:122] Setting up ReLU15
[2018-04-23 20:40:20]  I0423 12:39:45.787077    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:20]  I0423 12:39:45.787107    21 net.cpp:137] Memory required for data: 506462976
[2018-04-23 20:40:20]  I0423 12:39:45.787137    21 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
[2018-04-23 20:40:20]  I0423 12:39:45.787179    21 net.cpp:84] Creating Layer Eltwise7_ReLU15_0_split
[2018-04-23 20:40:20]  I0423 12:39:45.787209    21 net.cpp:406] Eltwise7_ReLU15_0_split <- Eltwise7
[2018-04-23 20:40:20]  I0423 12:39:45.787271    21 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
[2018-04-23 20:40:20]  I0423 12:39:45.787328    21 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
[2018-04-23 20:40:20]  I0423 12:39:45.787454    21 net.cpp:122] Setting up Eltwise7_ReLU15_0_split
[2018-04-23 20:40:20]  I0423 12:39:45.787516    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:20]  I0423 12:39:45.787546    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:20]  I0423 12:39:45.787588    21 net.cpp:137] Memory required for data: 510657280
[2018-04-23 20:40:20]  I0423 12:39:45.787624    21 layer_factory.hpp:77] Creating layer Convolution18
[2018-04-23 20:40:20]  I0423 12:39:45.787708    21 net.cpp:84] Creating Layer Convolution18
[2018-04-23 20:40:20]  I0423 12:39:45.787758    21 net.cpp:406] Convolution18 <- Eltwise7_ReLU15_0_split_0
[2018-04-23 20:40:20]  I0423 12:39:45.787812    21 net.cpp:380] Convolution18 -> Convolution18
[2018-04-23 20:40:20]  I0423 12:39:45.790699    21 net.cpp:122] Setting up Convolution18
[2018-04-23 20:40:20]  I0423 12:39:45.790768    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:20]  I0423 12:39:45.790799    21 net.cpp:137] Memory required for data: 512754432
[2018-04-23 20:40:20]  I0423 12:39:45.790848    21 layer_factory.hpp:77] Creating layer BatchNorm18
[2018-04-23 20:40:20]  I0423 12:39:45.790881    21 net.cpp:84] Creating Layer BatchNorm18
[2018-04-23 20:40:20]  I0423 12:39:45.790915    21 net.cpp:406] BatchNorm18 <- Convolution18
[2018-04-23 20:40:20]  I0423 12:39:45.790978    21 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
[2018-04-23 20:40:20]  I0423 12:39:45.791378    21 net.cpp:122] Setting up BatchNorm18
[2018-04-23 20:40:20]  I0423 12:39:45.791432    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:20]  I0423 12:39:45.791460    21 net.cpp:137] Memory required for data: 514851584
[2018-04-23 20:40:20]  I0423 12:39:45.791507    21 layer_factory.hpp:77] Creating layer Scale18
[2018-04-23 20:40:20]  I0423 12:39:45.791599    21 net.cpp:84] Creating Layer Scale18
[2018-04-23 20:40:20]  I0423 12:39:45.791630    21 net.cpp:406] Scale18 <- Convolution18
[2018-04-23 20:40:20]  I0423 12:39:45.791664    21 net.cpp:367] Scale18 -> Convolution18 (in-place)
[2018-04-23 20:40:20]  I0423 12:39:45.791801    21 layer_factory.hpp:77] Creating layer Scale18
[2018-04-23 20:40:20]  I0423 12:39:45.792068    21 net.cpp:122] Setting up Scale18
[2018-04-23 20:40:20]  I0423 12:39:45.792121    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:20]  I0423 12:39:45.792161    21 net.cpp:137] Memory required for data: 516948736
[2018-04-23 20:40:20]  I0423 12:39:45.792198    21 layer_factory.hpp:77] Creating layer ReLU16
[2018-04-23 20:40:20]  I0423 12:39:45.792243    21 net.cpp:84] Creating Layer ReLU16
[2018-04-23 20:40:20]  I0423 12:39:45.792285    21 net.cpp:406] ReLU16 <- Convolution18
[2018-04-23 20:40:20]  I0423 12:39:45.792328    21 net.cpp:367] ReLU16 -> Convolution18 (in-place)
[2018-04-23 20:40:20]  I0423 12:39:45.792939    21 net.cpp:122] Setting up ReLU16
[2018-04-23 20:40:20]  I0423 12:39:45.792999    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:20]  I0423 12:39:45.793040    21 net.cpp:137] Memory required for data: 519045888
[2018-04-23 20:40:20]  I0423 12:39:45.793074    21 layer_factory.hpp:77] Creating layer Convolution19
[2018-04-23 20:40:20]  I0423 12:39:45.793177    21 net.cpp:84] Creating Layer Convolution19
[2018-04-23 20:40:20]  I0423 12:39:45.793215    21 net.cpp:406] Convolution19 <- Convolution18
[2018-04-23 20:40:20]  I0423 12:39:45.793306    21 net.cpp:380] Convolution19 -> Convolution19
[2018-04-23 20:40:20]  I0423 12:39:45.796352    21 net.cpp:122] Setting up Convolution19
[2018-04-23 20:40:20]  I0423 12:39:45.796423    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:20]  I0423 12:39:45.796452    21 net.cpp:137] Memory required for data: 521143040
[2018-04-23 20:40:20]  I0423 12:39:45.796512    21 layer_factory.hpp:77] Creating layer BatchNorm19
[2018-04-23 20:40:20]  I0423 12:39:45.796581    21 net.cpp:84] Creating Layer BatchNorm19
[2018-04-23 20:40:20]  I0423 12:39:45.796620    21 net.cpp:406] BatchNorm19 <- Convolution19
[2018-04-23 20:40:20]  I0423 12:39:45.796655    21 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
[2018-04-23 20:40:20]  I0423 12:39:45.797062    21 net.cpp:122] Setting up BatchNorm19
[2018-04-23 20:40:20]  I0423 12:39:45.797113    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:20]  I0423 12:39:45.797142    21 net.cpp:137] Memory required for data: 523240192
[2018-04-23 20:40:20]  I0423 12:39:45.797291    21 layer_factory.hpp:77] Creating layer Scale19
[2018-04-23 20:40:20]  I0423 12:39:45.797359    21 net.cpp:84] Creating Layer Scale19
[2018-04-23 20:40:20]  I0423 12:39:45.797395    21 net.cpp:406] Scale19 <- Convolution19
[2018-04-23 20:40:20]  I0423 12:39:45.797452    21 net.cpp:367] Scale19 -> Convolution19 (in-place)
[2018-04-23 20:40:20]  I0423 12:39:45.797591    21 layer_factory.hpp:77] Creating layer Scale19
[2018-04-23 20:40:20]  I0423 12:39:45.797911    21 net.cpp:122] Setting up Scale19
[2018-04-23 20:40:21]  I0423 12:39:45.797976    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:21]  I0423 12:39:45.798030    21 net.cpp:137] Memory required for data: 525337344
[2018-04-23 20:40:21]  I0423 12:39:45.798070    21 layer_factory.hpp:77] Creating layer Eltwise8
[2018-04-23 20:40:21]  I0423 12:39:45.798122    21 net.cpp:84] Creating Layer Eltwise8
[2018-04-23 20:40:21]  I0423 12:39:45.798154    21 net.cpp:406] Eltwise8 <- Eltwise7_ReLU15_0_split_1
[2018-04-23 20:40:21]  I0423 12:39:45.798213    21 net.cpp:406] Eltwise8 <- Convolution19
[2018-04-23 20:40:21]  I0423 12:39:45.798246    21 net.cpp:380] Eltwise8 -> Eltwise8
[2018-04-23 20:40:21]  I0423 12:39:45.798351    21 net.cpp:122] Setting up Eltwise8
[2018-04-23 20:40:21]  I0423 12:39:45.798403    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:21]  I0423 12:39:45.798439    21 net.cpp:137] Memory required for data: 527434496
[2018-04-23 20:40:21]  I0423 12:39:45.798475    21 layer_factory.hpp:77] Creating layer ReLU17
[2018-04-23 20:40:21]  I0423 12:39:45.798542    21 net.cpp:84] Creating Layer ReLU17
[2018-04-23 20:40:21]  I0423 12:39:45.798583    21 net.cpp:406] ReLU17 <- Eltwise8
[2018-04-23 20:40:21]  I0423 12:39:45.798642    21 net.cpp:367] ReLU17 -> Eltwise8 (in-place)
[2018-04-23 20:40:21]  I0423 12:39:45.799017    21 net.cpp:122] Setting up ReLU17
[2018-04-23 20:40:21]  I0423 12:39:45.799077    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:21]  I0423 12:39:45.799108    21 net.cpp:137] Memory required for data: 529531648
[2018-04-23 20:40:21]  I0423 12:39:45.799135    21 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
[2018-04-23 20:40:21]  I0423 12:39:45.799178    21 net.cpp:84] Creating Layer Eltwise8_ReLU17_0_split
[2018-04-23 20:40:21]  I0423 12:39:45.799207    21 net.cpp:406] Eltwise8_ReLU17_0_split <- Eltwise8
[2018-04-23 20:40:21]  I0423 12:39:45.799266    21 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
[2018-04-23 20:40:21]  I0423 12:39:45.799340    21 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
[2018-04-23 20:40:21]  I0423 12:39:45.799489    21 net.cpp:122] Setting up Eltwise8_ReLU17_0_split
[2018-04-23 20:40:21]  I0423 12:39:45.799554    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:21]  I0423 12:39:45.799583    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:21]  I0423 12:39:45.799624    21 net.cpp:137] Memory required for data: 533725952
[2018-04-23 20:40:21]  I0423 12:39:45.799671    21 layer_factory.hpp:77] Creating layer Convolution20
[2018-04-23 20:40:21]  I0423 12:39:45.799727    21 net.cpp:84] Creating Layer Convolution20
[2018-04-23 20:40:21]  I0423 12:39:45.799765    21 net.cpp:406] Convolution20 <- Eltwise8_ReLU17_0_split_0
[2018-04-23 20:40:21]  I0423 12:39:45.799815    21 net.cpp:380] Convolution20 -> Convolution20
[2018-04-23 20:40:21]  I0423 12:39:45.805654    21 net.cpp:122] Setting up Convolution20
[2018-04-23 20:40:21]  I0423 12:39:45.805725    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:21]  I0423 12:39:45.805757    21 net.cpp:137] Memory required for data: 535823104
[2018-04-23 20:40:21]  I0423 12:39:45.805788    21 layer_factory.hpp:77] Creating layer BatchNorm20
[2018-04-23 20:40:21]  I0423 12:39:45.805871    21 net.cpp:84] Creating Layer BatchNorm20
[2018-04-23 20:40:21]  I0423 12:39:45.805922    21 net.cpp:406] BatchNorm20 <- Convolution20
[2018-04-23 20:40:21]  I0423 12:39:45.805981    21 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
[2018-04-23 20:40:21]  I0423 12:39:45.806404    21 net.cpp:122] Setting up BatchNorm20
[2018-04-23 20:40:21]  I0423 12:39:45.806458    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:21]  I0423 12:39:45.806510    21 net.cpp:137] Memory required for data: 537920256
[2018-04-23 20:40:21]  I0423 12:39:45.806553    21 layer_factory.hpp:77] Creating layer Scale20
[2018-04-23 20:40:21]  I0423 12:39:45.806601    21 net.cpp:84] Creating Layer Scale20
[2018-04-23 20:40:21]  I0423 12:39:45.806639    21 net.cpp:406] Scale20 <- Convolution20
[2018-04-23 20:40:21]  I0423 12:39:45.806897    21 net.cpp:367] Scale20 -> Convolution20 (in-place)
[2018-04-23 20:40:21]  I0423 12:39:45.807044    21 layer_factory.hpp:77] Creating layer Scale20
[2018-04-23 20:40:21]  I0423 12:39:45.807329    21 net.cpp:122] Setting up Scale20
[2018-04-23 20:40:21]  I0423 12:39:45.807391    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:21]  I0423 12:39:45.807447    21 net.cpp:137] Memory required for data: 540017408
[2018-04-23 20:40:21]  I0423 12:39:45.807488    21 layer_factory.hpp:77] Creating layer ReLU18
[2018-04-23 20:40:21]  I0423 12:39:45.807560    21 net.cpp:84] Creating Layer ReLU18
[2018-04-23 20:40:21]  I0423 12:39:45.807607    21 net.cpp:406] ReLU18 <- Convolution20
[2018-04-23 20:40:21]  I0423 12:39:45.807644    21 net.cpp:367] ReLU18 -> Convolution20 (in-place)
[2018-04-23 20:40:21]  I0423 12:39:45.808049    21 net.cpp:122] Setting up ReLU18
[2018-04-23 20:40:21]  I0423 12:39:45.808118    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:21]  I0423 12:39:45.808173    21 net.cpp:137] Memory required for data: 542114560
[2018-04-23 20:40:21]  I0423 12:39:45.808245    21 layer_factory.hpp:77] Creating layer Convolution21
[2018-04-23 20:40:21]  I0423 12:39:45.808324    21 net.cpp:84] Creating Layer Convolution21
[2018-04-23 20:40:21]  I0423 12:39:45.808369    21 net.cpp:406] Convolution21 <- Convolution20
[2018-04-23 20:40:21]  I0423 12:39:45.808413    21 net.cpp:380] Convolution21 -> Convolution21
[2018-04-23 20:40:21]  I0423 12:39:45.811280    21 net.cpp:122] Setting up Convolution21
[2018-04-23 20:40:21]  I0423 12:39:45.811352    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:21]  I0423 12:39:45.811383    21 net.cpp:137] Memory required for data: 544211712
[2018-04-23 20:40:21]  I0423 12:39:45.811424    21 layer_factory.hpp:77] Creating layer BatchNorm21
[2018-04-23 20:40:21]  I0423 12:39:45.811483    21 net.cpp:84] Creating Layer BatchNorm21
[2018-04-23 20:40:21]  I0423 12:39:45.811549    21 net.cpp:406] BatchNorm21 <- Convolution21
[2018-04-23 20:40:21]  I0423 12:39:45.811607    21 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
[2018-04-23 20:40:21]  I0423 12:39:45.812007    21 net.cpp:122] Setting up BatchNorm21
[2018-04-23 20:40:21]  I0423 12:39:45.812062    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:21]  I0423 12:39:45.812099    21 net.cpp:137] Memory required for data: 546308864
[2018-04-23 20:40:21]  I0423 12:39:45.812150    21 layer_factory.hpp:77] Creating layer Scale21
[2018-04-23 20:40:21]  I0423 12:39:45.812207    21 net.cpp:84] Creating Layer Scale21
[2018-04-23 20:40:21]  I0423 12:39:45.812242    21 net.cpp:406] Scale21 <- Convolution21
[2018-04-23 20:40:21]  I0423 12:39:45.812320    21 net.cpp:367] Scale21 -> Convolution21 (in-place)
[2018-04-23 20:40:21]  I0423 12:39:45.812441    21 layer_factory.hpp:77] Creating layer Scale21
[2018-04-23 20:40:22]  I0423 12:39:45.812731    21 net.cpp:122] Setting up Scale21
[2018-04-23 20:40:22]  I0423 12:39:45.812786    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:22]  I0423 12:39:45.812824    21 net.cpp:137] Memory required for data: 548406016
[2018-04-23 20:40:22]  I0423 12:39:45.812880    21 layer_factory.hpp:77] Creating layer Eltwise9
[2018-04-23 20:40:22]  I0423 12:39:45.812922    21 net.cpp:84] Creating Layer Eltwise9
[2018-04-23 20:40:22]  I0423 12:39:45.812978    21 net.cpp:406] Eltwise9 <- Eltwise8_ReLU17_0_split_1
[2018-04-23 20:40:22]  I0423 12:39:45.813045    21 net.cpp:406] Eltwise9 <- Convolution21
[2018-04-23 20:40:22]  I0423 12:39:45.813132    21 net.cpp:380] Eltwise9 -> Eltwise9
[2018-04-23 20:40:22]  I0423 12:39:45.813253    21 net.cpp:122] Setting up Eltwise9
[2018-04-23 20:40:22]  I0423 12:39:45.813316    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:22]  I0423 12:39:45.813396    21 net.cpp:137] Memory required for data: 550503168
[2018-04-23 20:40:22]  I0423 12:39:45.813443    21 layer_factory.hpp:77] Creating layer ReLU19
[2018-04-23 20:40:22]  I0423 12:39:45.813508    21 net.cpp:84] Creating Layer ReLU19
[2018-04-23 20:40:22]  I0423 12:39:45.813547    21 net.cpp:406] ReLU19 <- Eltwise9
[2018-04-23 20:40:22]  I0423 12:39:45.813616    21 net.cpp:367] ReLU19 -> Eltwise9 (in-place)
[2018-04-23 20:40:22]  I0423 12:39:45.814013    21 net.cpp:122] Setting up ReLU19
[2018-04-23 20:40:22]  I0423 12:39:45.814070    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:22]  I0423 12:39:45.814100    21 net.cpp:137] Memory required for data: 552600320
[2018-04-23 20:40:22]  I0423 12:39:45.814126    21 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
[2018-04-23 20:40:22]  I0423 12:39:45.814177    21 net.cpp:84] Creating Layer Eltwise9_ReLU19_0_split
[2018-04-23 20:40:22]  I0423 12:39:45.814211    21 net.cpp:406] Eltwise9_ReLU19_0_split <- Eltwise9
[2018-04-23 20:40:22]  I0423 12:39:45.814268    21 net.cpp:380] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
[2018-04-23 20:40:22]  I0423 12:39:45.814303    21 net.cpp:380] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
[2018-04-23 20:40:22]  I0423 12:39:45.814451    21 net.cpp:122] Setting up Eltwise9_ReLU19_0_split
[2018-04-23 20:40:22]  I0423 12:39:45.814514    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:22]  I0423 12:39:45.814546    21 net.cpp:129] Top shape: 64 128 8 8 (524288)
[2018-04-23 20:40:22]  I0423 12:39:45.814571    21 net.cpp:137] Memory required for data: 556794624
[2018-04-23 20:40:22]  I0423 12:39:45.814632    21 layer_factory.hpp:77] Creating layer Convolution22
[2018-04-23 20:40:22]  I0423 12:39:45.814689    21 net.cpp:84] Creating Layer Convolution22
[2018-04-23 20:40:22]  I0423 12:39:45.814745    21 net.cpp:406] Convolution22 <- Eltwise9_ReLU19_0_split_0
[2018-04-23 20:40:22]  I0423 12:39:45.814792    21 net.cpp:380] Convolution22 -> Convolution22
[2018-04-23 20:40:22]  I0423 12:39:45.816738    21 net.cpp:122] Setting up Convolution22
[2018-04-23 20:40:22]  I0423 12:39:45.816807    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:22]  I0423 12:39:45.816835    21 net.cpp:137] Memory required for data: 557843200
[2018-04-23 20:40:22]  I0423 12:39:45.816879    21 layer_factory.hpp:77] Creating layer BatchNorm22
[2018-04-23 20:40:22]  I0423 12:39:45.816948    21 net.cpp:84] Creating Layer BatchNorm22
[2018-04-23 20:40:22]  I0423 12:39:45.816983    21 net.cpp:406] BatchNorm22 <- Convolution22
[2018-04-23 20:40:22]  I0423 12:39:45.817023    21 net.cpp:367] BatchNorm22 -> Convolution22 (in-place)
[2018-04-23 20:40:22]  I0423 12:39:45.817438    21 net.cpp:122] Setting up BatchNorm22
[2018-04-23 20:40:22]  I0423 12:39:45.817504    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:22]  I0423 12:39:45.817535    21 net.cpp:137] Memory required for data: 558891776
[2018-04-23 20:40:22]  I0423 12:39:45.817580    21 layer_factory.hpp:77] Creating layer Scale22
[2018-04-23 20:40:22]  I0423 12:39:45.817641    21 net.cpp:84] Creating Layer Scale22
[2018-04-23 20:40:22]  I0423 12:39:45.817685    21 net.cpp:406] Scale22 <- Convolution22
[2018-04-23 20:40:22]  I0423 12:39:45.817749    21 net.cpp:367] Scale22 -> Convolution22 (in-place)
[2018-04-23 20:40:22]  I0423 12:39:45.817895    21 layer_factory.hpp:77] Creating layer Scale22
[2018-04-23 20:40:22]  I0423 12:39:45.818193    21 net.cpp:122] Setting up Scale22
[2018-04-23 20:40:22]  I0423 12:39:45.818259    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:22]  I0423 12:39:45.818300    21 net.cpp:137] Memory required for data: 559940352
[2018-04-23 20:40:22]  I0423 12:39:45.818339    21 layer_factory.hpp:77] Creating layer Convolution23
[2018-04-23 20:40:22]  I0423 12:39:45.818394    21 net.cpp:84] Creating Layer Convolution23
[2018-04-23 20:40:22]  I0423 12:39:45.818442    21 net.cpp:406] Convolution23 <- Eltwise9_ReLU19_0_split_1
[2018-04-23 20:40:22]  I0423 12:39:45.818485    21 net.cpp:380] Convolution23 -> Convolution23
[2018-04-23 20:40:22]  I0423 12:39:45.825826    21 net.cpp:122] Setting up Convolution23
[2018-04-23 20:40:22]  I0423 12:39:45.825907    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:22]  I0423 12:39:45.825942    21 net.cpp:137] Memory required for data: 560988928
[2018-04-23 20:40:22]  I0423 12:39:45.825973    21 layer_factory.hpp:77] Creating layer BatchNorm23
[2018-04-23 20:40:22]  I0423 12:39:45.826004    21 net.cpp:84] Creating Layer BatchNorm23
[2018-04-23 20:40:22]  I0423 12:39:45.826030    21 net.cpp:406] BatchNorm23 <- Convolution23
[2018-04-23 20:40:22]  I0423 12:39:45.826081    21 net.cpp:367] BatchNorm23 -> Convolution23 (in-place)
[2018-04-23 20:40:22]  I0423 12:39:45.826470    21 net.cpp:122] Setting up BatchNorm23
[2018-04-23 20:40:22]  I0423 12:39:45.826537    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:22]  I0423 12:39:45.826565    21 net.cpp:137] Memory required for data: 562037504
[2018-04-23 20:40:22]  I0423 12:39:45.826596    21 layer_factory.hpp:77] Creating layer Scale23
[2018-04-23 20:40:22]  I0423 12:39:45.826653    21 net.cpp:84] Creating Layer Scale23
[2018-04-23 20:40:22]  I0423 12:39:45.826696    21 net.cpp:406] Scale23 <- Convolution23
[2018-04-23 20:40:22]  I0423 12:39:45.826731    21 net.cpp:367] Scale23 -> Convolution23 (in-place)
[2018-04-23 20:40:22]  I0423 12:39:45.826871    21 layer_factory.hpp:77] Creating layer Scale23
[2018-04-23 20:40:22]  I0423 12:39:45.827157    21 net.cpp:122] Setting up Scale23
[2018-04-23 20:40:22]  I0423 12:39:45.827211    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:22]  I0423 12:39:45.827239    21 net.cpp:137] Memory required for data: 563086080
[2018-04-23 20:40:22]  I0423 12:39:45.827267    21 layer_factory.hpp:77] Creating layer ReLU20
[2018-04-23 20:40:23]  I0423 12:39:45.827318    21 net.cpp:84] Creating Layer ReLU20
[2018-04-23 20:40:23]  I0423 12:39:45.827349    21 net.cpp:406] ReLU20 <- Convolution23
[2018-04-23 20:40:23]  I0423 12:39:45.827410    21 net.cpp:367] ReLU20 -> Convolution23 (in-place)
[2018-04-23 20:40:23]  I0423 12:39:45.827993    21 net.cpp:122] Setting up ReLU20
[2018-04-23 20:40:23]  I0423 12:39:45.828058    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:23]  I0423 12:39:45.828088    21 net.cpp:137] Memory required for data: 564134656
[2018-04-23 20:40:23]  I0423 12:39:45.828114    21 layer_factory.hpp:77] Creating layer Convolution24
[2018-04-23 20:40:23]  I0423 12:39:45.828197    21 net.cpp:84] Creating Layer Convolution24
[2018-04-23 20:40:23]  I0423 12:39:45.828233    21 net.cpp:406] Convolution24 <- Convolution23
[2018-04-23 20:40:23]  I0423 12:39:45.828281    21 net.cpp:380] Convolution24 -> Convolution24
[2018-04-23 20:40:23]  I0423 12:39:45.836866    21 net.cpp:122] Setting up Convolution24
[2018-04-23 20:40:23]  I0423 12:39:45.836937    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:23]  I0423 12:39:45.836968    21 net.cpp:137] Memory required for data: 565183232
[2018-04-23 20:40:23]  I0423 12:39:45.837011    21 layer_factory.hpp:77] Creating layer BatchNorm24
[2018-04-23 20:40:23]  I0423 12:39:45.837069    21 net.cpp:84] Creating Layer BatchNorm24
[2018-04-23 20:40:23]  I0423 12:39:45.837126    21 net.cpp:406] BatchNorm24 <- Convolution24
[2018-04-23 20:40:23]  I0423 12:39:45.837157    21 net.cpp:367] BatchNorm24 -> Convolution24 (in-place)
[2018-04-23 20:40:23]  I0423 12:39:45.837647    21 net.cpp:122] Setting up BatchNorm24
[2018-04-23 20:40:23]  I0423 12:39:45.837704    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:23]  I0423 12:39:45.837743    21 net.cpp:137] Memory required for data: 566231808
[2018-04-23 20:40:23]  I0423 12:39:45.837797    21 layer_factory.hpp:77] Creating layer Scale24
[2018-04-23 20:40:23]  I0423 12:39:45.837852    21 net.cpp:84] Creating Layer Scale24
[2018-04-23 20:40:23]  I0423 12:39:45.837906    21 net.cpp:406] Scale24 <- Convolution24
[2018-04-23 20:40:23]  I0423 12:39:45.837965    21 net.cpp:367] Scale24 -> Convolution24 (in-place)
[2018-04-23 20:40:23]  I0423 12:39:45.838124    21 layer_factory.hpp:77] Creating layer Scale24
[2018-04-23 20:40:23]  I0423 12:39:45.838407    21 net.cpp:122] Setting up Scale24
[2018-04-23 20:40:23]  I0423 12:39:45.838467    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:23]  I0423 12:39:45.838526    21 net.cpp:137] Memory required for data: 567280384
[2018-04-23 20:40:23]  I0423 12:39:45.838591    21 layer_factory.hpp:77] Creating layer Eltwise10
[2018-04-23 20:40:23]  I0423 12:39:45.838629    21 net.cpp:84] Creating Layer Eltwise10
[2018-04-23 20:40:23]  I0423 12:39:45.838667    21 net.cpp:406] Eltwise10 <- Convolution22
[2018-04-23 20:40:23]  I0423 12:39:45.838703    21 net.cpp:406] Eltwise10 <- Convolution24
[2018-04-23 20:40:23]  I0423 12:39:45.838762    21 net.cpp:380] Eltwise10 -> Eltwise10
[2018-04-23 20:40:23]  I0423 12:39:45.838855    21 net.cpp:122] Setting up Eltwise10
[2018-04-23 20:40:23]  I0423 12:39:45.838903    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:23]  I0423 12:39:45.838932    21 net.cpp:137] Memory required for data: 568328960
[2018-04-23 20:40:23]  I0423 12:39:45.838984    21 layer_factory.hpp:77] Creating layer ReLU21
[2018-04-23 20:40:23]  I0423 12:39:45.839033    21 net.cpp:84] Creating Layer ReLU21
[2018-04-23 20:40:23]  I0423 12:39:45.839085    21 net.cpp:406] ReLU21 <- Eltwise10
[2018-04-23 20:40:23]  I0423 12:39:45.839128    21 net.cpp:367] ReLU21 -> Eltwise10 (in-place)
[2018-04-23 20:40:23]  I0423 12:39:45.839532    21 net.cpp:122] Setting up ReLU21
[2018-04-23 20:40:23]  I0423 12:39:45.839591    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:23]  I0423 12:39:45.839620    21 net.cpp:137] Memory required for data: 569377536
[2018-04-23 20:40:23]  I0423 12:39:45.839645    21 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
[2018-04-23 20:40:23]  I0423 12:39:45.839694    21 net.cpp:84] Creating Layer Eltwise10_ReLU21_0_split
[2018-04-23 20:40:23]  I0423 12:39:45.839743    21 net.cpp:406] Eltwise10_ReLU21_0_split <- Eltwise10
[2018-04-23 20:40:23]  I0423 12:39:45.839805    21 net.cpp:380] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
[2018-04-23 20:40:23]  I0423 12:39:45.839846    21 net.cpp:380] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
[2018-04-23 20:40:23]  I0423 12:39:45.839967    21 net.cpp:122] Setting up Eltwise10_ReLU21_0_split
[2018-04-23 20:40:23]  I0423 12:39:45.840018    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:23]  I0423 12:39:45.840046    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:23]  I0423 12:39:45.840070    21 net.cpp:137] Memory required for data: 571474688
[2018-04-23 20:40:23]  I0423 12:39:45.840111    21 layer_factory.hpp:77] Creating layer Convolution25
[2018-04-23 20:40:23]  I0423 12:39:45.840163    21 net.cpp:84] Creating Layer Convolution25
[2018-04-23 20:40:23]  I0423 12:39:45.840207    21 net.cpp:406] Convolution25 <- Eltwise10_ReLU21_0_split_0
[2018-04-23 20:40:23]  I0423 12:39:45.840265    21 net.cpp:380] Convolution25 -> Convolution25
[2018-04-23 20:40:23]  I0423 12:39:45.852119    21 net.cpp:122] Setting up Convolution25
[2018-04-23 20:40:23]  I0423 12:39:45.852190    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:23]  I0423 12:39:45.852221    21 net.cpp:137] Memory required for data: 572523264
[2018-04-23 20:40:23]  I0423 12:39:45.852275    21 layer_factory.hpp:77] Creating layer BatchNorm25
[2018-04-23 20:40:23]  I0423 12:39:45.852313    21 net.cpp:84] Creating Layer BatchNorm25
[2018-04-23 20:40:23]  I0423 12:39:45.852356    21 net.cpp:406] BatchNorm25 <- Convolution25
[2018-04-23 20:40:23]  I0423 12:39:45.852489    21 net.cpp:367] BatchNorm25 -> Convolution25 (in-place)
[2018-04-23 20:40:23]  I0423 12:39:45.852902    21 net.cpp:122] Setting up BatchNorm25
[2018-04-23 20:40:23]  I0423 12:39:45.852957    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:23]  I0423 12:39:45.852985    21 net.cpp:137] Memory required for data: 573571840
[2018-04-23 20:40:23]  I0423 12:39:45.853031    21 layer_factory.hpp:77] Creating layer Scale25
[2018-04-23 20:40:23]  I0423 12:39:45.853096    21 net.cpp:84] Creating Layer Scale25
[2018-04-23 20:40:23]  I0423 12:39:45.853133    21 net.cpp:406] Scale25 <- Convolution25
[2018-04-23 20:40:23]  I0423 12:39:45.853171    21 net.cpp:367] Scale25 -> Convolution25 (in-place)
[2018-04-23 20:40:23]  I0423 12:39:45.853299    21 layer_factory.hpp:77] Creating layer Scale25
[2018-04-23 20:40:23]  I0423 12:39:45.853617    21 net.cpp:122] Setting up Scale25
[2018-04-23 20:40:23]  I0423 12:39:45.853675    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:24]  I0423 12:39:45.853713    21 net.cpp:137] Memory required for data: 574620416
[2018-04-23 20:40:24]  I0423 12:39:45.853750    21 layer_factory.hpp:77] Creating layer ReLU22
[2018-04-23 20:40:24]  I0423 12:39:45.853816    21 net.cpp:84] Creating Layer ReLU22
[2018-04-23 20:40:24]  I0423 12:39:45.853849    21 net.cpp:406] ReLU22 <- Convolution25
[2018-04-23 20:40:24]  I0423 12:39:45.853891    21 net.cpp:367] ReLU22 -> Convolution25 (in-place)
[2018-04-23 20:40:24]  I0423 12:39:45.854300    21 net.cpp:122] Setting up ReLU22
[2018-04-23 20:40:24]  I0423 12:39:45.854358    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:24]  I0423 12:39:45.854387    21 net.cpp:137] Memory required for data: 575668992
[2018-04-23 20:40:24]  I0423 12:39:45.854413    21 layer_factory.hpp:77] Creating layer Convolution26
[2018-04-23 20:40:24]  I0423 12:39:45.854511    21 net.cpp:84] Creating Layer Convolution26
[2018-04-23 20:40:24]  I0423 12:39:45.854552    21 net.cpp:406] Convolution26 <- Convolution25
[2018-04-23 20:40:24]  I0423 12:39:45.854595    21 net.cpp:380] Convolution26 -> Convolution26
[2018-04-23 20:40:24]  I0423 12:39:45.863227    21 net.cpp:122] Setting up Convolution26
[2018-04-23 20:40:24]  I0423 12:39:45.863298    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:24]  I0423 12:39:45.863330    21 net.cpp:137] Memory required for data: 576717568
[2018-04-23 20:40:24]  I0423 12:39:45.863373    21 layer_factory.hpp:77] Creating layer BatchNorm26
[2018-04-23 20:40:24]  I0423 12:39:45.863433    21 net.cpp:84] Creating Layer BatchNorm26
[2018-04-23 20:40:24]  I0423 12:39:45.863481    21 net.cpp:406] BatchNorm26 <- Convolution26
[2018-04-23 20:40:24]  I0423 12:39:45.863529    21 net.cpp:367] BatchNorm26 -> Convolution26 (in-place)
[2018-04-23 20:40:24]  I0423 12:39:45.863909    21 net.cpp:122] Setting up BatchNorm26
[2018-04-23 20:40:24]  I0423 12:39:45.863962    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:24]  I0423 12:39:45.864003    21 net.cpp:137] Memory required for data: 577766144
[2018-04-23 20:40:24]  I0423 12:39:45.864044    21 layer_factory.hpp:77] Creating layer Scale26
[2018-04-23 20:40:24]  I0423 12:39:45.864089    21 net.cpp:84] Creating Layer Scale26
[2018-04-23 20:40:24]  I0423 12:39:45.864130    21 net.cpp:406] Scale26 <- Convolution26
[2018-04-23 20:40:24]  I0423 12:39:45.864172    21 net.cpp:367] Scale26 -> Convolution26 (in-place)
[2018-04-23 20:40:24]  I0423 12:39:45.864306    21 layer_factory.hpp:77] Creating layer Scale26
[2018-04-23 20:40:24]  I0423 12:39:45.864616    21 net.cpp:122] Setting up Scale26
[2018-04-23 20:40:24]  I0423 12:39:45.864670    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:24]  I0423 12:39:45.864699    21 net.cpp:137] Memory required for data: 578814720
[2018-04-23 20:40:24]  I0423 12:39:45.864728    21 layer_factory.hpp:77] Creating layer Eltwise11
[2018-04-23 20:40:24]  I0423 12:39:45.864781    21 net.cpp:84] Creating Layer Eltwise11
[2018-04-23 20:40:24]  I0423 12:39:45.864831    21 net.cpp:406] Eltwise11 <- Eltwise10_ReLU21_0_split_1
[2018-04-23 20:40:24]  I0423 12:39:45.864873    21 net.cpp:406] Eltwise11 <- Convolution26
[2018-04-23 20:40:24]  I0423 12:39:45.864907    21 net.cpp:380] Eltwise11 -> Eltwise11
[2018-04-23 20:40:24]  I0423 12:39:45.865022    21 net.cpp:122] Setting up Eltwise11
[2018-04-23 20:40:24]  I0423 12:39:45.865073    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:24]  I0423 12:39:45.865101    21 net.cpp:137] Memory required for data: 579863296
[2018-04-23 20:40:24]  I0423 12:39:45.865135    21 layer_factory.hpp:77] Creating layer ReLU23
[2018-04-23 20:40:24]  I0423 12:39:45.865176    21 net.cpp:84] Creating Layer ReLU23
[2018-04-23 20:40:24]  I0423 12:39:45.865214    21 net.cpp:406] ReLU23 <- Eltwise11
[2018-04-23 20:40:24]  I0423 12:39:45.865273    21 net.cpp:367] ReLU23 -> Eltwise11 (in-place)
[2018-04-23 20:40:24]  I0423 12:39:45.865861    21 net.cpp:122] Setting up ReLU23
[2018-04-23 20:40:24]  I0423 12:39:45.865927    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:24]  I0423 12:39:45.865957    21 net.cpp:137] Memory required for data: 580911872
[2018-04-23 20:40:24]  I0423 12:39:45.866003    21 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
[2018-04-23 20:40:24]  I0423 12:39:45.866055    21 net.cpp:84] Creating Layer Eltwise11_ReLU23_0_split
[2018-04-23 20:40:24]  I0423 12:39:45.866091    21 net.cpp:406] Eltwise11_ReLU23_0_split <- Eltwise11
[2018-04-23 20:40:24]  I0423 12:39:45.866144    21 net.cpp:380] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
[2018-04-23 20:40:24]  I0423 12:39:45.866186    21 net.cpp:380] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
[2018-04-23 20:40:24]  I0423 12:39:45.866338    21 net.cpp:122] Setting up Eltwise11_ReLU23_0_split
[2018-04-23 20:40:24]  I0423 12:39:45.866420    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:24]  I0423 12:39:45.866456    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:24]  I0423 12:39:45.866524    21 net.cpp:137] Memory required for data: 583009024
[2018-04-23 20:40:24]  I0423 12:39:45.866559    21 layer_factory.hpp:77] Creating layer Convolution27
[2018-04-23 20:40:24]  I0423 12:39:45.866616    21 net.cpp:84] Creating Layer Convolution27
[2018-04-23 20:40:24]  I0423 12:39:45.866654    21 net.cpp:406] Convolution27 <- Eltwise11_ReLU23_0_split_0
[2018-04-23 20:40:24]  I0423 12:39:45.866708    21 net.cpp:380] Convolution27 -> Convolution27
[2018-04-23 20:40:24]  I0423 12:39:45.875360    21 net.cpp:122] Setting up Convolution27
[2018-04-23 20:40:24]  I0423 12:39:45.875430    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:24]  I0423 12:39:45.875473    21 net.cpp:137] Memory required for data: 584057600
[2018-04-23 20:40:24]  I0423 12:39:45.875530    21 layer_factory.hpp:77] Creating layer BatchNorm27
[2018-04-23 20:40:24]  I0423 12:39:45.875576    21 net.cpp:84] Creating Layer BatchNorm27
[2018-04-23 20:40:24]  I0423 12:39:45.875612    21 net.cpp:406] BatchNorm27 <- Convolution27
[2018-04-23 20:40:24]  I0423 12:39:45.875653    21 net.cpp:367] BatchNorm27 -> Convolution27 (in-place)
[2018-04-23 20:40:24]  I0423 12:39:45.876068    21 net.cpp:122] Setting up BatchNorm27
[2018-04-23 20:40:24]  I0423 12:39:45.876125    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:24]  I0423 12:39:45.876153    21 net.cpp:137] Memory required for data: 585106176
[2018-04-23 20:40:24]  I0423 12:39:45.876204    21 layer_factory.hpp:77] Creating layer Scale27
[2018-04-23 20:40:24]  I0423 12:39:45.876322    21 net.cpp:84] Creating Layer Scale27
[2018-04-23 20:40:24]  I0423 12:39:45.876372    21 net.cpp:406] Scale27 <- Convolution27
[2018-04-23 20:40:24]  I0423 12:39:45.876405    21 net.cpp:367] Scale27 -> Convolution27 (in-place)
[2018-04-23 20:40:24]  I0423 12:39:45.876545    21 layer_factory.hpp:77] Creating layer Scale27
[2018-04-23 20:40:24]  I0423 12:39:45.876835    21 net.cpp:122] Setting up Scale27
[2018-04-23 20:40:24]  I0423 12:39:45.876885    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:24]  I0423 12:39:45.876914    21 net.cpp:137] Memory required for data: 586154752
[2018-04-23 20:40:24]  I0423 12:39:45.876974    21 layer_factory.hpp:77] Creating layer ReLU24
[2018-04-23 20:40:24]  I0423 12:39:45.877020    21 net.cpp:84] Creating Layer ReLU24
[2018-04-23 20:40:24]  I0423 12:39:45.877054    21 net.cpp:406] ReLU24 <- Convolution27
[2018-04-23 20:40:24]  I0423 12:39:45.877105    21 net.cpp:367] ReLU24 -> Convolution27 (in-place)
[2018-04-23 20:40:24]  I0423 12:39:45.877678    21 net.cpp:122] Setting up ReLU24
[2018-04-23 20:40:25]  I0423 12:39:45.877743    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:25]  I0423 12:39:45.877782    21 net.cpp:137] Memory required for data: 587203328
[2018-04-23 20:40:25]  I0423 12:39:45.877815    21 layer_factory.hpp:77] Creating layer Convolution28
[2018-04-23 20:40:25]  I0423 12:39:45.877871    21 net.cpp:84] Creating Layer Convolution28
[2018-04-23 20:40:25]  I0423 12:39:45.877918    21 net.cpp:406] Convolution28 <- Convolution27
[2018-04-23 20:40:25]  I0423 12:39:45.877970    21 net.cpp:380] Convolution28 -> Convolution28
[2018-04-23 20:40:25]  I0423 12:39:45.886715    21 net.cpp:122] Setting up Convolution28
[2018-04-23 20:40:25]  I0423 12:39:45.886787    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:25]  I0423 12:39:45.886831    21 net.cpp:137] Memory required for data: 588251904
[2018-04-23 20:40:25]  I0423 12:39:45.886871    21 layer_factory.hpp:77] Creating layer BatchNorm28
[2018-04-23 20:40:25]  I0423 12:39:45.886905    21 net.cpp:84] Creating Layer BatchNorm28
[2018-04-23 20:40:25]  I0423 12:39:45.886941    21 net.cpp:406] BatchNorm28 <- Convolution28
[2018-04-23 20:40:25]  I0423 12:39:45.887004    21 net.cpp:367] BatchNorm28 -> Convolution28 (in-place)
[2018-04-23 20:40:25]  I0423 12:39:45.887415    21 net.cpp:122] Setting up BatchNorm28
[2018-04-23 20:40:25]  I0423 12:39:45.887468    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:25]  I0423 12:39:45.887521    21 net.cpp:137] Memory required for data: 589300480
[2018-04-23 20:40:25]  I0423 12:39:45.887563    21 layer_factory.hpp:77] Creating layer Scale28
[2018-04-23 20:40:25]  I0423 12:39:45.887596    21 net.cpp:84] Creating Layer Scale28
[2018-04-23 20:40:25]  I0423 12:39:45.887626    21 net.cpp:406] Scale28 <- Convolution28
[2018-04-23 20:40:25]  I0423 12:39:45.887671    21 net.cpp:367] Scale28 -> Convolution28 (in-place)
[2018-04-23 20:40:25]  I0423 12:39:45.887789    21 layer_factory.hpp:77] Creating layer Scale28
[2018-04-23 20:40:25]  I0423 12:39:45.888077    21 net.cpp:122] Setting up Scale28
[2018-04-23 20:40:25]  I0423 12:39:45.888133    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:25]  I0423 12:39:45.888161    21 net.cpp:137] Memory required for data: 590349056
[2018-04-23 20:40:25]  I0423 12:39:45.888209    21 layer_factory.hpp:77] Creating layer Eltwise12
[2018-04-23 20:40:25]  I0423 12:39:45.888280    21 net.cpp:84] Creating Layer Eltwise12
[2018-04-23 20:40:25]  I0423 12:39:45.888321    21 net.cpp:406] Eltwise12 <- Eltwise11_ReLU23_0_split_1
[2018-04-23 20:40:25]  I0423 12:39:45.888361    21 net.cpp:406] Eltwise12 <- Convolution28
[2018-04-23 20:40:25]  I0423 12:39:45.888397    21 net.cpp:380] Eltwise12 -> Eltwise12
[2018-04-23 20:40:25]  I0423 12:39:45.888535    21 net.cpp:122] Setting up Eltwise12
[2018-04-23 20:40:25]  I0423 12:39:45.888584    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:25]  I0423 12:39:45.888623    21 net.cpp:137] Memory required for data: 591397632
[2018-04-23 20:40:25]  I0423 12:39:45.888655    21 layer_factory.hpp:77] Creating layer ReLU25
[2018-04-23 20:40:25]  I0423 12:39:45.888691    21 net.cpp:84] Creating Layer ReLU25
[2018-04-23 20:40:25]  I0423 12:39:45.888720    21 net.cpp:406] ReLU25 <- Eltwise12
[2018-04-23 20:40:25]  I0423 12:39:45.888787    21 net.cpp:367] ReLU25 -> Eltwise12 (in-place)
[2018-04-23 20:40:25]  I0423 12:39:45.889204    21 net.cpp:122] Setting up ReLU25
[2018-04-23 20:40:25]  I0423 12:39:45.889264    21 net.cpp:129] Top shape: 64 256 4 4 (262144)
[2018-04-23 20:40:25]  I0423 12:39:45.889303    21 net.cpp:137] Memory required for data: 592446208
[2018-04-23 20:40:25]  I0423 12:39:45.889376    21 layer_factory.hpp:77] Creating layer Pooling1
[2018-04-23 20:40:25]  I0423 12:39:45.889438    21 net.cpp:84] Creating Layer Pooling1
[2018-04-23 20:40:25]  I0423 12:39:45.889482    21 net.cpp:406] Pooling1 <- Eltwise12
[2018-04-23 20:40:25]  I0423 12:39:45.889564    21 net.cpp:380] Pooling1 -> Pooling1
[2018-04-23 20:40:25]  I0423 12:39:45.890274    21 net.cpp:122] Setting up Pooling1
[2018-04-23 20:40:25]  I0423 12:39:45.890338    21 net.cpp:129] Top shape: 64 256 1 1 (16384)
[2018-04-23 20:40:25]  I0423 12:39:45.890388    21 net.cpp:137] Memory required for data: 592511744
[2018-04-23 20:40:25]  I0423 12:39:45.890439    21 layer_factory.hpp:77] Creating layer InnerProduct1
[2018-04-23 20:40:25]  I0423 12:39:45.890528    21 net.cpp:84] Creating Layer InnerProduct1
[2018-04-23 20:40:25]  I0423 12:39:45.890588    21 net.cpp:406] InnerProduct1 <- Pooling1
[2018-04-23 20:40:25]  I0423 12:39:45.890640    21 net.cpp:380] InnerProduct1 -> InnerProduct1
[2018-04-23 20:40:25]  I0423 12:39:45.891115    21 net.cpp:122] Setting up InnerProduct1
[2018-04-23 20:40:25]  I0423 12:39:45.891170    21 net.cpp:129] Top shape: 64 100 (6400)
[2018-04-23 20:40:25]  I0423 12:39:45.891211    21 net.cpp:137] Memory required for data: 592537344
[2018-04-23 20:40:25]  I0423 12:39:45.891260    21 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
[2018-04-23 20:40:25]  I0423 12:39:45.891309    21 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
[2018-04-23 20:40:25]  I0423 12:39:45.891351    21 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
[2018-04-23 20:40:25]  I0423 12:39:45.891402    21 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
[2018-04-23 20:40:25]  I0423 12:39:45.891455    21 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
[2018-04-23 20:40:25]  I0423 12:39:45.891619    21 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
[2018-04-23 20:40:25]  I0423 12:39:45.891674    21 net.cpp:129] Top shape: 64 100 (6400)
[2018-04-23 20:40:25]  I0423 12:39:45.891712    21 net.cpp:129] Top shape: 64 100 (6400)
[2018-04-23 20:40:25]  I0423 12:39:45.891743    21 net.cpp:137] Memory required for data: 592588544
[2018-04-23 20:40:25]  I0423 12:39:45.891786    21 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
[2018-04-23 20:40:25]  I0423 12:39:45.891854    21 net.cpp:84] Creating Layer SoftmaxWithLoss1
[2018-04-23 20:40:25]  I0423 12:39:45.891891    21 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
[2018-04-23 20:40:25]  I0423 12:39:45.891944    21 net.cpp:406] SoftmaxWithLoss1 <- label_data_1_split_0
[2018-04-23 20:40:25]  I0423 12:39:45.891996    21 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
[2018-04-23 20:40:25]  I0423 12:39:45.892093    21 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
[2018-04-23 20:40:25]  I0423 12:39:45.892680    21 net.cpp:122] Setting up SoftmaxWithLoss1
[2018-04-23 20:40:25]  I0423 12:39:45.892740    21 net.cpp:129] Top shape: (1)
[2018-04-23 20:40:25]  I0423 12:39:45.892787    21 net.cpp:132]     with loss weight 1
[2018-04-23 20:40:25]  I0423 12:39:45.892868    21 net.cpp:137] Memory required for data: 592588548
[2018-04-23 20:40:25]  I0423 12:39:45.892899    21 layer_factory.hpp:77] Creating layer Accuracy_train
[2018-04-23 20:40:25]  I0423 12:39:45.892967    21 net.cpp:84] Creating Layer Accuracy_train
[2018-04-23 20:40:25]  I0423 12:39:45.893012    21 net.cpp:406] Accuracy_train <- InnerProduct1_InnerProduct1_0_split_1
[2018-04-23 20:40:25]  I0423 12:39:45.893059    21 net.cpp:406] Accuracy_train <- label_data_1_split_1
[2018-04-23 20:40:25]  I0423 12:39:45.893102    21 net.cpp:380] Accuracy_train -> Accuracy_train
[2018-04-23 20:40:25]  I0423 12:39:45.893214    21 net.cpp:122] Setting up Accuracy_train
[2018-04-23 20:40:25]  I0423 12:39:45.893263    21 net.cpp:129] Top shape: (1)
[2018-04-23 20:40:25]  I0423 12:39:45.893293    21 net.cpp:137] Memory required for data: 592588552
[2018-04-23 20:40:26]  I0423 12:39:45.893375    21 net.cpp:200] Accuracy_train does not need backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.893429    21 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.893473    21 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.893532    21 net.cpp:198] InnerProduct1 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.893580    21 net.cpp:198] Pooling1 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.893613    21 net.cpp:198] ReLU25 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.893656    21 net.cpp:198] Eltwise12 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.893702    21 net.cpp:198] Scale28 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.893738    21 net.cpp:198] BatchNorm28 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.893769    21 net.cpp:198] Convolution28 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.893813    21 net.cpp:198] ReLU24 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.893864    21 net.cpp:198] Scale27 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.893903    21 net.cpp:198] BatchNorm27 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.893932    21 net.cpp:198] Convolution27 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.893985    21 net.cpp:198] Eltwise11_ReLU23_0_split needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894029    21 net.cpp:198] ReLU23 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894067    21 net.cpp:198] Eltwise11 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894107    21 net.cpp:198] Scale26 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894148    21 net.cpp:198] BatchNorm26 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894183    21 net.cpp:198] Convolution26 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894222    21 net.cpp:198] ReLU22 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894253    21 net.cpp:198] Scale25 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894300    21 net.cpp:198] BatchNorm25 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894336    21 net.cpp:198] Convolution25 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894369    21 net.cpp:198] Eltwise10_ReLU21_0_split needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894412    21 net.cpp:198] ReLU21 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894444    21 net.cpp:198] Eltwise10 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894484    21 net.cpp:198] Scale24 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894531    21 net.cpp:198] BatchNorm24 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894565    21 net.cpp:198] Convolution24 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894608    21 net.cpp:198] ReLU20 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894652    21 net.cpp:198] Scale23 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894688    21 net.cpp:198] BatchNorm23 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894719    21 net.cpp:198] Convolution23 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894752    21 net.cpp:198] Scale22 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894778    21 net.cpp:198] BatchNorm22 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894804    21 net.cpp:198] Convolution22 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894832    21 net.cpp:198] Eltwise9_ReLU19_0_split needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894865    21 net.cpp:198] ReLU19 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894901    21 net.cpp:198] Eltwise9 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894955    21 net.cpp:198] Scale21 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.894999    21 net.cpp:198] BatchNorm21 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895035    21 net.cpp:198] Convolution21 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895066    21 net.cpp:198] ReLU18 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895099    21 net.cpp:198] Scale20 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895124    21 net.cpp:198] BatchNorm20 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895166    21 net.cpp:198] Convolution20 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895205    21 net.cpp:198] Eltwise8_ReLU17_0_split needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895257    21 net.cpp:198] ReLU17 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895282    21 net.cpp:198] Eltwise8 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895328    21 net.cpp:198] Scale19 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895375    21 net.cpp:198] BatchNorm19 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895402    21 net.cpp:198] Convolution19 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895436    21 net.cpp:198] ReLU16 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895469    21 net.cpp:198] Scale18 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895521    21 net.cpp:198] BatchNorm18 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895550    21 net.cpp:198] Convolution18 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895581    21 net.cpp:198] Eltwise7_ReLU15_0_split needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895627    21 net.cpp:198] ReLU15 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895659    21 net.cpp:198] Eltwise7 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895696    21 net.cpp:198] Scale17 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895740    21 net.cpp:198] BatchNorm17 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895776    21 net.cpp:198] Convolution17 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895808    21 net.cpp:198] ReLU14 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895843    21 net.cpp:198] Scale16 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895880    21 net.cpp:198] BatchNorm16 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895911    21 net.cpp:198] Convolution16 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.895962    21 net.cpp:198] Scale15 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.896006    21 net.cpp:198] BatchNorm15 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.896044    21 net.cpp:198] Convolution15 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.896082    21 net.cpp:198] Eltwise6_ReLU13_0_split needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.896127    21 net.cpp:198] ReLU13 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.896170    21 net.cpp:198] Eltwise6 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.896209    21 net.cpp:198] Scale14 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.896240    21 net.cpp:198] BatchNorm14 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.896289    21 net.cpp:198] Convolution14 needs backward computation.
[2018-04-23 20:40:26]  I0423 12:39:45.896333    21 net.cpp:198] ReLU12 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896361    21 net.cpp:198] Scale13 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896428    21 net.cpp:198] BatchNorm13 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896466    21 net.cpp:198] Convolution13 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896519    21 net.cpp:198] Eltwise5_ReLU11_0_split needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896559    21 net.cpp:198] ReLU11 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896590    21 net.cpp:198] Eltwise5 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896625    21 net.cpp:198] Scale12 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896651    21 net.cpp:198] BatchNorm12 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896692    21 net.cpp:198] Convolution12 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896729    21 net.cpp:198] ReLU10 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896775    21 net.cpp:198] Scale11 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896803    21 net.cpp:198] BatchNorm11 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896834    21 net.cpp:198] Convolution11 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896875    21 net.cpp:198] Eltwise4_ReLU9_0_split needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896901    21 net.cpp:198] ReLU9 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896950    21 net.cpp:198] Eltwise4 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.896996    21 net.cpp:198] Scale10 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897024    21 net.cpp:198] BatchNorm10 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897056    21 net.cpp:198] Convolution10 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897086    21 net.cpp:198] ReLU8 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897130    21 net.cpp:198] Scale9 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897162    21 net.cpp:198] BatchNorm9 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897207    21 net.cpp:198] Convolution9 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897243    21 net.cpp:198] Scale8 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897285    21 net.cpp:198] BatchNorm8 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897321    21 net.cpp:198] Convolution8 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897384    21 net.cpp:198] Eltwise3_ReLU7_0_split needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897420    21 net.cpp:198] ReLU7 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897464    21 net.cpp:198] Eltwise3 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897517    21 net.cpp:198] Scale7 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897545    21 net.cpp:198] BatchNorm7 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897569    21 net.cpp:198] Convolution7 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897613    21 net.cpp:198] ReLU6 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897660    21 net.cpp:198] Scale6 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897699    21 net.cpp:198] BatchNorm6 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897730    21 net.cpp:198] Convolution6 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897779    21 net.cpp:198] Eltwise2_ReLU5_0_split needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897807    21 net.cpp:198] ReLU5 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897838    21 net.cpp:198] Eltwise2 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897873    21 net.cpp:198] Scale5 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897899    21 net.cpp:198] BatchNorm5 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897940    21 net.cpp:198] Convolution5 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.897990    21 net.cpp:198] ReLU4 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898027    21 net.cpp:198] Scale4 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898059    21 net.cpp:198] BatchNorm4 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898108    21 net.cpp:198] Convolution4 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898144    21 net.cpp:198] Eltwise1_ReLU3_0_split needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898177    21 net.cpp:198] ReLU3 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898211    21 net.cpp:198] Eltwise1 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898267    21 net.cpp:198] Scale3 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898305    21 net.cpp:198] BatchNorm3 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898342    21 net.cpp:198] Convolution3 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898366    21 net.cpp:198] ReLU2 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898398    21 net.cpp:198] Scale2 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898434    21 net.cpp:198] BatchNorm2 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898466    21 net.cpp:198] Convolution2 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898545    21 net.cpp:198] Convolution1_ReLU1_0_split needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898579    21 net.cpp:198] ReLU1 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898648    21 net.cpp:198] Scale1 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898684    21 net.cpp:198] BatchNorm1 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898735    21 net.cpp:198] Convolution1 needs backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898762    21 net.cpp:200] label_data_1_split does not need backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898807    21 net.cpp:200] data does not need backward computation.
[2018-04-23 20:40:27]  I0423 12:39:45.898843    21 net.cpp:242] This network produces output Accuracy_train
[2018-04-23 20:40:27]  I0423 12:39:45.898872    21 net.cpp:242] This network produces output SoftmaxWithLoss1
[2018-04-23 20:40:27]  I0423 12:39:45.899039    21 net.cpp:255] Network initialization done.
[2018-04-23 20:40:27]  I0423 12:39:45.900578    21 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: /data/train_val_myres26.prototxt
[2018-04-23 20:40:27]  I0423 12:39:45.900645    21 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
[2018-04-23 20:40:27]  I0423 12:39:45.900720    21 solver.cpp:172] Creating test net (#0) specified by net file: /data/train_val_myres26.prototxt
[2018-04-23 20:40:27]  I0423 12:39:45.900938    21 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
[2018-04-23 20:40:27]  I0423 12:39:45.901057    21 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer Accuracy_train
[2018-04-23 20:40:27]  I0423 12:39:45.901865    21 net.cpp:51] Initializing net from parameters: 
[2018-04-23 20:40:27]  name: "resnet_26"
[2018-04-23 20:40:27]  state {
[2018-04-23 20:40:27]    phase: TEST
[2018-04-23 20:40:27]  }
[2018-04-23 20:40:27]  layer {
[2018-04-23 20:40:27]    name: "data"
[2018-04-23 20:40:27]    type: "ImageData"
[2018-04-23 20:40:27]    top: "data"
[2018-04-23 20:40:28]    top: "label"
[2018-04-23 20:40:28]    include {
[2018-04-23 20:40:28]      phase: TEST
[2018-04-23 20:40:28]    }
[2018-04-23 20:40:28]    transform_param {
[2018-04-23 20:40:28]      scale: 0.00390625
[2018-04-23 20:40:28]      mirror: false
[2018-04-23 20:40:28]    }
[2018-04-23 20:40:28]    image_data_param {
[2018-04-23 20:40:28]      source: "./mydata/val.txt"
[2018-04-23 20:40:28]      batch_size: 32
[2018-04-23 20:40:28]      shuffle: true
[2018-04-23 20:40:28]      new_height: 64
[2018-04-23 20:40:28]      new_width: 64
[2018-04-23 20:40:28]      root_folder: "/data/mydata/"
[2018-04-23 20:40:28]    }
[2018-04-23 20:40:28]  }
[2018-04-23 20:40:28]  layer {
[2018-04-23 20:40:28]    name: "Convolution1"
[2018-04-23 20:40:28]    type: "Convolution"
[2018-04-23 20:40:28]    bottom: "data"
[2018-04-23 20:40:28]    top: "Convolution1"
[2018-04-23 20:40:28]    param {
[2018-04-23 20:40:28]      lr_mult: 1
[2018-04-23 20:40:28]      decay_mult: 1
[2018-04-23 20:40:28]    }
[2018-04-23 20:40:28]    param {
[2018-04-23 20:40:28]      lr_mult: 2
[2018-04-23 20:40:28]      decay_mult: 0
[2018-04-23 20:40:28]    }
[2018-04-23 20:40:28]    convolution_param {
[2018-04-23 20:40:28]      num_output: 32
[2018-04-23 20:40:28]      pad: 2
[2018-04-23 20:40:28]      kernel_size: 5
[2018-04-23 20:40:28]      stride: 2
[2018-04-23 20:40:28]      weight_filler {
[2018-04-23 20:40:28]        type: "xavier"
[2018-04-23 20:40:28]      }
[2018-04-23 20:40:28]      bias_filler {
[2018-04-23 20:40:28]        type: "constant"
[2018-04-23 20:40:28]        value: 0
[2018-04-23 20:40:28]      }
[2018-04-23 20:40:28]    }
[2018-04-23 20:40:28]  }
[2018-04-23 20:40:28]  layer {
[2018-04-23 20:40:28]    name: "BatchNorm1"
[2018-04-23 20:40:28]    type: "BatchNorm"
[2018-04-23 20:40:28]    bottom: "Convolution1"
[2018-04-23 20:40:28]    top: "Convolution1"
[2018-04-23 20:40:28]    param {
[2018-04-23 20:40:28]      lr_mult: 0
[2018-04-23 20:40:28]      decay_mult: 0
[2018-04-23 20:40:28]    }
[2018-04-23 20:40:28]    param {
[2018-04-23 20:40:28]      lr_mult: 0
[2018-04-23 20:40:28]      decay_mult: 0
[2018-04-23 20:40:28]    }
[2018-04-23 20:40:28]    param {
[2018-04-23 20:40:28]      lr_mult: 0
[2018-04-23 20:40:28]      decay_mult: 0
[2018-04-23 20:40:28]    }
[2018-04-23 20:40:28]  }
[2018-04-23 20:40:28]  layer {
[2018-04-23 20:40:28]    name: "Scale1"
[2018-04-23 20:40:28]    type: "Scale"
[2018-04-23 20:40:28]    bottom: "Convolution1"
[2018-04-23 20:40:28]    top: "Convolution1"
[2018-04-23 20:40:28]    scale_param {
[2018-04-23 20:40:28]      bias_term: true
[2018-04-23 20:40:28]    }
[2018-04-23 20:40:28]  }
[2018-04-23 20:40:28]  layer {
[2018-04-23 20:40:28]    name: "ReLU1"
[2018-04-23 20:40:28]    type: "ReLU"
[2018-04-23 20:40:29]    bottom: "Convolution1"
[2018-04-23 20:40:29]    top: "Convolution1"
[2018-04-23 20:40:29]  }
[2018-04-23 20:40:29]  layer {
[2018-04-23 20:40:29]    name: "Convolution2"
[2018-04-23 20:40:29]    type: "Convolution"
[2018-04-23 20:40:29]    bottom: "Convolution1"
[2018-04-23 20:40:29]    top: "Convolution2"
[2018-04-23 20:40:29]    param {
[2018-04-23 20:40:29]      lr_mult: 1
[2018-04-23 20:40:29]      decay_mult: 1
[2018-04-23 20:40:29]    }
[2018-04-23 20:40:29]    param {
[2018-04-23 20:40:29]      lr_mult: 2
[2018-04-23 20:40:29]      decay_mult: 0
[2018-04-23 20:40:29]    }
[2018-04-23 20:40:29]    convolution_param {
[2018-04-23 20:40:29]      num_output: 32
[2018-04-23 20:40:29]      pad: 1
[2018-04-23 20:40:29]      kernel_size: 3
[2018-04-23 20:40:29]      stride: 1
[2018-04-23 20:40:29]      weight_filler {
[2018-04-23 20:40:29]        type: "xavier"
[2018-04-23 20:40:29]      }
[2018-04-23 20:40:29]      bias_filler {
[2018-04-23 20:40:29]        type: "constant"
[2018-04-23 20:40:29]        value: 0
[2018-04-23 20:40:29]      }
[2018-04-23 20:40:29]    }
[2018-04-23 20:40:29]  }
[2018-04-23 20:40:29]  layer {
[2018-04-23 20:40:29]    name: "BatchNorm2"
[2018-04-23 20:40:29]    type: "BatchNorm"
[2018-04-23 20:40:29]    bottom: "Convolution2"
[2018-04-23 20:40:29]    top: "Convolution2"
[2018-04-23 20:40:29]    param {
[2018-04-23 20:40:29]      lr_mult: 0
[2018-04-23 20:40:29]      decay_mult: 0
[2018-04-23 20:40:29]    }
[2018-04-23 20:40:29]    param {
[2018-04-23 20:40:29]      lr_mult: 0
[2018-04-23 20:40:29]      decay_mult: 0
[2018-04-23 20:40:29]    }
[2018-04-23 20:40:29]    param {
[2018-04-23 20:40:29]      lr_mult: 0
[2018-04-23 20:40:29]      decay_mult: 0
[2018-04-23 20:40:29]    }
[2018-04-23 20:40:29]  }
[2018-04-23 20:40:29]  layer {
[2018-04-23 20:40:29]    name: "Scale2"
[2018-04-23 20:40:29]    type: "Scale"
[2018-04-23 20:40:29]    bottom: "Convolution2"
[2018-04-23 20:40:29]    top: "Convolution2"
[2018-04-23 20:40:29]    scale_param {
[2018-04-23 20:40:29]      bias_term: true
[2018-04-23 20:40:29]    }
[2018-04-23 20:40:29]  }
[2018-04-23 20:40:29]  layer {
[2018-04-23 20:40:29]    name: "ReLU2"
[2018-04-23 20:40:29]    type: "ReLU"
[2018-04-23 20:40:29]    bottom: "Convolution2"
[2018-04-23 20:40:29]    top: "Convolution2"
[2018-04-23 20:40:29]  }
[2018-04-23 20:40:29]  layer {
[2018-04-23 20:40:29]    name: "Convolution3"
[2018-04-23 20:40:29]    type: "Convolution"
[2018-04-23 20:40:29]    bottom: "Convolution2"
[2018-04-23 20:40:29]    top: "Convolution3"
[2018-04-23 20:40:29]    param {
[2018-04-23 20:40:29]      lr_mult: 1
[2018-04-23 20:40:29]      decay_mult: 1
[2018-04-23 20:40:29]    }
[2018-04-23 20:40:29]    param {
[2018-04-23 20:40:29]      lr_mult: 2
[2018-04-23 20:40:29]      decay_mult: 0
[2018-04-23 20:40:29]    }
[2018-04-23 20:40:29]    convolution_param {
[2018-04-23 20:40:29]      num_output: 32
[2018-04-23 20:40:29]      pad: 1
[2018-04-23 20:40:29]      kernel_size: 3
[2018-04-23 20:40:30]      stride: 1
[2018-04-23 20:40:30]      weight_filler {
[2018-04-23 20:40:30]        type: "xavier"
[2018-04-23 20:40:30]      }
[2018-04-23 20:40:30]      bias_filler {
[2018-04-23 20:40:30]        type: "constant"
[2018-04-23 20:40:30]        value: 0
[2018-04-23 20:40:30]      }
[2018-04-23 20:40:30]    }
[2018-04-23 20:40:30]  }
[2018-04-23 20:40:30]  layer {
[2018-04-23 20:40:30]    name: "BatchNorm3"
[2018-04-23 20:40:30]    type: "BatchNorm"
[2018-04-23 20:40:30]    bottom: "Convolution3"
[2018-04-23 20:40:30]    top: "Convolution3"
[2018-04-23 20:40:30]    param {
[2018-04-23 20:40:30]      lr_mult: 0
[2018-04-23 20:40:30]      decay_mult: 0
[2018-04-23 20:40:30]    }
[2018-04-23 20:40:30]    param {
[2018-04-23 20:40:30]      lr_mult: 0
[2018-04-23 20:40:30]      decay_mult: 0
[2018-04-23 20:40:30]    }
[2018-04-23 20:40:30]    param {
[2018-04-23 20:40:30]      lr_mult: 0
[2018-04-23 20:40:30]      decay_mult: 0
[2018-04-23 20:40:30]    }
[2018-04-23 20:40:30]  }
[2018-04-23 20:40:30]  layer {
[2018-04-23 20:40:30]    name: "Scale3"
[2018-04-23 20:40:30]    type: "Scale"
[2018-04-23 20:40:30]    bottom: "Convolution3"
[2018-04-23 20:40:30]    top: "Convolution3"
[2018-04-23 20:40:30]    scale_param {
[2018-04-23 20:40:30]      bias_term: true
[2018-04-23 20:40:30]    }
[2018-04-23 20:40:30]  }
[2018-04-23 20:40:30]  layer {
[2018-04-23 20:40:30]    name: "Eltwise1"
[2018-04-23 20:40:30]    type: "Eltwise"
[2018-04-23 20:40:30]    bottom: "Convolution1"
[2018-04-23 20:40:30]    bottom: "Convolution3"
[2018-04-23 20:40:30]    top: "Eltwise1"
[2018-04-23 20:40:30]    eltwise_param {
[2018-04-23 20:40:30]      operation: SUM
[2018-04-23 20:40:30]    }
[2018-04-23 20:40:30]  }
[2018-04-23 20:40:30]  layer {
[2018-04-23 20:40:30]    name: "ReLU3"
[2018-04-23 20:40:30]    type: "ReLU"
[2018-04-23 20:40:30]    bottom: "Eltwise1"
[2018-04-23 20:40:30]    top: "Eltwise1"
[2018-04-23 20:40:30]  }
[2018-04-23 20:40:30]  layer {
[2018-04-23 20:40:30]    name: "Convolution4"
[2018-04-23 20:40:30]    type: "Convolution"
[2018-04-23 20:40:30]    bottom: "Eltwise1"
[2018-04-23 20:40:30]    top: "Convolution4"
[2018-04-23 20:40:30]    param {
[2018-04-23 20:40:30]      lr_mult: 1
[2018-04-23 20:40:30]      decay_mult: 1
[2018-04-23 20:40:30]    }
[2018-04-23 20:40:30]    param {
[2018-04-23 20:40:30]      lr_mult: 2
[2018-04-23 20:40:30]      decay_mult: 0
[2018-04-23 20:40:30]    }
[2018-04-23 20:40:30]    convolution_param {
[2018-04-23 20:40:30]      num_output: 32
[2018-04-23 20:40:30]      pad: 1
[2018-04-23 20:40:30]      kernel_size: 3
[2018-04-23 20:40:30]      stride: 1
[2018-04-23 20:40:30]      weight_filler {
[2018-04-23 20:40:30]        type: "xavier"
[2018-04-23 20:40:30]      }
[2018-04-23 20:40:30]      bias_filler {
[2018-04-23 20:40:30]        type: "constant"
[2018-04-23 20:40:31]        value: 0
[2018-04-23 20:40:31]      }
[2018-04-23 20:40:31]    }
[2018-04-23 20:40:31]  }
[2018-04-23 20:40:31]  layer {
[2018-04-23 20:40:31]    name: "BatchNorm4"
[2018-04-23 20:40:31]    type: "BatchNorm"
[2018-04-23 20:40:31]    bottom: "Convolution4"
[2018-04-23 20:40:31]    top: "Convolution4"
[2018-04-23 20:40:31]    param {
[2018-04-23 20:40:31]      lr_mult: 0
[2018-04-23 20:40:31]      decay_mult: 0
[2018-04-23 20:40:31]    }
[2018-04-23 20:40:31]    param {
[2018-04-23 20:40:31]      lr_mult: 0
[2018-04-23 20:40:31]      decay_mult: 0
[2018-04-23 20:40:31]    }
[2018-04-23 20:40:31]    param {
[2018-04-23 20:40:31]      lr_mult: 0
[2018-04-23 20:40:31]      decay_mult: 0
[2018-04-23 20:40:31]    }
[2018-04-23 20:40:31]  }
[2018-04-23 20:40:31]  layer {
[2018-04-23 20:40:31]    name: "Scale4"
[2018-04-23 20:40:31]    type: "Scale"
[2018-04-23 20:40:31]    bottom: "Convolution4"
[2018-04-23 20:40:31]    top: "Convolution4"
[2018-04-23 20:40:31]    scale_param {
[2018-04-23 20:40:31]      bias_term: true
[2018-04-23 20:40:31]    }
[2018-04-23 20:40:31]  }
[2018-04-23 20:40:31]  layer {
[2018-04-23 20:40:31]    name: "ReLU4"
[2018-04-23 20:40:31]    type: "ReLU"
[2018-04-23 20:40:31]    bottom: "Convolution4"
[2018-04-23 20:40:31]    top: "Convolution4"
[2018-04-23 20:40:31]  }
[2018-04-23 20:40:31]  layer {
[2018-04-23 20:40:31]    name: "Convolution5"
[2018-04-23 20:40:31]    type: "Convolution"
[2018-04-23 20:40:31]    bottom: "Convolution4"
[2018-04-23 20:40:31]    top: "Convolution5"
[2018-04-23 20:40:31]    param {
[2018-04-23 20:40:31]      lr_mult: 1
[2018-04-23 20:40:31]      decay_mult: 1
[2018-04-23 20:40:31]    }
[2018-04-23 20:40:31]    param {
[2018-04-23 20:40:31]      lr_mult: 2
[2018-04-23 20:40:31]      decay_mult: 0
[2018-04-23 20:40:31]    }
[2018-04-23 20:40:31]    convolution_param {
[2018-04-23 20:40:31]      num_output: 32
[2018-04-23 20:40:31]      pad: 1
[2018-04-23 20:40:31]      kernel_size: 3
[2018-04-23 20:40:31]      stride: 1
[2018-04-23 20:40:31]      weight_filler {
[2018-04-23 20:40:31]        type: "xavier"
[2018-04-23 20:40:31]      }
[2018-04-23 20:40:31]      bias_filler {
[2018-04-23 20:40:31]        type: "constant"
[2018-04-23 20:40:31]        value: 0
[2018-04-23 20:40:31]      }
[2018-04-23 20:40:31]    }
[2018-04-23 20:40:31]  }
[2018-04-23 20:40:31]  layer {
[2018-04-23 20:40:31]    name: "BatchNorm5"
[2018-04-23 20:40:31]    type: "BatchNorm"
[2018-04-23 20:40:32]    bottom: "Convolution5"
[2018-04-23 20:40:32]    top: "Convolution5"
[2018-04-23 20:40:32]    param {
[2018-04-23 20:40:32]      lr_mult: 0
[2018-04-23 20:40:32]      decay_mult: 0
[2018-04-23 20:40:32]    }
[2018-04-23 20:40:32]    param {
[2018-04-23 20:40:32]      lr_mult: 0
[2018-04-23 20:40:32]      decay_mult: 0
[2018-04-23 20:40:32]    }
[2018-04-23 20:40:32]    param {
[2018-04-23 20:40:32]      lr_mult: 0
[2018-04-23 20:40:32]      decay_mult: 0
[2018-04-23 20:40:32]    }
[2018-04-23 20:40:32]  }
[2018-04-23 20:40:32]  layer {
[2018-04-23 20:40:32]    name: "Scale5"
[2018-04-23 20:40:32]    type: "Scale"
[2018-04-23 20:40:32]    bottom: "Convolution5"
[2018-04-23 20:40:32]    top: "Convolution5"
[2018-04-23 20:40:32]    scale_param {
[2018-04-23 20:40:32]      bias_term: true
[2018-04-23 20:40:32]    }
[2018-04-23 20:40:32]  }
[2018-04-23 20:40:32]  layer {
[2018-04-23 20:40:32]    name: "Eltwise2"
[2018-04-23 20:40:32]    type: "Eltwise"
[2018-04-23 20:40:32]    bottom: "Eltwise1"
[2018-04-23 20:40:32]    bottom: "Convolution5"
[2018-04-23 20:40:32]    top: "Eltwise2"
[2018-04-23 20:40:32]    eltwise_param {
[2018-04-23 20:40:32]      operation: SUM
[2018-04-23 20:40:32]    }
[2018-04-23 20:40:32]  }
[2018-04-23 20:40:32]  layer {
[2018-04-23 20:40:32]    name: "ReLU5"
[2018-04-23 20:40:32]    type: "ReLU"
[2018-04-23 20:40:32]    bottom: "Eltwise2"
[2018-04-23 20:40:32]    top: "Eltwise2"
[2018-04-23 20:40:32]  }
[2018-04-23 20:40:32]  layer {
[2018-04-23 20:40:32]    name: "Convolution6"
[2018-04-23 20:40:32]    type: "Convolution"
[2018-04-23 20:40:32]    bottom: "Eltwise2"
[2018-04-23 20:40:32]    top: "Convolution6"
[2018-04-23 20:40:32]    param {
[2018-04-23 20:40:32]      lr_mult: 1
[2018-04-23 20:40:32]      decay_mult: 1
[2018-04-23 20:40:32]    }
[2018-04-23 20:40:32]    param {
[2018-04-23 20:40:32]      lr_mult: 2
[2018-04-23 20:40:32]      decay_mult: 0
[2018-04-23 20:40:32]    }
[2018-04-23 20:40:32]    convolution_param {
[2018-04-23 20:40:32]      num_output: 32
[2018-04-23 20:40:32]      pad: 1
[2018-04-23 20:40:32]      kernel_size: 3
[2018-04-23 20:40:32]      stride: 1
[2018-04-23 20:40:32]      weight_filler {
[2018-04-23 20:40:32]        type: "xavier"
[2018-04-23 20:40:32]      }
[2018-04-23 20:40:33]      bias_filler {
[2018-04-23 20:40:33]        type: "constant"
[2018-04-23 20:40:33]        value: 0
[2018-04-23 20:40:33]      }
[2018-04-23 20:40:33]    }
[2018-04-23 20:40:33]  }
[2018-04-23 20:40:33]  layer {
[2018-04-23 20:40:33]    name: "BatchNorm6"
[2018-04-23 20:40:33]    type: "BatchNorm"
[2018-04-23 20:40:33]    bottom: "Convolution6"
[2018-04-23 20:40:33]    top: "Convolution6"
[2018-04-23 20:40:33]    param {
[2018-04-23 20:40:33]      lr_mult: 0
[2018-04-23 20:40:33]      decay_mult: 0
[2018-04-23 20:40:33]    }
[2018-04-23 20:40:33]    param {
[2018-04-23 20:40:33]      lr_mult: 0
[2018-04-23 20:40:33]      decay_mult: 0
[2018-04-23 20:40:33]    }
[2018-04-23 20:40:33]    param {
[2018-04-23 20:40:33]      lr_mult: 0
[2018-04-23 20:40:33]      decay_mult: 0
[2018-04-23 20:40:33]    }
[2018-04-23 20:40:33]  }
[2018-04-23 20:40:33]  layer {
[2018-04-23 20:40:33]    name: "Scale6"
[2018-04-23 20:40:33]    type: "Scale"
[2018-04-23 20:40:33]    bottom: "Convolution6"
[2018-04-23 20:40:33]    top: "Convolution6"
[2018-04-23 20:40:33]    scale_param {
[2018-04-23 20:40:33]      bias_term: true
[2018-04-23 20:40:33]    }
[2018-04-23 20:40:33]  }
[2018-04-23 20:40:33]  layer {
[2018-04-23 20:40:33]    name: "ReLU6"
[2018-04-23 20:40:33]    type: "ReLU"
[2018-04-23 20:40:33]    bottom: "Convolution6"
[2018-04-23 20:40:33]    top: "Convolution6"
[2018-04-23 20:40:33]  }
[2018-04-23 20:40:33]  layer {
[2018-04-23 20:40:33]    name: "Convolution7"
[2018-04-23 20:40:33]    type: "Convolution"
[2018-04-23 20:40:33]    bottom: "Convolution6"
[2018-04-23 20:40:33]    top: "Convolution7"
[2018-04-23 20:40:33]    param {
[2018-04-23 20:40:33]      lr_mult: 1
[2018-04-23 20:40:33]      decay_mult: 1
[2018-04-23 20:40:33]    }
[2018-04-23 20:40:33]    param {
[2018-04-23 20:40:33]      lr_mult: 2
[2018-04-23 20:40:33]      decay_mult: 0
[2018-04-23 20:40:33]    }
[2018-04-23 20:40:33]    convolution_param {
[2018-04-23 20:40:34]      num_output: 32
[2018-04-23 20:40:34]      pad: 1
[2018-04-23 20:40:34]      kernel_size: 3
[2018-04-23 20:40:34]      stride: 1
[2018-04-23 20:40:34]      weight_filler {
[2018-04-23 20:40:34]        type: "xavier"
[2018-04-23 20:40:34]      }
[2018-04-23 20:40:34]      bias_filler {
[2018-04-23 20:40:34]        type: "constant"
[2018-04-23 20:40:34]        value: 0
[2018-04-23 20:40:34]      }
[2018-04-23 20:40:34]    }
[2018-04-23 20:40:34]  }
[2018-04-23 20:40:34]  layer {
[2018-04-23 20:40:34]    name: "BatchNorm7"
[2018-04-23 20:40:34]    type: "BatchNorm"
[2018-04-23 20:40:34]    bottom: "Convolution7"
[2018-04-23 20:40:34]    top: "Convolution7"
[2018-04-23 20:40:34]    param {
[2018-04-23 20:40:34]      lr_mult: 0
[2018-04-23 20:40:34]      decay_mult: 0
[2018-04-23 20:40:34]    }
[2018-04-23 20:40:34]    param {
[2018-04-23 20:40:34]      lr_mult: 0
[2018-04-23 20:40:34]      decay_mult: 0
[2018-04-23 20:40:34]    }
[2018-04-23 20:40:34]    param {
[2018-04-23 20:40:34]      lr_mult: 0
[2018-04-23 20:40:34]      decay_mult: 0
[2018-04-23 20:40:34]    }
[2018-04-23 20:40:34]  }
[2018-04-23 20:40:34]  layer {
[2018-04-23 20:40:34]    name: "Scale7"
[2018-04-23 20:40:34]    type: "Scale"
[2018-04-23 20:40:34]    bottom: "Convolution7"
[2018-04-23 20:40:34]    top: "Convolution7"
[2018-04-23 20:40:34]    scale_param {
[2018-04-23 20:40:34]      bias_term: true
[2018-04-23 20:40:34]    }
[2018-04-23 20:40:34]  }
[2018-04-23 20:40:34]  layer {
[2018-04-23 20:40:34]    name: "Eltwise3"
[2018-04-23 20:40:34]    type: "Eltwise"
[2018-04-23 20:40:34]    bottom: "Eltwise2"
[2018-04-23 20:40:34]    bottom: "Convolution7"
[2018-04-23 20:40:34]    top: "Eltwise3"
[2018-04-23 20:40:34]    eltwise_param {
[2018-04-23 20:40:34]      operation: SUM
[2018-04-23 20:40:34]    }
[2018-04-23 20:40:34]  }
[2018-04-23 20:40:34]  layer {
[2018-04-23 20:40:34]    name: "ReLU7"
[2018-04-23 20:40:34]    type: "ReLU"
[2018-04-23 20:40:34]    bottom: "Eltwise3"
[2018-04-23 20:40:34]    top: "Eltwise3"
[2018-04-23 20:40:34]  }
[2018-04-23 20:40:34]  layer {
[2018-04-23 20:40:34]    name: "Convolution8"
[2018-04-23 20:40:34]    type: "Convolution"
[2018-04-23 20:40:34]    bottom: "Eltwise3"
[2018-04-23 20:40:34]    top: "Convolution8"
[2018-04-23 20:40:35]    param {
[2018-04-23 20:40:35]      lr_mult: 1
[2018-04-23 20:40:35]      decay_mult: 1
[2018-04-23 20:40:35]    }
[2018-04-23 20:40:35]    param {
[2018-04-23 20:40:35]      lr_mult: 2
[2018-04-23 20:40:35]      decay_mult: 0
[2018-04-23 20:40:35]    }
[2018-04-23 20:40:35]    convolution_param {
[2018-04-23 20:40:35]      num_output: 64
[2018-04-23 20:40:35]      pad: 0
[2018-04-23 20:40:35]      kernel_size: 1
[2018-04-23 20:40:35]      stride: 2
[2018-04-23 20:40:35]      weight_filler {
[2018-04-23 20:40:35]        type: "xavier"
[2018-04-23 20:40:35]      }
[2018-04-23 20:40:35]      bias_filler {
[2018-04-23 20:40:35]        type: "constant"
[2018-04-23 20:40:35]        value: 0
[2018-04-23 20:40:35]      }
[2018-04-23 20:40:35]    }
[2018-04-23 20:40:35]  }
[2018-04-23 20:40:35]  layer {
[2018-04-23 20:40:35]    name: "BatchNorm8"
[2018-04-23 20:40:35]    type: "BatchNorm"
[2018-04-23 20:40:35]    bottom: "Convolution8"
[2018-04-23 20:40:35]    top: "Convolution8"
[2018-04-23 20:40:35]    param {
[2018-04-23 20:40:35]      lr_mult: 0
[2018-04-23 20:40:35]      decay_mult: 0
[2018-04-23 20:40:35]    }
[2018-04-23 20:40:35]    param {
[2018-04-23 20:40:35]      lr_mult: 0
[2018-04-23 20:40:35]      decay_mult: 0
[2018-04-23 20:40:35]    }
[2018-04-23 20:40:35]    param {
[2018-04-23 20:40:35]      lr_mult: 0
[2018-04-23 20:40:35]      decay_mult: 0
[2018-04-23 20:40:35]    }
[2018-04-23 20:40:35]  }
[2018-04-23 20:40:35]  layer {
[2018-04-23 20:40:35]    name: "Scale8"
[2018-04-23 20:40:35]    type: "Scale"
[2018-04-23 20:40:35]    bottom: "Convolution8"
[2018-04-23 20:40:35]    top: "Convolution8"
[2018-04-23 20:40:35]    scale_param {
[2018-04-23 20:40:35]      bias_term: true
[2018-04-23 20:40:35]    }
[2018-04-23 20:40:35]  }
[2018-04-23 20:40:35]  layer {
[2018-04-23 20:40:35]    name: "Convolution9"
[2018-04-23 20:40:35]    type: "Convolution"
[2018-04-23 20:40:35]    bottom: "Eltwise3"
[2018-04-23 20:40:35]    top: "Convolution9"
[2018-04-23 20:40:35]    param {
[2018-04-23 20:40:35]      lr_mult: 1
[2018-04-23 20:40:35]      decay_mult: 1
[2018-04-23 20:40:35]    }
[2018-04-23 20:40:36]    param {
[2018-04-23 20:40:36]      lr_mult: 2
[2018-04-23 20:40:36]      decay_mult: 0
[2018-04-23 20:40:36]    }
[2018-04-23 20:40:36]    convolution_param {
[2018-04-23 20:40:36]      num_output: 64
[2018-04-23 20:40:36]      pad: 1
[2018-04-23 20:40:36]      kernel_size: 3
[2018-04-23 20:40:36]      stride: 2
[2018-04-23 20:40:36]      weight_filler {
[2018-04-23 20:40:36]        type: "xavier"
[2018-04-23 20:40:36]      }
[2018-04-23 20:40:36]      bias_filler {
[2018-04-23 20:40:36]        type: "constant"
[2018-04-23 20:40:36]        value: 0
[2018-04-23 20:40:36]      }
[2018-04-23 20:40:36]    }
[2018-04-23 20:40:36]  }
[2018-04-23 20:40:36]  layer {
[2018-04-23 20:40:36]    name: "BatchNorm9"
[2018-04-23 20:40:36]    type: "BatchNorm"
[2018-04-23 20:40:36]    bottom: "Convolution9"
[2018-04-23 20:40:36]    top: "Convolution9"
[2018-04-23 20:40:36]    param {
[2018-04-23 20:40:36]      lr_mult: 0
[2018-04-23 20:40:36]      decay_mult: 0
[2018-04-23 20:40:36]    }
[2018-04-23 20:40:36]    param {
[2018-04-23 20:40:36]      lr_mult: 0
[2018-04-23 20:40:36]      decay_mult: 0
[2018-04-23 20:40:36]    }
[2018-04-23 20:40:36]    param {
[2018-04-23 20:40:36]      lr_mult: 0
[2018-04-23 20:40:36]      decay_mult: 0
[2018-04-23 20:40:36]    }
[2018-04-23 20:40:36]  }
[2018-04-23 20:40:36]  layer {
[2018-04-23 20:40:36]    name: "Scale9"
[2018-04-23 20:40:36]    type: "Scale"
[2018-04-23 20:40:36]    bottom: "Convolution9"
[2018-04-23 20:40:36]    top: "Convolution9"
[2018-04-23 20:40:36]    scale_param {
[2018-04-23 20:40:36]      bias_term: true
[2018-04-23 20:40:36]    }
[2018-04-23 20:40:36]  }
[2018-04-23 20:40:36]  layer {
[2018-04-23 20:40:36]    name: "ReLU8"
[2018-04-23 20:40:36]    type: "ReLU"
[2018-04-23 20:40:36]    bottom: "Convolution9"
[2018-04-23 20:40:36]    top: "Convolution9"
[2018-04-23 20:40:36]  }
[2018-04-23 20:40:36]  layer {
[2018-04-23 20:40:36]    name: "Convolution10"
[2018-04-23 20:40:36]    type: "Convolution"
[2018-04-23 20:40:36]    bottom: "Convolution9"
[2018-04-23 20:40:36]    top: "Convolution10"
[2018-04-23 20:40:36]    param {
[2018-04-23 20:40:36]      lr_mult: 1
[2018-04-23 20:40:36]      decay_mult: 1
[2018-04-23 20:40:36]    }
[2018-04-23 20:40:36]    param {
[2018-04-23 20:40:36]      lr_mult: 2
[2018-04-23 20:40:36]      decay_mult: 0
[2018-04-23 20:40:36]    }
[2018-04-23 20:40:36]    convolution_param {
[2018-04-23 20:40:36]      num_output: 64
[2018-04-23 20:40:36]      pad: 1
[2018-04-23 20:40:36]      kernel_size: 3
[2018-04-23 20:40:36]      stride: 1
[2018-04-23 20:40:37]      weight_filler {
[2018-04-23 20:40:37]        type: "xavier"
[2018-04-23 20:40:37]      }
[2018-04-23 20:40:37]      bias_filler {
[2018-04-23 20:40:37]        type: "constant"
[2018-04-23 20:40:37]        value: 0
[2018-04-23 20:40:37]      }
[2018-04-23 20:40:37]    }
[2018-04-23 20:40:37]  }
[2018-04-23 20:40:37]  layer {
[2018-04-23 20:40:37]    name: "BatchNorm10"
[2018-04-23 20:40:37]    type: "BatchNorm"
[2018-04-23 20:40:37]    bottom: "Convolution10"
[2018-04-23 20:40:37]    top: "Convolution10"
[2018-04-23 20:40:37]    param {
[2018-04-23 20:40:37]      lr_mult: 0
[2018-04-23 20:40:37]      decay_mult: 0
[2018-04-23 20:40:37]    }
[2018-04-23 20:40:37]    param {
[2018-04-23 20:40:37]      lr_mult: 0
[2018-04-23 20:40:37]      decay_mult: 0
[2018-04-23 20:40:37]    }
[2018-04-23 20:40:37]    param {
[2018-04-23 20:40:37]      lr_mult: 0
[2018-04-23 20:40:37]      decay_mult: 0
[2018-04-23 20:40:37]    }
[2018-04-23 20:40:37]  }
[2018-04-23 20:40:37]  layer {
[2018-04-23 20:40:37]    name: "Scale10"
[2018-04-23 20:40:37]    type: "Scale"
[2018-04-23 20:40:37]    bottom: "Convolution10"
[2018-04-23 20:40:37]    top: "Convolution10"
[2018-04-23 20:40:37]    scale_param {
[2018-04-23 20:40:37]      bias_term: true
[2018-04-23 20:40:37]    }
[2018-04-23 20:40:37]  }
[2018-04-23 20:40:37]  layer {
[2018-04-23 20:40:37]    name: "Eltwise4"
[2018-04-23 20:40:37]    type: "Eltwise"
[2018-04-23 20:40:37]    bottom: "Convolution8"
[2018-04-23 20:40:37]    bottom: "Convolution10"
[2018-04-23 20:40:37]    top: "Eltwise4"
[2018-04-23 20:40:37]    eltwise_param {
[2018-04-23 20:40:37]      operation: SUM
[2018-04-23 20:40:37]    }
[2018-04-23 20:40:37]  }
[2018-04-23 20:40:37]  layer {
[2018-04-23 20:40:37]    name: "ReLU9"
[2018-04-23 20:40:37]    type: "ReLU"
[2018-04-23 20:40:37]    bottom: "Eltwise4"
[2018-04-23 20:40:37]    top: "Eltwise4"
[2018-04-23 20:40:37]  }
[2018-04-23 20:40:38]  layer {
[2018-04-23 20:40:38]    name: "Convolution11"
[2018-04-23 20:40:38]    type: "Convolution"
[2018-04-23 20:40:38]    bottom: "Eltwise4"
[2018-04-23 20:40:38]    top: "Convolution11"
[2018-04-23 20:40:38]    param {
[2018-04-23 20:40:38]      lr_mult: 1
[2018-04-23 20:40:38]      decay_mult: 1
[2018-04-23 20:40:38]    }
[2018-04-23 20:40:38]    param {
[2018-04-23 20:40:38]      lr_mult: 2
[2018-04-23 20:40:38]      decay_mult: 0
[2018-04-23 20:40:38]    }
[2018-04-23 20:40:38]    convolution_param {
[2018-04-23 20:40:38]      num_output: 64
[2018-04-23 20:40:38]      pad: 1
[2018-04-23 20:40:38]      kernel_size: 3
[2018-04-23 20:40:38]      stride: 1
[2018-04-23 20:40:38]      weight_filler {
[2018-04-23 20:40:38]        type: "xavier"
[2018-04-23 20:40:38]      }
[2018-04-23 20:40:38]      bias_filler {
[2018-04-23 20:40:38]        type: "constant"
[2018-04-23 20:40:38]        value: 0
[2018-04-23 20:40:38]      }
[2018-04-23 20:40:38]    }
[2018-04-23 20:40:38]  }
[2018-04-23 20:40:38]  layer {
[2018-04-23 20:40:38]    name: "BatchNorm11"
[2018-04-23 20:40:38]    type: "BatchNorm"
[2018-04-23 20:40:38]    bottom: "Convolution11"
[2018-04-23 20:40:38]    top: "Convolution11"
[2018-04-23 20:40:38]    param {
[2018-04-23 20:40:38]      lr_mult: 0
[2018-04-23 20:40:38]      decay_mult: 0
[2018-04-23 20:40:38]    }
[2018-04-23 20:40:38]    param {
[2018-04-23 20:40:38]      lr_mult: 0
[2018-04-23 20:40:38]      decay_mult: 0
[2018-04-23 20:40:38]    }
[2018-04-23 20:40:38]    param {
[2018-04-23 20:40:38]      lr_mult: 0
[2018-04-23 20:40:38]      decay_mult: 0
[2018-04-23 20:40:38]    }
[2018-04-23 20:40:38]  }
[2018-04-23 20:40:38]  layer {
[2018-04-23 20:40:38]    name: "Scale11"
[2018-04-23 20:40:38]    type: "Scale"
[2018-04-23 20:40:38]    bottom: "Convolution11"
[2018-04-23 20:40:38]    top: "Convolution11"
[2018-04-23 20:40:38]    scale_param {
[2018-04-23 20:40:38]      bias_term: true
[2018-04-23 20:40:38]    }
[2018-04-23 20:40:38]  }
[2018-04-23 20:40:38]  layer {
[2018-04-23 20:40:38]    name: "ReLU10"
[2018-04-23 20:40:38]    type: "ReLU"
[2018-04-23 20:40:38]    bottom: "Convolution11"
[2018-04-23 20:40:38]    top: "Convolution11"
[2018-04-23 20:40:38]  }
[2018-04-23 20:40:38]  layer {
[2018-04-23 20:40:38]    name: "Convolution12"
[2018-04-23 20:40:38]    type: "Convolution"
[2018-04-23 20:40:38]    bottom: "Convolution11"
[2018-04-23 20:40:38]    top: "Convolution12"
[2018-04-23 20:40:38]    param {
[2018-04-23 20:40:38]      lr_mult: 1
[2018-04-23 20:40:39]      decay_mult: 1
[2018-04-23 20:40:39]    }
[2018-04-23 20:40:39]    param {
[2018-04-23 20:40:39]      lr_mult: 2
[2018-04-23 20:40:39]      decay_mult: 0
[2018-04-23 20:40:39]    }
[2018-04-23 20:40:39]    convolution_param {
[2018-04-23 20:40:39]      num_output: 64
[2018-04-23 20:40:39]      pad: 1
[2018-04-23 20:40:39]      kernel_size: 3
[2018-04-23 20:40:39]      stride: 1
[2018-04-23 20:40:39]      weight_filler {
[2018-04-23 20:40:39]        type: "xavier"
[2018-04-23 20:40:39]      }
[2018-04-23 20:40:39]      bias_filler {
[2018-04-23 20:40:39]        type: "constant"
[2018-04-23 20:40:39]        value: 0
[2018-04-23 20:40:39]      }
[2018-04-23 20:40:39]    }
[2018-04-23 20:40:39]  }
[2018-04-23 20:40:39]  layer {
[2018-04-23 20:40:39]    name: "BatchNorm12"
[2018-04-23 20:40:39]    type: "BatchNorm"
[2018-04-23 20:40:39]    bottom: "Convolution12"
[2018-04-23 20:40:39]    top: "Convolution12"
[2018-04-23 20:40:39]    param {
[2018-04-23 20:40:39]      lr_mult: 0
[2018-04-23 20:40:39]      decay_mult: 0
[2018-04-23 20:40:39]    }
[2018-04-23 20:40:39]    param {
[2018-04-23 20:40:39]      lr_mult: 0
[2018-04-23 20:40:39]      decay_mult: 0
[2018-04-23 20:40:39]    }
[2018-04-23 20:40:39]    param {
[2018-04-23 20:40:39]      lr_mult: 0
[2018-04-23 20:40:39]      decay_mult: 0
[2018-04-23 20:40:39]    }
[2018-04-23 20:40:39]  }
[2018-04-23 20:40:39]  layer {
[2018-04-23 20:40:39]    name: "Scale12"
[2018-04-23 20:40:39]    type: "Scale"
[2018-04-23 20:40:39]    bottom: "Convolution12"
[2018-04-23 20:40:39]    top: "Convolution12"
[2018-04-23 20:40:39]    scale_param {
[2018-04-23 20:40:39]      bias_term: true
[2018-04-23 20:40:39]    }
[2018-04-23 20:40:39]  }
[2018-04-23 20:40:39]  layer {
[2018-04-23 20:40:39]    name: "Eltwise5"
[2018-04-23 20:40:39]    type: "Eltwise"
[2018-04-23 20:40:39]    bottom: "Eltwise4"
[2018-04-23 20:40:39]    bottom: "Convolution12"
[2018-04-23 20:40:39]    top: "Eltwise5"
[2018-04-23 20:40:39]    eltwise_param {
[2018-04-23 20:40:39]      operation: SUM
[2018-04-23 20:40:39]    }
[2018-04-23 20:40:39]  }
[2018-04-23 20:40:39]  layer {
[2018-04-23 20:40:39]    name: "ReLU11"
[2018-04-23 20:40:39]    type: "ReLU"
[2018-04-23 20:40:39]    bottom: "Eltwise5"
[2018-04-23 20:40:39]    top: "Eltwise5"
[2018-04-23 20:40:39]  }
[2018-04-23 20:40:39]  layer {
[2018-04-23 20:40:40]    name: "Convolution13"
[2018-04-23 20:40:40]    type: "Convolution"
[2018-04-23 20:40:40]    bottom: "Eltwise5"
[2018-04-23 20:40:40]    top: "Convolution13"
[2018-04-23 20:40:40]    param {
[2018-04-23 20:40:40]      lr_mult: 1
[2018-04-23 20:40:40]      decay_mult: 1
[2018-04-23 20:40:40]    }
[2018-04-23 20:40:40]    param {
[2018-04-23 20:40:40]      lr_mult: 2
[2018-04-23 20:40:40]      decay_mult: 0
[2018-04-23 20:40:40]    }
[2018-04-23 20:40:40]    convolution_param {
[2018-04-23 20:40:40]      num_output: 64
[2018-04-23 20:40:40]      pad: 1
[2018-04-23 20:40:40]      kernel_size: 3
[2018-04-23 20:40:40]      stride: 1
[2018-04-23 20:40:40]      weight_filler {
[2018-04-23 20:40:40]        type: "xavier"
[2018-04-23 20:40:40]      }
[2018-04-23 20:40:40]      bias_filler {
[2018-04-23 20:40:40]        type: "constant"
[2018-04-23 20:40:40]        value: 0
[2018-04-23 20:40:40]      }
[2018-04-23 20:40:40]    }
[2018-04-23 20:40:40]  }
[2018-04-23 20:40:40]  layer {
[2018-04-23 20:40:40]    name: "BatchNorm13"
[2018-04-23 20:40:40]    type: "BatchNorm"
[2018-04-23 20:40:40]    bottom: "Convolution13"
[2018-04-23 20:40:40]    top: "Convolution13"
[2018-04-23 20:40:40]    param {
[2018-04-23 20:40:40]      lr_mult: 0
[2018-04-23 20:40:40]      decay_mult: 0
[2018-04-23 20:40:40]    }
[2018-04-23 20:40:40]    param {
[2018-04-23 20:40:40]      lr_mult: 0
[2018-04-23 20:40:40]      decay_mult: 0
[2018-04-23 20:40:40]    }
[2018-04-23 20:40:40]    param {
[2018-04-23 20:40:40]      lr_mult: 0
[2018-04-23 20:40:40]      decay_mult: 0
[2018-04-23 20:40:40]    }
[2018-04-23 20:40:40]  }
[2018-04-23 20:40:40]  layer {
[2018-04-23 20:40:40]    name: "Scale13"
[2018-04-23 20:40:40]    type: "Scale"
[2018-04-23 20:40:40]    bottom: "Convolution13"
[2018-04-23 20:40:40]    top: "Convolution13"
[2018-04-23 20:40:40]    scale_param {
[2018-04-23 20:40:40]      bias_term: true
[2018-04-23 20:40:40]    }
[2018-04-23 20:40:40]  }
[2018-04-23 20:40:40]  layer {
[2018-04-23 20:40:40]    name: "ReLU12"
[2018-04-23 20:40:40]    type: "ReLU"
[2018-04-23 20:40:40]    bottom: "Convolution13"
[2018-04-23 20:40:40]    top: "Convolution13"
[2018-04-23 20:40:40]  }
[2018-04-23 20:40:40]  layer {
[2018-04-23 20:40:40]    name: "Convolution14"
[2018-04-23 20:40:41]    type: "Convolution"
[2018-04-23 20:40:41]    bottom: "Convolution13"
[2018-04-23 20:40:41]    top: "Convolution14"
[2018-04-23 20:40:41]    param {
[2018-04-23 20:40:41]      lr_mult: 1
[2018-04-23 20:40:41]      decay_mult: 1
[2018-04-23 20:40:41]    }
[2018-04-23 20:40:41]    param {
[2018-04-23 20:40:41]      lr_mult: 2
[2018-04-23 20:40:41]      decay_mult: 0
[2018-04-23 20:40:41]    }
[2018-04-23 20:40:41]    convolution_param {
[2018-04-23 20:40:41]      num_output: 64
[2018-04-23 20:40:41]      pad: 1
[2018-04-23 20:40:41]      kernel_size: 3
[2018-04-23 20:40:41]      stride: 1
[2018-04-23 20:40:41]      weight_filler {
[2018-04-23 20:40:41]        type: "xavier"
[2018-04-23 20:40:41]      }
[2018-04-23 20:40:41]      bias_filler {
[2018-04-23 20:40:41]        type: "constant"
[2018-04-23 20:40:41]        value: 0
[2018-04-23 20:40:41]      }
[2018-04-23 20:40:41]    }
[2018-04-23 20:40:41]  }
[2018-04-23 20:40:41]  layer {
[2018-04-23 20:40:41]    name: "BatchNorm14"
[2018-04-23 20:40:41]    type: "BatchNorm"
[2018-04-23 20:40:41]    bottom: "Convolution14"
[2018-04-23 20:40:41]    top: "Convolution14"
[2018-04-23 20:40:41]    param {
[2018-04-23 20:40:41]      lr_mult: 0
[2018-04-23 20:40:41]      decay_mult: 0
[2018-04-23 20:40:41]    }
[2018-04-23 20:40:41]    param {
[2018-04-23 20:40:41]      lr_mult: 0
[2018-04-23 20:40:41]      decay_mult: 0
[2018-04-23 20:40:41]    }
[2018-04-23 20:40:41]    param {
[2018-04-23 20:40:41]      lr_mult: 0
[2018-04-23 20:40:41]      decay_mult: 0
[2018-04-23 20:40:41]    }
[2018-04-23 20:40:41]  }
[2018-04-23 20:40:41]  layer {
[2018-04-23 20:40:41]    name: "Scale14"
[2018-04-23 20:40:41]    type: "Scale"
[2018-04-23 20:40:41]    bottom: "Convolution14"
[2018-04-23 20:40:41]    top: "Convolution14"
[2018-04-23 20:40:41]    scale_param {
[2018-04-23 20:40:41]      bias_term: true
[2018-04-23 20:40:41]    }
[2018-04-23 20:40:41]  }
[2018-04-23 20:40:41]  layer {
[2018-04-23 20:40:41]    name: "Eltwise6"
[2018-04-23 20:40:41]    type: "Eltwise"
[2018-04-23 20:40:41]    bottom: "Eltwise5"
[2018-04-23 20:40:41]    bottom: "Convolution14"
[2018-04-23 20:40:41]    top: "Eltwise6"
[2018-04-23 20:40:41]    eltwise_param {
[2018-04-23 20:40:41]      operation: SUM
[2018-04-23 20:40:41]    }
[2018-04-23 20:40:41]  }
[2018-04-23 20:40:42]  layer {
[2018-04-23 20:40:42]    name: "ReLU13"
[2018-04-23 20:40:42]    type: "ReLU"
[2018-04-23 20:40:42]    bottom: "Eltwise6"
[2018-04-23 20:40:42]    top: "Eltwise6"
[2018-04-23 20:40:42]  }
[2018-04-23 20:40:42]  layer {
[2018-04-23 20:40:42]    name: "Convolution15"
[2018-04-23 20:40:42]    type: "Convolution"
[2018-04-23 20:40:42]    bottom: "Eltwise6"
[2018-04-23 20:40:42]    top: "Convolution15"
[2018-04-23 20:40:42]    param {
[2018-04-23 20:40:42]      lr_mult: 1
[2018-04-23 20:40:42]      decay_mult: 1
[2018-04-23 20:40:42]    }
[2018-04-23 20:40:42]    param {
[2018-04-23 20:40:42]      lr_mult: 2
[2018-04-23 20:40:42]      decay_mult: 0
[2018-04-23 20:40:42]    }
[2018-04-23 20:40:42]    convolution_param {
[2018-04-23 20:40:42]      num_output: 128
[2018-04-23 20:40:42]      pad: 0
[2018-04-23 20:40:42]      kernel_size: 1
[2018-04-23 20:40:42]      stride: 2
[2018-04-23 20:40:42]      weight_filler {
[2018-04-23 20:40:42]        type: "xavier"
[2018-04-23 20:40:42]      }
[2018-04-23 20:40:42]      bias_filler {
[2018-04-23 20:40:42]        type: "constant"
[2018-04-23 20:40:42]        value: 0
[2018-04-23 20:40:42]      }
[2018-04-23 20:40:42]    }
[2018-04-23 20:40:42]  }
[2018-04-23 20:40:42]  layer {
[2018-04-23 20:40:42]    name: "BatchNorm15"
[2018-04-23 20:40:42]    type: "BatchNorm"
[2018-04-23 20:40:42]    bottom: "Convolution15"
[2018-04-23 20:40:42]    top: "Convolution15"
[2018-04-23 20:40:42]    param {
[2018-04-23 20:40:42]      lr_mult: 0
[2018-04-23 20:40:42]      decay_mult: 0
[2018-04-23 20:40:42]    }
[2018-04-23 20:40:42]    param {
[2018-04-23 20:40:42]      lr_mult: 0
[2018-04-23 20:40:42]      decay_mult: 0
[2018-04-23 20:40:42]    }
[2018-04-23 20:40:42]    param {
[2018-04-23 20:40:42]      lr_mult: 0
[2018-04-23 20:40:42]      decay_mult: 0
[2018-04-23 20:40:42]    }
[2018-04-23 20:40:42]  }
[2018-04-23 20:40:42]  layer {
[2018-04-23 20:40:42]    name: "Scale15"
[2018-04-23 20:40:42]    type: "Scale"
[2018-04-23 20:40:42]    bottom: "Convolution15"
[2018-04-23 20:40:42]    top: "Convolution15"
[2018-04-23 20:40:42]    scale_param {
[2018-04-23 20:40:42]      bias_term: true
[2018-04-23 20:40:42]    }
[2018-04-23 20:40:42]  }
[2018-04-23 20:40:42]  layer {
[2018-04-23 20:40:42]    name: "Convolution16"
[2018-04-23 20:40:42]    type: "Convolution"
[2018-04-23 20:40:42]    bottom: "Eltwise6"
[2018-04-23 20:40:42]    top: "Convolution16"
[2018-04-23 20:40:42]    param {
[2018-04-23 20:40:43]      lr_mult: 1
[2018-04-23 20:40:43]      decay_mult: 1
[2018-04-23 20:40:43]    }
[2018-04-23 20:40:43]    param {
[2018-04-23 20:40:43]      lr_mult: 2
[2018-04-23 20:40:43]      decay_mult: 0
[2018-04-23 20:40:43]    }
[2018-04-23 20:40:43]    convolution_param {
[2018-04-23 20:40:43]      num_output: 128
[2018-04-23 20:40:43]      pad: 1
[2018-04-23 20:40:43]      kernel_size: 3
[2018-04-23 20:40:43]      stride: 2
[2018-04-23 20:40:43]      weight_filler {
[2018-04-23 20:40:43]        type: "xavier"
[2018-04-23 20:40:43]      }
[2018-04-23 20:40:43]      bias_filler {
[2018-04-23 20:40:43]        type: "constant"
[2018-04-23 20:40:43]        value: 0
[2018-04-23 20:40:43]      }
[2018-04-23 20:40:43]    }
[2018-04-23 20:40:43]  }
[2018-04-23 20:40:43]  layer {
[2018-04-23 20:40:43]    name: "BatchNorm16"
[2018-04-23 20:40:43]    type: "BatchNorm"
[2018-04-23 20:40:43]    bottom: "Convolution16"
[2018-04-23 20:40:43]    top: "Convolution16"
[2018-04-23 20:40:43]    param {
[2018-04-23 20:40:43]      lr_mult: 0
[2018-04-23 20:40:43]      decay_mult: 0
[2018-04-23 20:40:43]    }
[2018-04-23 20:40:43]    param {
[2018-04-23 20:40:43]      lr_mult: 0
[2018-04-23 20:40:43]      decay_mult: 0
[2018-04-23 20:40:43]    }
[2018-04-23 20:40:43]    param {
[2018-04-23 20:40:43]      lr_mult: 0
[2018-04-23 20:40:43]      decay_mult: 0
[2018-04-23 20:40:43]    }
[2018-04-23 20:40:43]  }
[2018-04-23 20:40:43]  layer {
[2018-04-23 20:40:43]    name: "Scale16"
[2018-04-23 20:40:43]    type: "Scale"
[2018-04-23 20:40:43]    bottom: "Convolution16"
[2018-04-23 20:40:43]    top: "Convolution16"
[2018-04-23 20:40:43]    scale_param {
[2018-04-23 20:40:43]      bias_term: true
[2018-04-23 20:40:43]    }
[2018-04-23 20:40:43]  }
[2018-04-23 20:40:43]  layer {
[2018-04-23 20:40:43]    name: "ReLU14"
[2018-04-23 20:40:43]    type: "ReLU"
[2018-04-23 20:40:43]    bottom: "Convolution16"
[2018-04-23 20:40:43]    top: "Convolution16"
[2018-04-23 20:40:43]  }
[2018-04-23 20:40:43]  layer {
[2018-04-23 20:40:43]    name: "Convolution17"
[2018-04-23 20:40:43]    type: "Convolution"
[2018-04-23 20:40:43]    bottom: "Convolution16"
[2018-04-23 20:40:43]    top: "Convolution17"
[2018-04-23 20:40:43]    param {
[2018-04-23 20:40:43]      lr_mult: 1
[2018-04-23 20:40:43]      decay_mult: 1
[2018-04-23 20:40:43]    }
[2018-04-23 20:40:43]    param {
[2018-04-23 20:40:43]      lr_mult: 2
[2018-04-23 20:40:44]      decay_mult: 0
[2018-04-23 20:40:44]    }
[2018-04-23 20:40:44]    convolution_param {
[2018-04-23 20:40:44]      num_output: 128
[2018-04-23 20:40:44]      pad: 1
[2018-04-23 20:40:44]      kernel_size: 3
[2018-04-23 20:40:44]      stride: 1
[2018-04-23 20:40:44]      weight_filler {
[2018-04-23 20:40:44]        type: "xavier"
[2018-04-23 20:40:44]      }
[2018-04-23 20:40:44]      bias_filler {
[2018-04-23 20:40:44]        type: "constant"
[2018-04-23 20:40:44]        value: 0
[2018-04-23 20:40:44]      }
[2018-04-23 20:40:44]    }
[2018-04-23 20:40:44]  }
[2018-04-23 20:40:44]  layer {
[2018-04-23 20:40:44]    name: "BatchNorm17"
[2018-04-23 20:40:44]    type: "BatchNorm"
[2018-04-23 20:40:44]    bottom: "Convolution17"
[2018-04-23 20:40:44]    top: "Convolution17"
[2018-04-23 20:40:44]    param {
[2018-04-23 20:40:44]      lr_mult: 0
[2018-04-23 20:40:44]      decay_mult: 0
[2018-04-23 20:40:44]    }
[2018-04-23 20:40:44]    param {
[2018-04-23 20:40:44]      lr_mult: 0
[2018-04-23 20:40:44]      decay_mult: 0
[2018-04-23 20:40:44]    }
[2018-04-23 20:40:44]    param {
[2018-04-23 20:40:44]      lr_mult: 0
[2018-04-23 20:40:44]      decay_mult: 0
[2018-04-23 20:40:44]    }
[2018-04-23 20:40:44]  }
[2018-04-23 20:40:44]  layer {
[2018-04-23 20:40:44]    name: "Scale17"
[2018-04-23 20:40:44]    type: "Scale"
[2018-04-23 20:40:44]    bottom: "Convolution17"
[2018-04-23 20:40:44]    top: "Convolution17"
[2018-04-23 20:40:44]    scale_param {
[2018-04-23 20:40:44]      bias_term: true
[2018-04-23 20:40:44]    }
[2018-04-23 20:40:44]  }
[2018-04-23 20:40:44]  layer {
[2018-04-23 20:40:44]    name: "Eltwise7"
[2018-04-23 20:40:44]    type: "Eltwise"
[2018-04-23 20:40:44]    bottom: "Convolution15"
[2018-04-23 20:40:44]    bottom: "Convolution17"
[2018-04-23 20:40:44]    top: "Eltwise7"
[2018-04-23 20:40:44]    eltwise_param {
[2018-04-23 20:40:44]      operation: SUM
[2018-04-23 20:40:44]    }
[2018-04-23 20:40:44]  }
[2018-04-23 20:40:44]  layer {
[2018-04-23 20:40:44]    name: "ReLU15"
[2018-04-23 20:40:44]    type: "ReLU"
[2018-04-23 20:40:44]    bottom: "Eltwise7"
[2018-04-23 20:40:44]    top: "Eltwise7"
[2018-04-23 20:40:44]  }
[2018-04-23 20:40:44]  layer {
[2018-04-23 20:40:44]    name: "Convolution18"
[2018-04-23 20:40:44]    type: "Convolution"
[2018-04-23 20:40:44]    bottom: "Eltwise7"
[2018-04-23 20:40:44]    top: "Convolution18"
[2018-04-23 20:40:44]    param {
[2018-04-23 20:40:44]      lr_mult: 1
[2018-04-23 20:40:44]      decay_mult: 1
[2018-04-23 20:40:44]    }
[2018-04-23 20:40:44]    param {
[2018-04-23 20:40:45]      lr_mult: 2
[2018-04-23 20:40:45]      decay_mult: 0
[2018-04-23 20:40:45]    }
[2018-04-23 20:40:45]    convolution_param {
[2018-04-23 20:40:45]      num_output: 128
[2018-04-23 20:40:45]      pad: 1
[2018-04-23 20:40:45]      kernel_size: 3
[2018-04-23 20:40:45]      stride: 1
[2018-04-23 20:40:45]      weight_filler {
[2018-04-23 20:40:45]        type: "xavier"
[2018-04-23 20:40:45]      }
[2018-04-23 20:40:45]      bias_filler {
[2018-04-23 20:40:45]        type: "constant"
[2018-04-23 20:40:45]        value: 0
[2018-04-23 20:40:45]      }
[2018-04-23 20:40:45]    }
[2018-04-23 20:40:45]  }
[2018-04-23 20:40:45]  layer {
[2018-04-23 20:40:45]    name: "BatchNorm18"
[2018-04-23 20:40:45]    type: "BatchNorm"
[2018-04-23 20:40:45]    bottom: "Convolution18"
[2018-04-23 20:40:45]    top: "Convolution18"
[2018-04-23 20:40:45]    param {
[2018-04-23 20:40:45]      lr_mult: 0
[2018-04-23 20:40:45]      decay_mult: 0
[2018-04-23 20:40:45]    }
[2018-04-23 20:40:45]    param {
[2018-04-23 20:40:45]      lr_mult: 0
[2018-04-23 20:40:45]      decay_mult: 0
[2018-04-23 20:40:45]    }
[2018-04-23 20:40:45]    param {
[2018-04-23 20:40:45]      lr_mult: 0
[2018-04-23 20:40:45]      decay_mult: 0
[2018-04-23 20:40:45]    }
[2018-04-23 20:40:45]  }
[2018-04-23 20:40:45]  layer {
[2018-04-23 20:40:45]    name: "Scale18"
[2018-04-23 20:40:45]    type: "Scale"
[2018-04-23 20:40:45]    bottom: "Convolution18"
[2018-04-23 20:40:45]    top: "Convolution18"
[2018-04-23 20:40:45]    scale_param {
[2018-04-23 20:40:45]      bias_term: true
[2018-04-23 20:40:45]    }
[2018-04-23 20:40:45]  }
[2018-04-23 20:40:45]  layer {
[2018-04-23 20:40:45]    name: "ReLU16"
[2018-04-23 20:40:45]    type: "ReLU"
[2018-04-23 20:40:45]    bottom: "Convolution18"
[2018-04-23 20:40:45]    top: "Convolution18"
[2018-04-23 20:40:45]  }
[2018-04-23 20:40:45]  layer {
[2018-04-23 20:40:45]    name: "Convolution19"
[2018-04-23 20:40:45]    type: "Convolution"
[2018-04-23 20:40:45]    bottom: "Convolution18"
[2018-04-23 20:40:45]    top: "Convolution19"
[2018-04-23 20:40:45]    param {
[2018-04-23 20:40:45]      lr_mult: 1
[2018-04-23 20:40:45]      decay_mult: 1
[2018-04-23 20:40:45]    }
[2018-04-23 20:40:45]    param {
[2018-04-23 20:40:45]      lr_mult: 2
[2018-04-23 20:40:45]      decay_mult: 0
[2018-04-23 20:40:45]    }
[2018-04-23 20:40:45]    convolution_param {
[2018-04-23 20:40:45]      num_output: 128
[2018-04-23 20:40:45]      pad: 1
[2018-04-23 20:40:45]      kernel_size: 3
[2018-04-23 20:40:45]      stride: 1
[2018-04-23 20:40:45]      weight_filler {
[2018-04-23 20:40:45]        type: "xavier"
[2018-04-23 20:40:45]      }
[2018-04-23 20:40:45]      bias_filler {
[2018-04-23 20:40:46]        type: "constant"
[2018-04-23 20:40:46]        value: 0
[2018-04-23 20:40:46]      }
[2018-04-23 20:40:46]    }
[2018-04-23 20:40:46]  }
[2018-04-23 20:40:46]  layer {
[2018-04-23 20:40:46]    name: "BatchNorm19"
[2018-04-23 20:40:46]    type: "BatchNorm"
[2018-04-23 20:40:46]    bottom: "Convolution19"
[2018-04-23 20:40:46]    top: "Convolution19"
[2018-04-23 20:40:46]    param {
[2018-04-23 20:40:46]      lr_mult: 0
[2018-04-23 20:40:46]      decay_mult: 0
[2018-04-23 20:40:46]    }
[2018-04-23 20:40:46]    param {
[2018-04-23 20:40:46]      lr_mult: 0
[2018-04-23 20:40:46]      decay_mult: 0
[2018-04-23 20:40:46]    }
[2018-04-23 20:40:46]    param {
[2018-04-23 20:40:46]      lr_mult: 0
[2018-04-23 20:40:46]      decay_mult: 0
[2018-04-23 20:40:46]    }
[2018-04-23 20:40:46]  }
[2018-04-23 20:40:46]  layer {
[2018-04-23 20:40:46]    name: "Scale19"
[2018-04-23 20:40:46]    type: "Scale"
[2018-04-23 20:40:46]    bottom: "Convolution19"
[2018-04-23 20:40:46]    top: "Convolution19"
[2018-04-23 20:40:46]    scale_param {
[2018-04-23 20:40:46]      bias_term: true
[2018-04-23 20:40:46]    }
[2018-04-23 20:40:46]  }
[2018-04-23 20:40:46]  layer {
[2018-04-23 20:40:46]    name: "Eltwise8"
[2018-04-23 20:40:46]    type: "Eltwise"
[2018-04-23 20:40:46]    bottom: "Eltwise7"
[2018-04-23 20:40:46]    bottom: "Convolution19"
[2018-04-23 20:40:46]    top: "Eltwise8"
[2018-04-23 20:40:46]    eltwise_param {
[2018-04-23 20:40:46]      operation: SUM
[2018-04-23 20:40:46]    }
[2018-04-23 20:40:46]  }
[2018-04-23 20:40:46]  layer {
[2018-04-23 20:40:46]    name: "ReLU17"
[2018-04-23 20:40:46]    type: "ReLU"
[2018-04-23 20:40:46]    bottom: "Eltwise8"
[2018-04-23 20:40:46]    top: "Eltwise8"
[2018-04-23 20:40:46]  }
[2018-04-23 20:40:46]  layer {
[2018-04-23 20:40:46]    name: "Convolution20"
[2018-04-23 20:40:46]    type: "Convolution"
[2018-04-23 20:40:46]    bottom: "Eltwise8"
[2018-04-23 20:40:46]    top: "Convolution20"
[2018-04-23 20:40:46]    param {
[2018-04-23 20:40:46]      lr_mult: 1
[2018-04-23 20:40:46]      decay_mult: 1
[2018-04-23 20:40:46]    }
[2018-04-23 20:40:46]    param {
[2018-04-23 20:40:46]      lr_mult: 2
[2018-04-23 20:40:46]      decay_mult: 0
[2018-04-23 20:40:46]    }
[2018-04-23 20:40:46]    convolution_param {
[2018-04-23 20:40:46]      num_output: 128
[2018-04-23 20:40:46]      pad: 1
[2018-04-23 20:40:46]      kernel_size: 3
[2018-04-23 20:40:46]      stride: 1
[2018-04-23 20:40:46]      weight_filler {
[2018-04-23 20:40:46]        type: "xavier"
[2018-04-23 20:40:46]      }
[2018-04-23 20:40:46]      bias_filler {
[2018-04-23 20:40:46]        type: "constant"
[2018-04-23 20:40:46]        value: 0
[2018-04-23 20:40:46]      }
[2018-04-23 20:40:46]    }
[2018-04-23 20:40:47]  }
[2018-04-23 20:40:47]  layer {
[2018-04-23 20:40:47]    name: "BatchNorm20"
[2018-04-23 20:40:47]    type: "BatchNorm"
[2018-04-23 20:40:47]    bottom: "Convolution20"
[2018-04-23 20:40:47]    top: "Convolution20"
[2018-04-23 20:40:47]    param {
[2018-04-23 20:40:47]      lr_mult: 0
[2018-04-23 20:40:47]      decay_mult: 0
[2018-04-23 20:40:47]    }
[2018-04-23 20:40:47]    param {
[2018-04-23 20:40:47]      lr_mult: 0
[2018-04-23 20:40:47]      decay_mult: 0
[2018-04-23 20:40:47]    }
[2018-04-23 20:40:47]    param {
[2018-04-23 20:40:47]      lr_mult: 0
[2018-04-23 20:40:47]      decay_mult: 0
[2018-04-23 20:40:47]    }
[2018-04-23 20:40:47]  }
[2018-04-23 20:40:47]  layer {
[2018-04-23 20:40:47]    name: "Scale20"
[2018-04-23 20:40:47]    type: "Scale"
[2018-04-23 20:40:47]    bottom: "Convolution20"
[2018-04-23 20:40:47]    top: "Convolution20"
[2018-04-23 20:40:47]    scale_param {
[2018-04-23 20:40:47]      bias_term: true
[2018-04-23 20:40:47]    }
[2018-04-23 20:40:47]  }
[2018-04-23 20:40:47]  layer {
[2018-04-23 20:40:47]    name: "ReLU18"
[2018-04-23 20:40:47]    type: "ReLU"
[2018-04-23 20:40:47]    bottom: "Convolution20"
[2018-04-23 20:40:47]    top: "Convolution20"
[2018-04-23 20:40:47]  }
[2018-04-23 20:40:47]  layer {
[2018-04-23 20:40:47]    name: "Convolution21"
[2018-04-23 20:40:47]    type: "Convolution"
[2018-04-23 20:40:47]    bottom: "Convolution20"
[2018-04-23 20:40:47]    top: "Convolution21"
[2018-04-23 20:40:47]    param {
[2018-04-23 20:40:47]      lr_mult: 1
[2018-04-23 20:40:47]      decay_mult: 1
[2018-04-23 20:40:47]    }
[2018-04-23 20:40:47]    param {
[2018-04-23 20:40:47]      lr_mult: 2
[2018-04-23 20:40:47]      decay_mult: 0
[2018-04-23 20:40:47]    }
[2018-04-23 20:40:47]    convolution_param {
[2018-04-23 20:40:47]      num_output: 128
[2018-04-23 20:40:47]      pad: 1
[2018-04-23 20:40:47]      kernel_size: 3
[2018-04-23 20:40:47]      stride: 1
[2018-04-23 20:40:47]      weight_filler {
[2018-04-23 20:40:47]        type: "xavier"
[2018-04-23 20:40:47]      }
[2018-04-23 20:40:47]      bias_filler {
[2018-04-23 20:40:47]        type: "constant"
[2018-04-23 20:40:47]        value: 0
[2018-04-23 20:40:47]      }
[2018-04-23 20:40:47]    }
[2018-04-23 20:40:47]  }
[2018-04-23 20:40:47]  layer {
[2018-04-23 20:40:47]    name: "BatchNorm21"
[2018-04-23 20:40:47]    type: "BatchNorm"
[2018-04-23 20:40:47]    bottom: "Convolution21"
[2018-04-23 20:40:48]    top: "Convolution21"
[2018-04-23 20:40:48]    param {
[2018-04-23 20:40:48]      lr_mult: 0
[2018-04-23 20:40:48]      decay_mult: 0
[2018-04-23 20:40:48]    }
[2018-04-23 20:40:48]    param {
[2018-04-23 20:40:48]      lr_mult: 0
[2018-04-23 20:40:48]      decay_mult: 0
[2018-04-23 20:40:48]    }
[2018-04-23 20:40:48]    param {
[2018-04-23 20:40:48]      lr_mult: 0
[2018-04-23 20:40:48]      decay_mult: 0
[2018-04-23 20:40:48]    }
[2018-04-23 20:40:48]  }
[2018-04-23 20:40:48]  layer {
[2018-04-23 20:40:48]    name: "Scale21"
[2018-04-23 20:40:48]    type: "Scale"
[2018-04-23 20:40:48]    bottom: "Convolution21"
[2018-04-23 20:40:48]    top: "Convolution21"
[2018-04-23 20:40:48]    scale_param {
[2018-04-23 20:40:48]      bias_term: true
[2018-04-23 20:40:48]    }
[2018-04-23 20:40:48]  }
[2018-04-23 20:40:48]  layer {
[2018-04-23 20:40:48]    name: "Eltwise9"
[2018-04-23 20:40:48]    type: "Eltwise"
[2018-04-23 20:40:48]    bottom: "Eltwise8"
[2018-04-23 20:40:48]    bottom: "Convolution21"
[2018-04-23 20:40:48]    top: "Eltwise9"
[2018-04-23 20:40:48]    eltwise_param {
[2018-04-23 20:40:48]      operation: SUM
[2018-04-23 20:40:48]    }
[2018-04-23 20:40:48]  }
[2018-04-23 20:40:48]  layer {
[2018-04-23 20:40:48]    name: "ReLU19"
[2018-04-23 20:40:48]    type: "ReLU"
[2018-04-23 20:40:48]    bottom: "Eltwise9"
[2018-04-23 20:40:48]    top: "Eltwise9"
[2018-04-23 20:40:48]  }
[2018-04-23 20:40:48]  layer {
[2018-04-23 20:40:48]    name: "Convolution22"
[2018-04-23 20:40:48]    type: "Convolution"
[2018-04-23 20:40:48]    bottom: "Eltwise9"
[2018-04-23 20:40:48]    top: "Convolution22"
[2018-04-23 20:40:48]    param {
[2018-04-23 20:40:48]      lr_mult: 1
[2018-04-23 20:40:48]      decay_mult: 1
[2018-04-23 20:40:48]    }
[2018-04-23 20:40:48]    param {
[2018-04-23 20:40:48]      lr_mult: 2
[2018-04-23 20:40:48]      decay_mult: 0
[2018-04-23 20:40:48]    }
[2018-04-23 20:40:48]    convolution_param {
[2018-04-23 20:40:48]      num_output: 256
[2018-04-23 20:40:48]      pad: 0
[2018-04-23 20:40:48]      kernel_size: 1
[2018-04-23 20:40:48]      stride: 2
[2018-04-23 20:40:48]      weight_filler {
[2018-04-23 20:40:48]        type: "xavier"
[2018-04-23 20:40:48]      }
[2018-04-23 20:40:48]      bias_filler {
[2018-04-23 20:40:48]        type: "constant"
[2018-04-23 20:40:48]        value: 0
[2018-04-23 20:40:48]      }
[2018-04-23 20:40:48]    }
[2018-04-23 20:40:48]  }
[2018-04-23 20:40:48]  layer {
[2018-04-23 20:40:48]    name: "BatchNorm22"
[2018-04-23 20:40:48]    type: "BatchNorm"
[2018-04-23 20:40:48]    bottom: "Convolution22"
[2018-04-23 20:40:48]    top: "Convolution22"
[2018-04-23 20:40:48]    param {
[2018-04-23 20:40:48]      lr_mult: 0
[2018-04-23 20:40:48]      decay_mult: 0
[2018-04-23 20:40:48]    }
[2018-04-23 20:40:48]    param {
[2018-04-23 20:40:49]      lr_mult: 0
[2018-04-23 20:40:49]      decay_mult: 0
[2018-04-23 20:40:49]    }
[2018-04-23 20:40:49]    param {
[2018-04-23 20:40:49]      lr_mult: 0
[2018-04-23 20:40:49]      decay_mult: 0
[2018-04-23 20:40:49]    }
[2018-04-23 20:40:49]  }
[2018-04-23 20:40:49]  layer {
[2018-04-23 20:40:49]    name: "Scale22"
[2018-04-23 20:40:49]    type: "Scale"
[2018-04-23 20:40:49]    bottom: "Convolution22"
[2018-04-23 20:40:49]    top: "Convolution22"
[2018-04-23 20:40:49]    scale_param {
[2018-04-23 20:40:49]      bias_term: true
[2018-04-23 20:40:49]    }
[2018-04-23 20:40:49]  }
[2018-04-23 20:40:49]  layer {
[2018-04-23 20:40:49]    name: "Convolution23"
[2018-04-23 20:40:49]    type: "Convolution"
[2018-04-23 20:40:49]    bottom: "Eltwise9"
[2018-04-23 20:40:49]    top: "Convolution23"
[2018-04-23 20:40:49]    param {
[2018-04-23 20:40:49]      lr_mult: 1
[2018-04-23 20:40:49]      decay_mult: 1
[2018-04-23 20:40:49]    }
[2018-04-23 20:40:49]    param {
[2018-04-23 20:40:49]      lr_mult: 2
[2018-04-23 20:40:49]      decay_mult: 0
[2018-04-23 20:40:49]    }
[2018-04-23 20:40:49]    convolution_param {
[2018-04-23 20:40:49]      num_output: 256
[2018-04-23 20:40:49]      pad: 1
[2018-04-23 20:40:49]      kernel_size: 3
[2018-04-23 20:40:49]      stride: 2
[2018-04-23 20:40:49]      weight_filler {
[2018-04-23 20:40:49]        type: "xavier"
[2018-04-23 20:40:49]      }
[2018-04-23 20:40:49]      bias_filler {
[2018-04-23 20:40:49]        type: "constant"
[2018-04-23 20:40:49]        value: 0
[2018-04-23 20:40:49]      }
[2018-04-23 20:40:49]    }
[2018-04-23 20:40:49]  }
[2018-04-23 20:40:49]  layer {
[2018-04-23 20:40:49]    name: "BatchNorm23"
[2018-04-23 20:40:49]    type: "BatchNorm"
[2018-04-23 20:40:49]    bottom: "Convolution23"
[2018-04-23 20:40:49]    top: "Convolution23"
[2018-04-23 20:40:49]    param {
[2018-04-23 20:40:49]      lr_mult: 0
[2018-04-23 20:40:49]      decay_mult: 0
[2018-04-23 20:40:49]    }
[2018-04-23 20:40:49]    param {
[2018-04-23 20:40:49]      lr_mult: 0
[2018-04-23 20:40:49]      decay_mult: 0
[2018-04-23 20:40:49]    }
[2018-04-23 20:40:49]    param {
[2018-04-23 20:40:49]      lr_mult: 0
[2018-04-23 20:40:49]      decay_mult: 0
[2018-04-23 20:40:49]    }
[2018-04-23 20:40:49]  }
[2018-04-23 20:40:49]  layer {
[2018-04-23 20:40:49]    name: "Scale23"
[2018-04-23 20:40:49]    type: "Scale"
[2018-04-23 20:40:49]    bottom: "Convolution23"
[2018-04-23 20:40:49]    top: "Convolution23"
[2018-04-23 20:40:49]    scale_param {
[2018-04-23 20:40:49]      bias_term: true
[2018-04-23 20:40:49]    }
[2018-04-23 20:40:49]  }
[2018-04-23 20:40:49]  layer {
[2018-04-23 20:40:49]    name: "ReLU20"
[2018-04-23 20:40:49]    type: "ReLU"
[2018-04-23 20:40:49]    bottom: "Convolution23"
[2018-04-23 20:40:50]    top: "Convolution23"
[2018-04-23 20:40:50]  }
[2018-04-23 20:40:50]  layer {
[2018-04-23 20:40:50]    name: "Convolution24"
[2018-04-23 20:40:50]    type: "Convolution"
[2018-04-23 20:40:50]    bottom: "Convolution23"
[2018-04-23 20:40:50]    top: "Convolution24"
[2018-04-23 20:40:50]    param {
[2018-04-23 20:40:50]      lr_mult: 1
[2018-04-23 20:40:50]      decay_mult: 1
[2018-04-23 20:40:50]    }
[2018-04-23 20:40:50]    param {
[2018-04-23 20:40:50]      lr_mult: 2
[2018-04-23 20:40:50]      decay_mult: 0
[2018-04-23 20:40:50]    }
[2018-04-23 20:40:50]    convolution_param {
[2018-04-23 20:40:50]      num_output: 256
[2018-04-23 20:40:50]      pad: 1
[2018-04-23 20:40:50]      kernel_size: 3
[2018-04-23 20:40:50]      stride: 1
[2018-04-23 20:40:50]      weight_filler {
[2018-04-23 20:40:50]        type: "xavier"
[2018-04-23 20:40:50]      }
[2018-04-23 20:40:50]      bias_filler {
[2018-04-23 20:40:50]        type: "constant"
[2018-04-23 20:40:50]        value: 0
[2018-04-23 20:40:50]      }
[2018-04-23 20:40:50]    }
[2018-04-23 20:40:50]  }
[2018-04-23 20:40:50]  layer {
[2018-04-23 20:40:50]    name: "BatchNorm24"
[2018-04-23 20:40:50]    type: "BatchNorm"
[2018-04-23 20:40:50]    bottom: "Convolution24"
[2018-04-23 20:40:50]    top: "Convolution24"
[2018-04-23 20:40:50]    param {
[2018-04-23 20:40:50]      lr_mult: 0
[2018-04-23 20:40:50]      decay_mult: 0
[2018-04-23 20:40:50]    }
[2018-04-23 20:40:50]    param {
[2018-04-23 20:40:50]      lr_mult: 0
[2018-04-23 20:40:50]      decay_mult: 0
[2018-04-23 20:40:50]    }
[2018-04-23 20:40:50]    param {
[2018-04-23 20:40:50]      lr_mult: 0
[2018-04-23 20:40:50]      decay_mult: 0
[2018-04-23 20:40:50]    }
[2018-04-23 20:40:50]  }
[2018-04-23 20:40:50]  layer {
[2018-04-23 20:40:50]    name: "Scale24"
[2018-04-23 20:40:50]    type: "Scale"
[2018-04-23 20:40:50]    bottom: "Convolution24"
[2018-04-23 20:40:50]    top: "Convolution24"
[2018-04-23 20:40:50]    scale_param {
[2018-04-23 20:40:50]      bias_term: true
[2018-04-23 20:40:50]    }
[2018-04-23 20:40:50]  }
[2018-04-23 20:40:50]  layer {
[2018-04-23 20:40:50]    name: "Eltwise10"
[2018-04-23 20:40:50]    type: "Eltwise"
[2018-04-23 20:40:50]    bottom: "Convolution22"
[2018-04-23 20:40:50]    bottom: "Convolution24"
[2018-04-23 20:40:50]    top: "Eltwise10"
[2018-04-23 20:40:50]    eltwise_param {
[2018-04-23 20:40:50]      operation: SUM
[2018-04-23 20:40:50]    }
[2018-04-23 20:40:50]  }
[2018-04-23 20:40:50]  layer {
[2018-04-23 20:40:50]    name: "ReLU21"
[2018-04-23 20:40:50]    type: "ReLU"
[2018-04-23 20:40:50]    bottom: "Eltwise10"
[2018-04-23 20:40:50]    top: "Eltwise10"
[2018-04-23 20:40:50]  }
[2018-04-23 20:40:50]  layer {
[2018-04-23 20:40:51]    name: "Convolution25"
[2018-04-23 20:40:51]    type: "Convolution"
[2018-04-23 20:40:51]    bottom: "Eltwise10"
[2018-04-23 20:40:51]    top: "Convolution25"
[2018-04-23 20:40:51]    param {
[2018-04-23 20:40:51]      lr_mult: 1
[2018-04-23 20:40:51]      decay_mult: 1
[2018-04-23 20:40:51]    }
[2018-04-23 20:40:51]    param {
[2018-04-23 20:40:51]      lr_mult: 2
[2018-04-23 20:40:51]      decay_mult: 0
[2018-04-23 20:40:51]    }
[2018-04-23 20:40:51]    convolution_param {
[2018-04-23 20:40:51]      num_output: 256
[2018-04-23 20:40:51]      pad: 1
[2018-04-23 20:40:51]      kernel_size: 3
[2018-04-23 20:40:51]      stride: 1
[2018-04-23 20:40:51]      weight_filler {
[2018-04-23 20:40:51]        type: "xavier"
[2018-04-23 20:40:51]      }
[2018-04-23 20:40:51]      bias_filler {
[2018-04-23 20:40:51]        type: "constant"
[2018-04-23 20:40:51]        value: 0
[2018-04-23 20:40:51]      }
[2018-04-23 20:40:51]    }
[2018-04-23 20:40:51]  }
[2018-04-23 20:40:51]  layer {
[2018-04-23 20:40:51]    name: "BatchNorm25"
[2018-04-23 20:40:51]    type: "BatchNorm"
[2018-04-23 20:40:51]    bottom: "Convolution25"
[2018-04-23 20:40:51]    top: "Convolution25"
[2018-04-23 20:40:51]    param {
[2018-04-23 20:40:51]      lr_mult: 0
[2018-04-23 20:40:51]      decay_mult: 0
[2018-04-23 20:40:51]    }
[2018-04-23 20:40:51]    param {
[2018-04-23 20:40:51]      lr_mult: 0
[2018-04-23 20:40:51]      decay_mult: 0
[2018-04-23 20:40:51]    }
[2018-04-23 20:40:51]    param {
[2018-04-23 20:40:51]      lr_mult: 0
[2018-04-23 20:40:51]      decay_mult: 0
[2018-04-23 20:40:51]    }
[2018-04-23 20:40:51]  }
[2018-04-23 20:40:51]  layer {
[2018-04-23 20:40:51]    name: "Scale25"
[2018-04-23 20:40:51]    type: "Scale"
[2018-04-23 20:40:51]    bottom: "Convolution25"
[2018-04-23 20:40:51]    top: "Convolution25"
[2018-04-23 20:40:51]    scale_param {
[2018-04-23 20:40:51]      bias_term: true
[2018-04-23 20:40:51]    }
[2018-04-23 20:40:51]  }
[2018-04-23 20:40:51]  layer {
[2018-04-23 20:40:51]    name: "ReLU22"
[2018-04-23 20:40:51]    type: "ReLU"
[2018-04-23 20:40:51]    bottom: "Convolution25"
[2018-04-23 20:40:51]    top: "Convolution25"
[2018-04-23 20:40:51]  }
[2018-04-23 20:40:51]  layer {
[2018-04-23 20:40:51]    name: "Convolution26"
[2018-04-23 20:40:51]    type: "Convolution"
[2018-04-23 20:40:51]    bottom: "Convolution25"
[2018-04-23 20:40:51]    top: "Convolution26"
[2018-04-23 20:40:51]    param {
[2018-04-23 20:40:51]      lr_mult: 1
[2018-04-23 20:40:51]      decay_mult: 1
[2018-04-23 20:40:51]    }
[2018-04-23 20:40:51]    param {
[2018-04-23 20:40:51]      lr_mult: 2
[2018-04-23 20:40:51]      decay_mult: 0
[2018-04-23 20:40:51]    }
[2018-04-23 20:40:51]    convolution_param {
[2018-04-23 20:40:51]      num_output: 256
[2018-04-23 20:40:51]      pad: 1
[2018-04-23 20:40:51]      kernel_size: 3
[2018-04-23 20:40:52]      stride: 1
[2018-04-23 20:40:52]      weight_filler {
[2018-04-23 20:40:52]        type: "xavier"
[2018-04-23 20:40:52]      }
[2018-04-23 20:40:52]      bias_filler {
[2018-04-23 20:40:52]        type: "constant"
[2018-04-23 20:40:52]        value: 0
[2018-04-23 20:40:52]      }
[2018-04-23 20:40:52]    }
[2018-04-23 20:40:52]  }
[2018-04-23 20:40:52]  layer {
[2018-04-23 20:40:52]    name: "BatchNorm26"
[2018-04-23 20:40:52]    type: "BatchNorm"
[2018-04-23 20:40:52]    bottom: "Convolution26"
[2018-04-23 20:40:52]    top: "Convolution26"
[2018-04-23 20:40:52]    param {
[2018-04-23 20:40:52]      lr_mult: 0
[2018-04-23 20:40:52]      decay_mult: 0
[2018-04-23 20:40:52]    }
[2018-04-23 20:40:52]    param {
[2018-04-23 20:40:52]      lr_mult: 0
[2018-04-23 20:40:52]      decay_mult: 0
[2018-04-23 20:40:52]    }
[2018-04-23 20:40:52]    param {
[2018-04-23 20:40:52]      lr_mult: 0
[2018-04-23 20:40:52]      decay_mult: 0
[2018-04-23 20:40:52]    }
[2018-04-23 20:40:52]  }
[2018-04-23 20:40:52]  layer {
[2018-04-23 20:40:52]    name: "Scale26"
[2018-04-23 20:40:52]    type: "Scale"
[2018-04-23 20:40:52]    bottom: "Convolution26"
[2018-04-23 20:40:52]    top: "Convolution26"
[2018-04-23 20:40:52]    scale_param {
[2018-04-23 20:40:52]      bias_term: true
[2018-04-23 20:40:52]    }
[2018-04-23 20:40:52]  }
[2018-04-23 20:40:52]  layer {
[2018-04-23 20:40:52]    name: "Eltwise11"
[2018-04-23 20:40:52]    type: "Eltwise"
[2018-04-23 20:40:52]    bottom: "Eltwise10"
[2018-04-23 20:40:52]    bottom: "Convolution26"
[2018-04-23 20:40:52]    top: "Eltwise11"
[2018-04-23 20:40:52]    eltwise_param {
[2018-04-23 20:40:52]      operation: SUM
[2018-04-23 20:40:52]    }
[2018-04-23 20:40:52]  }
[2018-04-23 20:40:52]  layer {
[2018-04-23 20:40:52]    name: "ReLU23"
[2018-04-23 20:40:52]    type: "ReLU"
[2018-04-23 20:40:52]    bottom: "Eltwise11"
[2018-04-23 20:40:52]    top: "Eltwise11"
[2018-04-23 20:40:52]  }
[2018-04-23 20:40:52]  layer {
[2018-04-23 20:40:52]    name: "Convolution27"
[2018-04-23 20:40:52]    type: "Convolution"
[2018-04-23 20:40:52]    bottom: "Eltwise11"
[2018-04-23 20:40:52]    top: "Convolution27"
[2018-04-23 20:40:52]    param {
[2018-04-23 20:40:52]      lr_mult: 1
[2018-04-23 20:40:52]      decay_mult: 1
[2018-04-23 20:40:52]    }
[2018-04-23 20:40:52]    param {
[2018-04-23 20:40:52]      lr_mult: 2
[2018-04-23 20:40:52]      decay_mult: 0
[2018-04-23 20:40:52]    }
[2018-04-23 20:40:52]    convolution_param {
[2018-04-23 20:40:52]      num_output: 256
[2018-04-23 20:40:52]      pad: 1
[2018-04-23 20:40:52]      kernel_size: 3
[2018-04-23 20:40:52]      stride: 1
[2018-04-23 20:40:52]      weight_filler {
[2018-04-23 20:40:52]        type: "xavier"
[2018-04-23 20:40:52]      }
[2018-04-23 20:40:52]      bias_filler {
[2018-04-23 20:40:52]        type: "constant"
[2018-04-23 20:40:52]        value: 0
[2018-04-23 20:40:53]      }
[2018-04-23 20:40:53]    }
[2018-04-23 20:40:53]  }
[2018-04-23 20:40:53]  layer {
[2018-04-23 20:40:53]    name: "BatchNorm27"
[2018-04-23 20:40:53]    type: "BatchNorm"
[2018-04-23 20:40:53]    bottom: "Convolution27"
[2018-04-23 20:40:53]    top: "Convolution27"
[2018-04-23 20:40:53]    param {
[2018-04-23 20:40:53]      lr_mult: 0
[2018-04-23 20:40:53]      decay_mult: 0
[2018-04-23 20:40:53]    }
[2018-04-23 20:40:53]    param {
[2018-04-23 20:40:53]      lr_mult: 0
[2018-04-23 20:40:53]      decay_mult: 0
[2018-04-23 20:40:53]    }
[2018-04-23 20:40:53]    param {
[2018-04-23 20:40:53]      lr_mult: 0
[2018-04-23 20:40:53]      decay_mult: 0
[2018-04-23 20:40:53]    }
[2018-04-23 20:40:53]  }
[2018-04-23 20:40:53]  layer {
[2018-04-23 20:40:53]    name: "Scale27"
[2018-04-23 20:40:53]    type: "Scale"
[2018-04-23 20:40:53]    bottom: "Convolution27"
[2018-04-23 20:40:53]    top: "Convolution27"
[2018-04-23 20:40:53]    scale_param {
[2018-04-23 20:40:53]      bias_term: true
[2018-04-23 20:40:53]    }
[2018-04-23 20:40:53]  }
[2018-04-23 20:40:53]  layer {
[2018-04-23 20:40:53]    name: "ReLU24"
[2018-04-23 20:40:53]    type: "ReLU"
[2018-04-23 20:40:53]    bottom: "Convolution27"
[2018-04-23 20:40:53]    top: "Convolution27"
[2018-04-23 20:40:53]  }
[2018-04-23 20:40:53]  layer {
[2018-04-23 20:40:53]    name: "Convolution28"
[2018-04-23 20:40:53]    type: "Convolution"
[2018-04-23 20:40:53]    bottom: "Convolution27"
[2018-04-23 20:40:53]    top: "Convolution28"
[2018-04-23 20:40:53]    param {
[2018-04-23 20:40:53]      lr_mult: 1
[2018-04-23 20:40:53]      decay_mult: 1
[2018-04-23 20:40:53]    }
[2018-04-23 20:40:53]    param {
[2018-04-23 20:40:53]      lr_mult: 2
[2018-04-23 20:40:53]      decay_mult: 0
[2018-04-23 20:40:53]    }
[2018-04-23 20:40:53]    convolution_param {
[2018-04-23 20:40:53]      num_output: 256
[2018-04-23 20:40:53]      pad: 1
[2018-04-23 20:40:53]      kernel_size: 3
[2018-04-23 20:40:53]      stride: 1
[2018-04-23 20:40:53]      weight_filler {
[2018-04-23 20:40:53]        type: "xavier"
[2018-04-23 20:40:53]      }
[2018-04-23 20:40:53]      bias_filler {
[2018-04-23 20:40:53]        type: "constant"
[2018-04-23 20:40:53]        value: 0
[2018-04-23 20:40:53]      }
[2018-04-23 20:40:53]    }
[2018-04-23 20:40:53]  }
[2018-04-23 20:40:53]  layer {
[2018-04-23 20:40:53]    name: "BatchNorm28"
[2018-04-23 20:40:53]    type: "BatchNorm"
[2018-04-23 20:40:53]    bottom: "Convolution28"
[2018-04-23 20:40:53]    top: "Convolution28"
[2018-04-23 20:40:53]    param {
[2018-04-23 20:40:53]      lr_mult: 0
[2018-04-23 20:40:53]      decay_mult: 0
[2018-04-23 20:40:54]    }
[2018-04-23 20:40:54]    param {
[2018-04-23 20:40:54]      lr_mult: 0
[2018-04-23 20:40:54]      decay_mult: 0
[2018-04-23 20:40:54]    }
[2018-04-23 20:40:54]    param {
[2018-04-23 20:40:54]      lr_mult: 0
[2018-04-23 20:40:54]      decay_mult: 0
[2018-04-23 20:40:54]    }
[2018-04-23 20:40:54]  }
[2018-04-23 20:40:54]  layer {
[2018-04-23 20:40:54]    name: "Scale28"
[2018-04-23 20:40:54]    type: "Scale"
[2018-04-23 20:40:54]    bottom: "Convolution28"
[2018-04-23 20:40:54]    top: "Convolution28"
[2018-04-23 20:40:54]    scale_param {
[2018-04-23 20:40:54]      bias_term: true
[2018-04-23 20:40:54]    }
[2018-04-23 20:40:54]  }
[2018-04-23 20:40:54]  layer {
[2018-04-23 20:40:54]    name: "Eltwise12"
[2018-04-23 20:40:54]    type: "Eltwise"
[2018-04-23 20:40:54]    bottom: "Eltwise11"
[2018-04-23 20:40:54]    bottom: "Convolution28"
[2018-04-23 20:40:54]    top: "Eltwise12"
[2018-04-23 20:40:54]    eltwise_param {
[2018-04-23 20:40:54]      operation: SUM
[2018-04-23 20:40:54]    }
[2018-04-23 20:40:54]  }
[2018-04-23 20:40:54]  layer {
[2018-04-23 20:40:54]    name: "ReLU25"
[2018-04-23 20:40:54]    type: "ReLU"
[2018-04-23 20:40:54]    bottom: "Eltwise12"
[2018-04-23 20:40:54]    top: "Eltwise12"
[2018-04-23 20:40:54]  }
[2018-04-23 20:40:54]  layer {
[2018-04-23 20:40:54]    name: "Pooling1"
[2018-04-23 20:40:54]    type: "Pooling"
[2018-04-23 20:40:54]    bottom: "Eltwise12"
[2018-04-23 20:40:54]    top: "Pooling1"
[2018-04-23 20:40:54]    pooling_param {
[2018-04-23 20:40:54]      pool: AVE
[2018-04-23 20:40:54]      global_pooling: true
[2018-04-23 20:40:54]    }
[2018-04-23 20:40:54]  }
[2018-04-23 20:40:54]  layer {
[2018-04-23 20:40:54]    name: "InnerProduct1"
[2018-04-23 20:40:54]    type: "InnerProduct"
[2018-04-23 20:40:54]    bottom: "Pooling1"
[2018-04-23 20:40:54]    top: "InnerProduct1"
[2018-04-23 20:40:54]    param {
[2018-04-23 20:40:54]      lr_mult: 1
[2018-04-23 20:40:54]      decay_mult: 1
[2018-04-23 20:40:54]    }
[2018-04-23 20:40:54]    param {
[2018-04-23 20:40:54]      lr_mult: 2
[2018-04-23 20:40:54]      decay_mult: 1
[2018-04-23 20:40:54]    }
[2018-04-23 20:40:54]    inner_product_param {
[2018-04-23 20:40:54]      num_output: 100
[2018-04-23 20:40:54]      weight_filler {
[2018-04-23 20:40:54]        type: "xavier"
[2018-04-23 20:40:54]      }
[2018-04-23 20:40:54]      bias_filler {
[2018-04-23 20:40:54]        type: "constant"
[2018-04-23 20:40:54]        value: 0
[2018-04-23 20:40:54]      }
[2018-04-23 20:40:54]    }
[2018-04-23 20:40:54]  }
[2018-04-23 20:40:54]  layer {
[2018-04-23 20:40:54]    name: "SoftmaxWithLoss1"
[2018-04-23 20:40:54]    type: "SoftmaxWithLoss"
[2018-04-23 20:40:54]    bottom: "InnerProduct1"
[2018-04-23 20:40:54]    bottom: "label"
[2018-04-23 20:40:54]    top: "SoftmaxWithLoss1"
[2018-04-23 20:40:55]  }
[2018-04-23 20:40:55]  layer {
[2018-04-23 20:40:55]    name: "Accuracy_test"
[2018-04-23 20:40:55]    type: "Accuracy"
[2018-04-23 20:40:55]    bottom: "InnerProduct1"
[2018-04-23 20:40:55]    bottom: "label"
[2018-04-23 20:40:55]    top: "Accuracy_test"
[2018-04-23 20:40:55]    include {
[2018-04-23 20:40:55]      phase: TEST
[2018-04-23 20:40:55]    }
[2018-04-23 20:40:55]    accuracy_param {
[2018-04-23 20:40:55]      top_k: 1
[2018-04-23 20:40:55]    }
[2018-04-23 20:40:55]  }
[2018-04-23 20:40:55]  I0423 12:39:45.902518    21 layer_factory.hpp:77] Creating layer data
[2018-04-23 20:40:55]  I0423 12:39:45.902590    21 net.cpp:84] Creating Layer data
[2018-04-23 20:40:55]  I0423 12:39:45.902637    21 net.cpp:380] data -> data
[2018-04-23 20:40:55]  I0423 12:39:45.902693    21 net.cpp:380] data -> label
[2018-04-23 20:40:55]  I0423 12:39:45.902750    21 image_data_layer.cpp:38] Opening file ./mydata/val.txt
[2018-04-23 20:40:55]  I0423 12:39:45.906641    21 image_data_layer.cpp:53] Shuffling data
[2018-04-23 20:40:55]  I0423 12:39:45.907325    21 image_data_layer.cpp:63] A total of 5000 images.
[2018-04-23 20:40:55]  I0423 12:39:45.908455    21 image_data_layer.cpp:90] output data size: 32,3,64,64
[2018-04-23 20:40:55]  I0423 12:39:45.921597    21 net.cpp:122] Setting up data
[2018-04-23 20:40:55]  I0423 12:39:45.921659    21 net.cpp:129] Top shape: 32 3 64 64 (393216)
[2018-04-23 20:40:55]  I0423 12:39:45.921691    21 net.cpp:129] Top shape: 32 (32)
[2018-04-23 20:40:55]  I0423 12:39:45.921727    21 net.cpp:137] Memory required for data: 1572992
[2018-04-23 20:40:55]  I0423 12:39:45.921753    21 layer_factory.hpp:77] Creating layer label_data_1_split
[2018-04-23 20:40:55]  I0423 12:39:45.921783    21 net.cpp:84] Creating Layer label_data_1_split
[2018-04-23 20:40:55]  I0423 12:39:45.921818    21 net.cpp:406] label_data_1_split <- label
[2018-04-23 20:40:55]  I0423 12:39:45.921859    21 net.cpp:380] label_data_1_split -> label_data_1_split_0
[2018-04-23 20:40:55]  I0423 12:39:45.921950    21 net.cpp:380] label_data_1_split -> label_data_1_split_1
[2018-04-23 20:40:55]  I0423 12:39:45.922224    21 net.cpp:122] Setting up label_data_1_split
[2018-04-23 20:40:55]  I0423 12:39:45.922281    21 net.cpp:129] Top shape: 32 (32)
[2018-04-23 20:40:55]  I0423 12:39:45.922322    21 net.cpp:129] Top shape: 32 (32)
[2018-04-23 20:40:55]  I0423 12:39:45.922345    21 net.cpp:137] Memory required for data: 1573248
[2018-04-23 20:40:55]  I0423 12:39:45.922390    21 layer_factory.hpp:77] Creating layer Convolution1
[2018-04-23 20:40:55]  I0423 12:39:45.922459    21 net.cpp:84] Creating Layer Convolution1
[2018-04-23 20:40:55]  I0423 12:39:45.922525    21 net.cpp:406] Convolution1 <- data
[2018-04-23 20:40:55]  I0423 12:39:45.922564    21 net.cpp:380] Convolution1 -> Convolution1
[2018-04-23 20:40:55]  I0423 12:39:45.924600    21 net.cpp:122] Setting up Convolution1
[2018-04-23 20:40:55]  I0423 12:39:45.924669    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:55]  I0423 12:39:45.924701    21 net.cpp:137] Memory required for data: 5767552
[2018-04-23 20:40:55]  I0423 12:39:45.924757    21 layer_factory.hpp:77] Creating layer BatchNorm1
[2018-04-23 20:40:55]  I0423 12:39:45.924820    21 net.cpp:84] Creating Layer BatchNorm1
[2018-04-23 20:40:55]  I0423 12:39:45.924865    21 net.cpp:406] BatchNorm1 <- Convolution1
[2018-04-23 20:40:55]  I0423 12:39:45.924896    21 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
[2018-04-23 20:40:55]  I0423 12:39:45.925251    21 net.cpp:122] Setting up BatchNorm1
[2018-04-23 20:40:55]  I0423 12:39:45.925308    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:55]  I0423 12:39:45.925379    21 net.cpp:137] Memory required for data: 9961856
[2018-04-23 20:40:55]  I0423 12:39:45.925427    21 layer_factory.hpp:77] Creating layer Scale1
[2018-04-23 20:40:55]  I0423 12:39:45.925508    21 net.cpp:84] Creating Layer Scale1
[2018-04-23 20:40:55]  I0423 12:39:45.925536    21 net.cpp:406] Scale1 <- Convolution1
[2018-04-23 20:40:55]  I0423 12:39:45.925590    21 net.cpp:367] Scale1 -> Convolution1 (in-place)
[2018-04-23 20:40:55]  I0423 12:39:45.925832    21 layer_factory.hpp:77] Creating layer Scale1
[2018-04-23 20:40:55]  I0423 12:39:45.926090    21 net.cpp:122] Setting up Scale1
[2018-04-23 20:40:55]  I0423 12:39:45.926146    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:55]  I0423 12:39:45.926182    21 net.cpp:137] Memory required for data: 14156160
[2018-04-23 20:40:55]  I0423 12:39:45.926220    21 layer_factory.hpp:77] Creating layer ReLU1
[2018-04-23 20:40:55]  I0423 12:39:45.926257    21 net.cpp:84] Creating Layer ReLU1
[2018-04-23 20:40:55]  I0423 12:39:45.926293    21 net.cpp:406] ReLU1 <- Convolution1
[2018-04-23 20:40:55]  I0423 12:39:45.926334    21 net.cpp:367] ReLU1 -> Convolution1 (in-place)
[2018-04-23 20:40:55]  I0423 12:39:45.926760    21 net.cpp:122] Setting up ReLU1
[2018-04-23 20:40:55]  I0423 12:39:45.926815    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:55]  I0423 12:39:45.926853    21 net.cpp:137] Memory required for data: 18350464
[2018-04-23 20:40:55]  I0423 12:39:45.926887    21 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
[2018-04-23 20:40:55]  I0423 12:39:45.926937    21 net.cpp:84] Creating Layer Convolution1_ReLU1_0_split
[2018-04-23 20:40:55]  I0423 12:39:45.926971    21 net.cpp:406] Convolution1_ReLU1_0_split <- Convolution1
[2018-04-23 20:40:55]  I0423 12:39:45.927023    21 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
[2018-04-23 20:40:55]  I0423 12:39:45.927068    21 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
[2018-04-23 20:40:55]  I0423 12:39:45.927207    21 net.cpp:122] Setting up Convolution1_ReLU1_0_split
[2018-04-23 20:40:55]  I0423 12:39:45.927258    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:55]  I0423 12:39:45.927297    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:55]  I0423 12:39:45.927328    21 net.cpp:137] Memory required for data: 26739072
[2018-04-23 20:40:55]  I0423 12:39:45.927366    21 layer_factory.hpp:77] Creating layer Convolution2
[2018-04-23 20:40:55]  I0423 12:39:45.927429    21 net.cpp:84] Creating Layer Convolution2
[2018-04-23 20:40:55]  I0423 12:39:45.927470    21 net.cpp:406] Convolution2 <- Convolution1_ReLU1_0_split_0
[2018-04-23 20:40:55]  I0423 12:39:45.927533    21 net.cpp:380] Convolution2 -> Convolution2
[2018-04-23 20:40:56]  I0423 12:39:45.929424    21 net.cpp:122] Setting up Convolution2
[2018-04-23 20:40:56]  I0423 12:39:45.929500    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:56]  I0423 12:39:45.929536    21 net.cpp:137] Memory required for data: 30933376
[2018-04-23 20:40:56]  I0423 12:39:45.929600    21 layer_factory.hpp:77] Creating layer BatchNorm2
[2018-04-23 20:40:56]  I0423 12:39:45.929692    21 net.cpp:84] Creating Layer BatchNorm2
[2018-04-23 20:40:56]  I0423 12:39:45.929730    21 net.cpp:406] BatchNorm2 <- Convolution2
[2018-04-23 20:40:56]  I0423 12:39:45.929783    21 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
[2018-04-23 20:40:56]  I0423 12:39:45.930275    21 net.cpp:122] Setting up BatchNorm2
[2018-04-23 20:40:56]  I0423 12:39:45.930331    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:56]  I0423 12:39:45.930362    21 net.cpp:137] Memory required for data: 35127680
[2018-04-23 20:40:56]  I0423 12:39:45.930408    21 layer_factory.hpp:77] Creating layer Scale2
[2018-04-23 20:40:56]  I0423 12:39:45.930450    21 net.cpp:84] Creating Layer Scale2
[2018-04-23 20:40:56]  I0423 12:39:45.930503    21 net.cpp:406] Scale2 <- Convolution2
[2018-04-23 20:40:56]  I0423 12:39:45.930543    21 net.cpp:367] Scale2 -> Convolution2 (in-place)
[2018-04-23 20:40:56]  I0423 12:39:45.930691    21 layer_factory.hpp:77] Creating layer Scale2
[2018-04-23 20:40:56]  I0423 12:39:45.930979    21 net.cpp:122] Setting up Scale2
[2018-04-23 20:40:56]  I0423 12:39:45.931031    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:56]  I0423 12:39:45.931061    21 net.cpp:137] Memory required for data: 39321984
[2018-04-23 20:40:56]  I0423 12:39:45.931110    21 layer_factory.hpp:77] Creating layer ReLU2
[2018-04-23 20:40:56]  I0423 12:39:45.931150    21 net.cpp:84] Creating Layer ReLU2
[2018-04-23 20:40:56]  I0423 12:39:45.931196    21 net.cpp:406] ReLU2 <- Convolution2
[2018-04-23 20:40:56]  I0423 12:39:45.931241    21 net.cpp:367] ReLU2 -> Convolution2 (in-place)
[2018-04-23 20:40:56]  I0423 12:39:45.931650    21 net.cpp:122] Setting up ReLU2
[2018-04-23 20:40:56]  I0423 12:39:45.931710    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:56]  I0423 12:39:45.931740    21 net.cpp:137] Memory required for data: 43516288
[2018-04-23 20:40:56]  I0423 12:39:45.931772    21 layer_factory.hpp:77] Creating layer Convolution3
[2018-04-23 20:40:56]  I0423 12:39:45.931829    21 net.cpp:84] Creating Layer Convolution3
[2018-04-23 20:40:56]  I0423 12:39:45.931866    21 net.cpp:406] Convolution3 <- Convolution2
[2018-04-23 20:40:56]  I0423 12:39:45.931924    21 net.cpp:380] Convolution3 -> Convolution3
[2018-04-23 20:40:56]  I0423 12:39:45.934293    21 net.cpp:122] Setting up Convolution3
[2018-04-23 20:40:56]  I0423 12:39:45.934360    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:56]  I0423 12:39:45.934391    21 net.cpp:137] Memory required for data: 47710592
[2018-04-23 20:40:56]  I0423 12:39:45.934430    21 layer_factory.hpp:77] Creating layer BatchNorm3
[2018-04-23 20:40:56]  I0423 12:39:45.934499    21 net.cpp:84] Creating Layer BatchNorm3
[2018-04-23 20:40:56]  I0423 12:39:45.934540    21 net.cpp:406] BatchNorm3 <- Convolution3
[2018-04-23 20:40:56]  I0423 12:39:45.934588    21 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
[2018-04-23 20:40:56]  I0423 12:39:45.935060    21 net.cpp:122] Setting up BatchNorm3
[2018-04-23 20:40:56]  I0423 12:39:45.935114    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:56]  I0423 12:39:45.935142    21 net.cpp:137] Memory required for data: 51904896
[2018-04-23 20:40:56]  I0423 12:39:45.935207    21 layer_factory.hpp:77] Creating layer Scale3
[2018-04-23 20:40:56]  I0423 12:39:45.935250    21 net.cpp:84] Creating Layer Scale3
[2018-04-23 20:40:56]  I0423 12:39:45.935298    21 net.cpp:406] Scale3 <- Convolution3
[2018-04-23 20:40:56]  I0423 12:39:45.935340    21 net.cpp:367] Scale3 -> Convolution3 (in-place)
[2018-04-23 20:40:56]  I0423 12:39:45.935528    21 layer_factory.hpp:77] Creating layer Scale3
[2018-04-23 20:40:56]  I0423 12:39:45.935850    21 net.cpp:122] Setting up Scale3
[2018-04-23 20:40:56]  I0423 12:39:45.935902    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:56]  I0423 12:39:45.935940    21 net.cpp:137] Memory required for data: 56099200
[2018-04-23 20:40:56]  I0423 12:39:45.935987    21 layer_factory.hpp:77] Creating layer Eltwise1
[2018-04-23 20:40:56]  I0423 12:39:45.936053    21 net.cpp:84] Creating Layer Eltwise1
[2018-04-23 20:40:56]  I0423 12:39:45.936089    21 net.cpp:406] Eltwise1 <- Convolution1_ReLU1_0_split_1
[2018-04-23 20:40:56]  I0423 12:39:45.936133    21 net.cpp:406] Eltwise1 <- Convolution3
[2018-04-23 20:40:56]  I0423 12:39:45.936183    21 net.cpp:380] Eltwise1 -> Eltwise1
[2018-04-23 20:40:56]  I0423 12:39:45.936313    21 net.cpp:122] Setting up Eltwise1
[2018-04-23 20:40:56]  I0423 12:39:45.936362    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:56]  I0423 12:39:45.936398    21 net.cpp:137] Memory required for data: 60293504
[2018-04-23 20:40:56]  I0423 12:39:45.936424    21 layer_factory.hpp:77] Creating layer ReLU3
[2018-04-23 20:40:56]  I0423 12:39:45.936450    21 net.cpp:84] Creating Layer ReLU3
[2018-04-23 20:40:56]  I0423 12:39:45.936502    21 net.cpp:406] ReLU3 <- Eltwise1
[2018-04-23 20:40:56]  I0423 12:39:45.936553    21 net.cpp:367] ReLU3 -> Eltwise1 (in-place)
[2018-04-23 20:40:56]  I0423 12:39:45.937310    21 net.cpp:122] Setting up ReLU3
[2018-04-23 20:40:56]  I0423 12:39:45.937398    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:56]  I0423 12:39:45.937433    21 net.cpp:137] Memory required for data: 64487808
[2018-04-23 20:40:56]  I0423 12:39:45.937467    21 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
[2018-04-23 20:40:56]  I0423 12:39:45.937544    21 net.cpp:84] Creating Layer Eltwise1_ReLU3_0_split
[2018-04-23 20:40:56]  I0423 12:39:45.937589    21 net.cpp:406] Eltwise1_ReLU3_0_split <- Eltwise1
[2018-04-23 20:40:56]  I0423 12:39:45.937623    21 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
[2018-04-23 20:40:56]  I0423 12:39:45.937664    21 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
[2018-04-23 20:40:56]  I0423 12:39:45.937808    21 net.cpp:122] Setting up Eltwise1_ReLU3_0_split
[2018-04-23 20:40:56]  I0423 12:39:45.937857    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:56]  I0423 12:39:45.937888    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:56]  I0423 12:39:45.937919    21 net.cpp:137] Memory required for data: 72876416
[2018-04-23 20:40:56]  I0423 12:39:45.937947    21 layer_factory.hpp:77] Creating layer Convolution4
[2018-04-23 20:40:56]  I0423 12:39:45.938015    21 net.cpp:84] Creating Layer Convolution4
[2018-04-23 20:40:56]  I0423 12:39:45.938057    21 net.cpp:406] Convolution4 <- Eltwise1_ReLU3_0_split_0
[2018-04-23 20:40:56]  I0423 12:39:45.938123    21 net.cpp:380] Convolution4 -> Convolution4
[2018-04-23 20:40:56]  I0423 12:39:45.940158    21 net.cpp:122] Setting up Convolution4
[2018-04-23 20:40:56]  I0423 12:39:45.940222    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:57]  I0423 12:39:45.940256    21 net.cpp:137] Memory required for data: 77070720
[2018-04-23 20:40:57]  I0423 12:39:45.940310    21 layer_factory.hpp:77] Creating layer BatchNorm4
[2018-04-23 20:40:57]  I0423 12:39:45.940352    21 net.cpp:84] Creating Layer BatchNorm4
[2018-04-23 20:40:57]  I0423 12:39:45.940390    21 net.cpp:406] BatchNorm4 <- Convolution4
[2018-04-23 20:40:57]  I0423 12:39:45.940436    21 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
[2018-04-23 20:40:57]  I0423 12:39:45.940906    21 net.cpp:122] Setting up BatchNorm4
[2018-04-23 20:40:57]  I0423 12:39:45.940963    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:57]  I0423 12:39:45.941002    21 net.cpp:137] Memory required for data: 81265024
[2018-04-23 20:40:57]  I0423 12:39:45.941038    21 layer_factory.hpp:77] Creating layer Scale4
[2018-04-23 20:40:57]  I0423 12:39:45.941092    21 net.cpp:84] Creating Layer Scale4
[2018-04-23 20:40:57]  I0423 12:39:45.941128    21 net.cpp:406] Scale4 <- Convolution4
[2018-04-23 20:40:57]  I0423 12:39:45.941198    21 net.cpp:367] Scale4 -> Convolution4 (in-place)
[2018-04-23 20:40:57]  I0423 12:39:45.941334    21 layer_factory.hpp:77] Creating layer Scale4
[2018-04-23 20:40:57]  I0423 12:39:45.941668    21 net.cpp:122] Setting up Scale4
[2018-04-23 20:40:57]  I0423 12:39:45.941730    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:57]  I0423 12:39:45.941757    21 net.cpp:137] Memory required for data: 85459328
[2018-04-23 20:40:57]  I0423 12:39:45.941787    21 layer_factory.hpp:77] Creating layer ReLU4
[2018-04-23 20:40:57]  I0423 12:39:45.941856    21 net.cpp:84] Creating Layer ReLU4
[2018-04-23 20:40:57]  I0423 12:39:45.941889    21 net.cpp:406] ReLU4 <- Convolution4
[2018-04-23 20:40:57]  I0423 12:39:45.941942    21 net.cpp:367] ReLU4 -> Convolution4 (in-place)
[2018-04-23 20:40:57]  I0423 12:39:45.942349    21 net.cpp:122] Setting up ReLU4
[2018-04-23 20:40:57]  I0423 12:39:45.942417    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:57]  I0423 12:39:45.942466    21 net.cpp:137] Memory required for data: 89653632
[2018-04-23 20:40:57]  I0423 12:39:45.942508    21 layer_factory.hpp:77] Creating layer Convolution5
[2018-04-23 20:40:57]  I0423 12:39:45.942569    21 net.cpp:84] Creating Layer Convolution5
[2018-04-23 20:40:57]  I0423 12:39:45.942611    21 net.cpp:406] Convolution5 <- Convolution4
[2018-04-23 20:40:57]  I0423 12:39:45.942649    21 net.cpp:380] Convolution5 -> Convolution5
[2018-04-23 20:40:57]  I0423 12:39:45.944454    21 net.cpp:122] Setting up Convolution5
[2018-04-23 20:40:57]  I0423 12:39:45.944528    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:57]  I0423 12:39:45.944563    21 net.cpp:137] Memory required for data: 93847936
[2018-04-23 20:40:57]  I0423 12:39:45.944614    21 layer_factory.hpp:77] Creating layer BatchNorm5
[2018-04-23 20:40:57]  I0423 12:39:45.944682    21 net.cpp:84] Creating Layer BatchNorm5
[2018-04-23 20:40:57]  I0423 12:39:45.944713    21 net.cpp:406] BatchNorm5 <- Convolution5
[2018-04-23 20:40:57]  I0423 12:39:45.944754    21 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
[2018-04-23 20:40:57]  I0423 12:39:45.945209    21 net.cpp:122] Setting up BatchNorm5
[2018-04-23 20:40:57]  I0423 12:39:45.945261    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:57]  I0423 12:39:45.945289    21 net.cpp:137] Memory required for data: 98042240
[2018-04-23 20:40:57]  I0423 12:39:45.945364    21 layer_factory.hpp:77] Creating layer Scale5
[2018-04-23 20:40:57]  I0423 12:39:45.945432    21 net.cpp:84] Creating Layer Scale5
[2018-04-23 20:40:57]  I0423 12:39:45.945477    21 net.cpp:406] Scale5 <- Convolution5
[2018-04-23 20:40:57]  I0423 12:39:45.945538    21 net.cpp:367] Scale5 -> Convolution5 (in-place)
[2018-04-23 20:40:57]  I0423 12:39:45.948097    21 layer_factory.hpp:77] Creating layer Scale5
[2018-04-23 20:40:57]  I0423 12:39:45.948698    21 net.cpp:122] Setting up Scale5
[2018-04-23 20:40:57]  I0423 12:39:45.948755    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:57]  I0423 12:39:45.948781    21 net.cpp:137] Memory required for data: 102236544
[2018-04-23 20:40:57]  I0423 12:39:45.948830    21 layer_factory.hpp:77] Creating layer Eltwise2
[2018-04-23 20:40:57]  I0423 12:39:45.948868    21 net.cpp:84] Creating Layer Eltwise2
[2018-04-23 20:40:57]  I0423 12:39:45.948904    21 net.cpp:406] Eltwise2 <- Eltwise1_ReLU3_0_split_1
[2018-04-23 20:40:57]  I0423 12:39:45.948943    21 net.cpp:406] Eltwise2 <- Convolution5
[2018-04-23 20:40:57]  I0423 12:39:45.949014    21 net.cpp:380] Eltwise2 -> Eltwise2
[2018-04-23 20:40:57]  I0423 12:39:45.949131    21 net.cpp:122] Setting up Eltwise2
[2018-04-23 20:40:57]  I0423 12:39:45.949180    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:57]  I0423 12:39:45.949213    21 net.cpp:137] Memory required for data: 106430848
[2018-04-23 20:40:57]  I0423 12:39:45.949246    21 layer_factory.hpp:77] Creating layer ReLU5
[2018-04-23 20:40:57]  I0423 12:39:45.949290    21 net.cpp:84] Creating Layer ReLU5
[2018-04-23 20:40:57]  I0423 12:39:45.949326    21 net.cpp:406] ReLU5 <- Eltwise2
[2018-04-23 20:40:57]  I0423 12:39:45.949400    21 net.cpp:367] ReLU5 -> Eltwise2 (in-place)
[2018-04-23 20:40:57]  I0423 12:39:45.949831    21 net.cpp:122] Setting up ReLU5
[2018-04-23 20:40:57]  I0423 12:39:45.949890    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:57]  I0423 12:39:45.949923    21 net.cpp:137] Memory required for data: 110625152
[2018-04-23 20:40:57]  I0423 12:39:45.949949    21 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
[2018-04-23 20:40:57]  I0423 12:39:45.949987    21 net.cpp:84] Creating Layer Eltwise2_ReLU5_0_split
[2018-04-23 20:40:57]  I0423 12:39:45.950022    21 net.cpp:406] Eltwise2_ReLU5_0_split <- Eltwise2
[2018-04-23 20:40:57]  I0423 12:39:45.950073    21 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
[2018-04-23 20:40:57]  I0423 12:39:45.950117    21 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
[2018-04-23 20:40:57]  I0423 12:39:45.950269    21 net.cpp:122] Setting up Eltwise2_ReLU5_0_split
[2018-04-23 20:40:57]  I0423 12:39:45.950320    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:57]  I0423 12:39:45.950361    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:57]  I0423 12:39:45.950392    21 net.cpp:137] Memory required for data: 119013760
[2018-04-23 20:40:57]  I0423 12:39:45.950434    21 layer_factory.hpp:77] Creating layer Convolution6
[2018-04-23 20:40:57]  I0423 12:39:45.950505    21 net.cpp:84] Creating Layer Convolution6
[2018-04-23 20:40:57]  I0423 12:39:45.950546    21 net.cpp:406] Convolution6 <- Eltwise2_ReLU5_0_split_0
[2018-04-23 20:40:57]  I0423 12:39:45.950618    21 net.cpp:380] Convolution6 -> Convolution6
[2018-04-23 20:40:57]  I0423 12:39:45.952725    21 net.cpp:122] Setting up Convolution6
[2018-04-23 20:40:57]  I0423 12:39:45.952790    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:58]  I0423 12:39:45.952824    21 net.cpp:137] Memory required for data: 123208064
[2018-04-23 20:40:58]  I0423 12:39:45.952888    21 layer_factory.hpp:77] Creating layer BatchNorm6
[2018-04-23 20:40:58]  I0423 12:39:45.952931    21 net.cpp:84] Creating Layer BatchNorm6
[2018-04-23 20:40:58]  I0423 12:39:45.952965    21 net.cpp:406] BatchNorm6 <- Convolution6
[2018-04-23 20:40:58]  I0423 12:39:45.953047    21 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
[2018-04-23 20:40:58]  I0423 12:39:45.953619    21 net.cpp:122] Setting up BatchNorm6
[2018-04-23 20:40:58]  I0423 12:39:45.953675    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:58]  I0423 12:39:45.953722    21 net.cpp:137] Memory required for data: 127402368
[2018-04-23 20:40:58]  I0423 12:39:45.953765    21 layer_factory.hpp:77] Creating layer Scale6
[2018-04-23 20:40:58]  I0423 12:39:45.953815    21 net.cpp:84] Creating Layer Scale6
[2018-04-23 20:40:58]  I0423 12:39:45.953862    21 net.cpp:406] Scale6 <- Convolution6
[2018-04-23 20:40:58]  I0423 12:39:45.953922    21 net.cpp:367] Scale6 -> Convolution6 (in-place)
[2018-04-23 20:40:58]  I0423 12:39:45.954059    21 layer_factory.hpp:77] Creating layer Scale6
[2018-04-23 20:40:58]  I0423 12:39:45.954365    21 net.cpp:122] Setting up Scale6
[2018-04-23 20:40:58]  I0423 12:39:45.954419    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:58]  I0423 12:39:45.954445    21 net.cpp:137] Memory required for data: 131596672
[2018-04-23 20:40:58]  I0423 12:39:45.954485    21 layer_factory.hpp:77] Creating layer ReLU6
[2018-04-23 20:40:58]  I0423 12:39:45.954550    21 net.cpp:84] Creating Layer ReLU6
[2018-04-23 20:40:58]  I0423 12:39:45.954586    21 net.cpp:406] ReLU6 <- Convolution6
[2018-04-23 20:40:58]  I0423 12:39:45.954628    21 net.cpp:367] ReLU6 -> Convolution6 (in-place)
[2018-04-23 20:40:58]  I0423 12:39:45.955230    21 net.cpp:122] Setting up ReLU6
[2018-04-23 20:40:58]  I0423 12:39:45.955294    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:58]  I0423 12:39:45.955323    21 net.cpp:137] Memory required for data: 135790976
[2018-04-23 20:40:58]  I0423 12:39:45.955359    21 layer_factory.hpp:77] Creating layer Convolution7
[2018-04-23 20:40:58]  I0423 12:39:45.955433    21 net.cpp:84] Creating Layer Convolution7
[2018-04-23 20:40:58]  I0423 12:39:45.955471    21 net.cpp:406] Convolution7 <- Convolution6
[2018-04-23 20:40:58]  I0423 12:39:45.955552    21 net.cpp:380] Convolution7 -> Convolution7
[2018-04-23 20:40:58]  I0423 12:39:45.957880    21 net.cpp:122] Setting up Convolution7
[2018-04-23 20:40:58]  I0423 12:39:45.957967    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:58]  I0423 12:39:45.958007    21 net.cpp:137] Memory required for data: 139985280
[2018-04-23 20:40:58]  I0423 12:39:45.958050    21 layer_factory.hpp:77] Creating layer BatchNorm7
[2018-04-23 20:40:58]  I0423 12:39:45.958132    21 net.cpp:84] Creating Layer BatchNorm7
[2018-04-23 20:40:58]  I0423 12:39:45.958184    21 net.cpp:406] BatchNorm7 <- Convolution7
[2018-04-23 20:40:58]  I0423 12:39:45.958241    21 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
[2018-04-23 20:40:58]  I0423 12:39:45.958734    21 net.cpp:122] Setting up BatchNorm7
[2018-04-23 20:40:58]  I0423 12:39:45.958791    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:58]  I0423 12:39:45.958828    21 net.cpp:137] Memory required for data: 144179584
[2018-04-23 20:40:58]  I0423 12:39:45.958866    21 layer_factory.hpp:77] Creating layer Scale7
[2018-04-23 20:40:58]  I0423 12:39:45.958905    21 net.cpp:84] Creating Layer Scale7
[2018-04-23 20:40:58]  I0423 12:39:45.958933    21 net.cpp:406] Scale7 <- Convolution7
[2018-04-23 20:40:58]  I0423 12:39:45.958972    21 net.cpp:367] Scale7 -> Convolution7 (in-place)
[2018-04-23 20:40:58]  I0423 12:39:45.959125    21 layer_factory.hpp:77] Creating layer Scale7
[2018-04-23 20:40:58]  I0423 12:39:45.959429    21 net.cpp:122] Setting up Scale7
[2018-04-23 20:40:58]  I0423 12:39:45.959482    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:58]  I0423 12:39:45.959520    21 net.cpp:137] Memory required for data: 148373888
[2018-04-23 20:40:58]  I0423 12:39:45.959555    21 layer_factory.hpp:77] Creating layer Eltwise3
[2018-04-23 20:40:58]  I0423 12:39:45.959615    21 net.cpp:84] Creating Layer Eltwise3
[2018-04-23 20:40:58]  I0423 12:39:45.959652    21 net.cpp:406] Eltwise3 <- Eltwise2_ReLU5_0_split_1
[2018-04-23 20:40:58]  I0423 12:39:45.959695    21 net.cpp:406] Eltwise3 <- Convolution7
[2018-04-23 20:40:58]  I0423 12:39:45.959755    21 net.cpp:380] Eltwise3 -> Eltwise3
[2018-04-23 20:40:58]  I0423 12:39:45.959870    21 net.cpp:122] Setting up Eltwise3
[2018-04-23 20:40:58]  I0423 12:39:45.959920    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:58]  I0423 12:39:45.959949    21 net.cpp:137] Memory required for data: 152568192
[2018-04-23 20:40:58]  I0423 12:39:45.959980    21 layer_factory.hpp:77] Creating layer ReLU7
[2018-04-23 20:40:58]  I0423 12:39:45.960194    21 net.cpp:84] Creating Layer ReLU7
[2018-04-23 20:40:58]  I0423 12:39:45.960243    21 net.cpp:406] ReLU7 <- Eltwise3
[2018-04-23 20:40:58]  I0423 12:39:45.960284    21 net.cpp:367] ReLU7 -> Eltwise3 (in-place)
[2018-04-23 20:40:58]  I0423 12:39:45.960711    21 net.cpp:122] Setting up ReLU7
[2018-04-23 20:40:58]  I0423 12:39:45.960769    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:58]  I0423 12:39:45.960808    21 net.cpp:137] Memory required for data: 156762496
[2018-04-23 20:40:58]  I0423 12:39:45.960842    21 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
[2018-04-23 20:40:58]  I0423 12:39:45.960889    21 net.cpp:84] Creating Layer Eltwise3_ReLU7_0_split
[2018-04-23 20:40:58]  I0423 12:39:45.960956    21 net.cpp:406] Eltwise3_ReLU7_0_split <- Eltwise3
[2018-04-23 20:40:58]  I0423 12:39:45.961006    21 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
[2018-04-23 20:40:58]  I0423 12:39:45.961050    21 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
[2018-04-23 20:40:58]  I0423 12:39:45.961215    21 net.cpp:122] Setting up Eltwise3_ReLU7_0_split
[2018-04-23 20:40:58]  I0423 12:39:45.961284    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:58]  I0423 12:39:45.961324    21 net.cpp:129] Top shape: 32 32 32 32 (1048576)
[2018-04-23 20:40:58]  I0423 12:39:45.961388    21 net.cpp:137] Memory required for data: 165151104
[2018-04-23 20:40:58]  I0423 12:39:45.961439    21 layer_factory.hpp:77] Creating layer Convolution8
[2018-04-23 20:40:58]  I0423 12:39:45.961486    21 net.cpp:84] Creating Layer Convolution8
[2018-04-23 20:40:58]  I0423 12:39:45.961534    21 net.cpp:406] Convolution8 <- Eltwise3_ReLU7_0_split_0
[2018-04-23 20:40:58]  I0423 12:39:45.961587    21 net.cpp:380] Convolution8 -> Convolution8
[2018-04-23 20:40:58]  I0423 12:39:45.963877    21 net.cpp:122] Setting up Convolution8
[2018-04-23 20:40:58]  I0423 12:39:45.963913    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:40:58]  I0423 12:39:45.963918    21 net.cpp:137] Memory required for data: 167248256
[2018-04-23 20:40:58]  I0423 12:39:45.963932    21 layer_factory.hpp:77] Creating layer BatchNorm8
[2018-04-23 20:40:59]  I0423 12:39:45.963953    21 net.cpp:84] Creating Layer BatchNorm8
[2018-04-23 20:40:59]  I0423 12:39:45.963965    21 net.cpp:406] BatchNorm8 <- Convolution8
[2018-04-23 20:40:59]  I0423 12:39:45.963981    21 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
[2018-04-23 20:40:59]  I0423 12:39:45.964354    21 net.cpp:122] Setting up BatchNorm8
[2018-04-23 20:40:59]  I0423 12:39:45.964371    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:40:59]  I0423 12:39:45.964376    21 net.cpp:137] Memory required for data: 169345408
[2018-04-23 20:40:59]  I0423 12:39:45.964385    21 layer_factory.hpp:77] Creating layer Scale8
[2018-04-23 20:40:59]  I0423 12:39:45.964395    21 net.cpp:84] Creating Layer Scale8
[2018-04-23 20:40:59]  I0423 12:39:45.964399    21 net.cpp:406] Scale8 <- Convolution8
[2018-04-23 20:40:59]  I0423 12:39:45.964406    21 net.cpp:367] Scale8 -> Convolution8 (in-place)
[2018-04-23 20:40:59]  I0423 12:39:45.964480    21 layer_factory.hpp:77] Creating layer Scale8
[2018-04-23 20:40:59]  I0423 12:39:45.964695    21 net.cpp:122] Setting up Scale8
[2018-04-23 20:40:59]  I0423 12:39:45.964712    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:40:59]  I0423 12:39:45.964716    21 net.cpp:137] Memory required for data: 171442560
[2018-04-23 20:40:59]  I0423 12:39:45.964723    21 layer_factory.hpp:77] Creating layer Convolution9
[2018-04-23 20:40:59]  I0423 12:39:45.964743    21 net.cpp:84] Creating Layer Convolution9
[2018-04-23 20:40:59]  I0423 12:39:45.964751    21 net.cpp:406] Convolution9 <- Eltwise3_ReLU7_0_split_1
[2018-04-23 20:40:59]  I0423 12:39:45.964764    21 net.cpp:380] Convolution9 -> Convolution9
[2018-04-23 20:40:59]  I0423 12:39:45.966759    21 net.cpp:122] Setting up Convolution9
[2018-04-23 20:40:59]  I0423 12:39:45.966786    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:40:59]  I0423 12:39:45.966791    21 net.cpp:137] Memory required for data: 173539712
[2018-04-23 20:40:59]  I0423 12:39:45.966800    21 layer_factory.hpp:77] Creating layer BatchNorm9
[2018-04-23 20:40:59]  I0423 12:39:45.966815    21 net.cpp:84] Creating Layer BatchNorm9
[2018-04-23 20:40:59]  I0423 12:39:45.966821    21 net.cpp:406] BatchNorm9 <- Convolution9
[2018-04-23 20:40:59]  I0423 12:39:45.966828    21 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
[2018-04-23 20:40:59]  I0423 12:39:45.967175    21 net.cpp:122] Setting up BatchNorm9
[2018-04-23 20:40:59]  I0423 12:39:45.967200    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:40:59]  I0423 12:39:45.967203    21 net.cpp:137] Memory required for data: 175636864
[2018-04-23 20:40:59]  I0423 12:39:45.967212    21 layer_factory.hpp:77] Creating layer Scale9
[2018-04-23 20:40:59]  I0423 12:39:45.967222    21 net.cpp:84] Creating Layer Scale9
[2018-04-23 20:40:59]  I0423 12:39:45.967226    21 net.cpp:406] Scale9 <- Convolution9
[2018-04-23 20:40:59]  I0423 12:39:45.967233    21 net.cpp:367] Scale9 -> Convolution9 (in-place)
[2018-04-23 20:40:59]  I0423 12:39:45.967303    21 layer_factory.hpp:77] Creating layer Scale9
[2018-04-23 20:40:59]  I0423 12:39:45.967536    21 net.cpp:122] Setting up Scale9
[2018-04-23 20:40:59]  I0423 12:39:45.967555    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:40:59]  I0423 12:39:45.967558    21 net.cpp:137] Memory required for data: 177734016
[2018-04-23 20:40:59]  I0423 12:39:45.967566    21 layer_factory.hpp:77] Creating layer ReLU8
[2018-04-23 20:40:59]  I0423 12:39:45.967581    21 net.cpp:84] Creating Layer ReLU8
[2018-04-23 20:40:59]  I0423 12:39:45.967586    21 net.cpp:406] ReLU8 <- Convolution9
[2018-04-23 20:40:59]  I0423 12:39:45.967592    21 net.cpp:367] ReLU8 -> Convolution9 (in-place)
[2018-04-23 20:40:59]  I0423 12:39:45.967947    21 net.cpp:122] Setting up ReLU8
[2018-04-23 20:40:59]  I0423 12:39:45.967967    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:40:59]  I0423 12:39:45.967972    21 net.cpp:137] Memory required for data: 179831168
[2018-04-23 20:40:59]  I0423 12:39:45.967975    21 layer_factory.hpp:77] Creating layer Convolution10
[2018-04-23 20:40:59]  I0423 12:39:45.968000    21 net.cpp:84] Creating Layer Convolution10
[2018-04-23 20:40:59]  I0423 12:39:45.968005    21 net.cpp:406] Convolution10 <- Convolution9
[2018-04-23 20:40:59]  I0423 12:39:45.968014    21 net.cpp:380] Convolution10 -> Convolution10
[2018-04-23 20:40:59]  I0423 12:39:45.970407    21 net.cpp:122] Setting up Convolution10
[2018-04-23 20:40:59]  I0423 12:39:45.970433    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:40:59]  I0423 12:39:45.970438    21 net.cpp:137] Memory required for data: 181928320
[2018-04-23 20:40:59]  I0423 12:39:45.970470    21 layer_factory.hpp:77] Creating layer BatchNorm10
[2018-04-23 20:40:59]  I0423 12:39:45.970487    21 net.cpp:84] Creating Layer BatchNorm10
[2018-04-23 20:40:59]  I0423 12:39:45.970500    21 net.cpp:406] BatchNorm10 <- Convolution10
[2018-04-23 20:40:59]  I0423 12:39:45.970508    21 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
[2018-04-23 20:40:59]  I0423 12:39:45.970949    21 net.cpp:122] Setting up BatchNorm10
[2018-04-23 20:40:59]  I0423 12:39:45.970973    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:40:59]  I0423 12:39:45.970976    21 net.cpp:137] Memory required for data: 184025472
[2018-04-23 20:40:59]  I0423 12:39:45.970986    21 layer_factory.hpp:77] Creating layer Scale10
[2018-04-23 20:40:59]  I0423 12:39:45.971001    21 net.cpp:84] Creating Layer Scale10
[2018-04-23 20:40:59]  I0423 12:39:45.971007    21 net.cpp:406] Scale10 <- Convolution10
[2018-04-23 20:40:59]  I0423 12:39:45.971014    21 net.cpp:367] Scale10 -> Convolution10 (in-place)
[2018-04-23 20:40:59]  I0423 12:39:45.971228    21 layer_factory.hpp:77] Creating layer Scale10
[2018-04-23 20:40:59]  I0423 12:39:45.971544    21 net.cpp:122] Setting up Scale10
[2018-04-23 20:40:59]  I0423 12:39:45.971577    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:40:59]  I0423 12:39:45.971593    21 net.cpp:137] Memory required for data: 186122624
[2018-04-23 20:40:59]  I0423 12:39:45.971601    21 layer_factory.hpp:77] Creating layer Eltwise4
[2018-04-23 20:40:59]  I0423 12:39:45.971700    21 net.cpp:84] Creating Layer Eltwise4
[2018-04-23 20:40:59]  I0423 12:39:45.971721    21 net.cpp:406] Eltwise4 <- Convolution8
[2018-04-23 20:40:59]  I0423 12:39:45.971729    21 net.cpp:406] Eltwise4 <- Convolution10
[2018-04-23 20:40:59]  I0423 12:39:45.971736    21 net.cpp:380] Eltwise4 -> Eltwise4
[2018-04-23 20:40:59]  I0423 12:39:45.971828    21 net.cpp:122] Setting up Eltwise4
[2018-04-23 20:40:59]  I0423 12:39:45.971849    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:40:59]  I0423 12:39:45.971854    21 net.cpp:137] Memory required for data: 188219776
[2018-04-23 20:40:59]  I0423 12:39:45.971858    21 layer_factory.hpp:77] Creating layer ReLU9
[2018-04-23 20:40:59]  I0423 12:39:45.971866    21 net.cpp:84] Creating Layer ReLU9
[2018-04-23 20:40:59]  I0423 12:39:45.971954    21 net.cpp:406] ReLU9 <- Eltwise4
[2018-04-23 20:40:59]  I0423 12:39:45.971985    21 net.cpp:367] ReLU9 -> Eltwise4 (in-place)
[2018-04-23 20:40:59]  I0423 12:39:45.972647    21 net.cpp:122] Setting up ReLU9
[2018-04-23 20:40:59]  I0423 12:39:45.972672    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:00]  I0423 12:39:45.972677    21 net.cpp:137] Memory required for data: 190316928
[2018-04-23 20:41:00]  I0423 12:39:45.972682    21 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
[2018-04-23 20:41:00]  I0423 12:39:45.972692    21 net.cpp:84] Creating Layer Eltwise4_ReLU9_0_split
[2018-04-23 20:41:00]  I0423 12:39:45.972697    21 net.cpp:406] Eltwise4_ReLU9_0_split <- Eltwise4
[2018-04-23 20:41:00]  I0423 12:39:45.972829    21 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
[2018-04-23 20:41:00]  I0423 12:39:45.972856    21 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
[2018-04-23 20:41:00]  I0423 12:39:45.973004    21 net.cpp:122] Setting up Eltwise4_ReLU9_0_split
[2018-04-23 20:41:00]  I0423 12:39:45.973024    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:00]  I0423 12:39:45.973031    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:00]  I0423 12:39:45.973034    21 net.cpp:137] Memory required for data: 194511232
[2018-04-23 20:41:00]  I0423 12:39:45.973039    21 layer_factory.hpp:77] Creating layer Convolution11
[2018-04-23 20:41:00]  I0423 12:39:45.973140    21 net.cpp:84] Creating Layer Convolution11
[2018-04-23 20:41:00]  I0423 12:39:45.973161    21 net.cpp:406] Convolution11 <- Eltwise4_ReLU9_0_split_0
[2018-04-23 20:41:00]  I0423 12:39:45.973184    21 net.cpp:380] Convolution11 -> Convolution11
[2018-04-23 20:41:00]  I0423 12:39:45.975301    21 net.cpp:122] Setting up Convolution11
[2018-04-23 20:41:00]  I0423 12:39:45.975330    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:00]  I0423 12:39:45.975337    21 net.cpp:137] Memory required for data: 196608384
[2018-04-23 20:41:00]  I0423 12:39:45.975345    21 layer_factory.hpp:77] Creating layer BatchNorm11
[2018-04-23 20:41:00]  I0423 12:39:45.975354    21 net.cpp:84] Creating Layer BatchNorm11
[2018-04-23 20:41:00]  I0423 12:39:45.975360    21 net.cpp:406] BatchNorm11 <- Convolution11
[2018-04-23 20:41:00]  I0423 12:39:45.975482    21 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
[2018-04-23 20:41:00]  I0423 12:39:45.975956    21 net.cpp:122] Setting up BatchNorm11
[2018-04-23 20:41:00]  I0423 12:39:45.975977    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:00]  I0423 12:39:45.975982    21 net.cpp:137] Memory required for data: 198705536
[2018-04-23 20:41:00]  I0423 12:39:45.975991    21 layer_factory.hpp:77] Creating layer Scale11
[2018-04-23 20:41:00]  I0423 12:39:45.975999    21 net.cpp:84] Creating Layer Scale11
[2018-04-23 20:41:00]  I0423 12:39:45.976004    21 net.cpp:406] Scale11 <- Convolution11
[2018-04-23 20:41:00]  I0423 12:39:45.976011    21 net.cpp:367] Scale11 -> Convolution11 (in-place)
[2018-04-23 20:41:00]  I0423 12:39:45.976089    21 layer_factory.hpp:77] Creating layer Scale11
[2018-04-23 20:41:00]  I0423 12:39:45.976450    21 net.cpp:122] Setting up Scale11
[2018-04-23 20:41:00]  I0423 12:39:45.976477    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:00]  I0423 12:39:45.976482    21 net.cpp:137] Memory required for data: 200802688
[2018-04-23 20:41:00]  I0423 12:39:45.976490    21 layer_factory.hpp:77] Creating layer ReLU10
[2018-04-23 20:41:00]  I0423 12:39:45.976505    21 net.cpp:84] Creating Layer ReLU10
[2018-04-23 20:41:00]  I0423 12:39:45.976511    21 net.cpp:406] ReLU10 <- Convolution11
[2018-04-23 20:41:00]  I0423 12:39:45.976516    21 net.cpp:367] ReLU10 -> Convolution11 (in-place)
[2018-04-23 20:41:00]  I0423 12:39:45.976927    21 net.cpp:122] Setting up ReLU10
[2018-04-23 20:41:00]  I0423 12:39:45.976950    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:00]  I0423 12:39:45.976954    21 net.cpp:137] Memory required for data: 202899840
[2018-04-23 20:41:00]  I0423 12:39:45.976959    21 layer_factory.hpp:77] Creating layer Convolution12
[2018-04-23 20:41:00]  I0423 12:39:45.976977    21 net.cpp:84] Creating Layer Convolution12
[2018-04-23 20:41:00]  I0423 12:39:45.976984    21 net.cpp:406] Convolution12 <- Convolution11
[2018-04-23 20:41:00]  I0423 12:39:45.976999    21 net.cpp:380] Convolution12 -> Convolution12
[2018-04-23 20:41:00]  I0423 12:39:45.979609    21 net.cpp:122] Setting up Convolution12
[2018-04-23 20:41:00]  I0423 12:39:45.979634    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:00]  I0423 12:39:45.979640    21 net.cpp:137] Memory required for data: 204996992
[2018-04-23 20:41:00]  I0423 12:39:45.979647    21 layer_factory.hpp:77] Creating layer BatchNorm12
[2018-04-23 20:41:00]  I0423 12:39:45.979660    21 net.cpp:84] Creating Layer BatchNorm12
[2018-04-23 20:41:00]  I0423 12:39:45.979665    21 net.cpp:406] BatchNorm12 <- Convolution12
[2018-04-23 20:41:00]  I0423 12:39:45.979672    21 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
[2018-04-23 20:41:00]  I0423 12:39:45.980069    21 net.cpp:122] Setting up BatchNorm12
[2018-04-23 20:41:00]  I0423 12:39:45.980092    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:00]  I0423 12:39:45.980095    21 net.cpp:137] Memory required for data: 207094144
[2018-04-23 20:41:00]  I0423 12:39:45.980105    21 layer_factory.hpp:77] Creating layer Scale12
[2018-04-23 20:41:00]  I0423 12:39:45.980132    21 net.cpp:84] Creating Layer Scale12
[2018-04-23 20:41:00]  I0423 12:39:45.980139    21 net.cpp:406] Scale12 <- Convolution12
[2018-04-23 20:41:00]  I0423 12:39:45.980248    21 net.cpp:367] Scale12 -> Convolution12 (in-place)
[2018-04-23 20:41:00]  I0423 12:39:45.980633    21 layer_factory.hpp:77] Creating layer Scale12
[2018-04-23 20:41:00]  I0423 12:39:45.980890    21 net.cpp:122] Setting up Scale12
[2018-04-23 20:41:00]  I0423 12:39:45.980912    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:00]  I0423 12:39:45.980916    21 net.cpp:137] Memory required for data: 209191296
[2018-04-23 20:41:00]  I0423 12:39:45.980926    21 layer_factory.hpp:77] Creating layer Eltwise5
[2018-04-23 20:41:00]  I0423 12:39:45.980943    21 net.cpp:84] Creating Layer Eltwise5
[2018-04-23 20:41:00]  I0423 12:39:45.981039    21 net.cpp:406] Eltwise5 <- Eltwise4_ReLU9_0_split_1
[2018-04-23 20:41:00]  I0423 12:39:45.981058    21 net.cpp:406] Eltwise5 <- Convolution12
[2018-04-23 20:41:00]  I0423 12:39:45.981078    21 net.cpp:380] Eltwise5 -> Eltwise5
[2018-04-23 20:41:00]  I0423 12:39:45.981161    21 net.cpp:122] Setting up Eltwise5
[2018-04-23 20:41:00]  I0423 12:39:45.981181    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:00]  I0423 12:39:45.981185    21 net.cpp:137] Memory required for data: 211288448
[2018-04-23 20:41:00]  I0423 12:39:45.981191    21 layer_factory.hpp:77] Creating layer ReLU11
[2018-04-23 20:41:00]  I0423 12:39:45.981199    21 net.cpp:84] Creating Layer ReLU11
[2018-04-23 20:41:00]  I0423 12:39:45.981284    21 net.cpp:406] ReLU11 <- Eltwise5
[2018-04-23 20:41:00]  I0423 12:39:45.981396    21 net.cpp:367] ReLU11 -> Eltwise5 (in-place)
[2018-04-23 20:41:01]  I0423 12:39:45.981961    21 net.cpp:122] Setting up ReLU11
[2018-04-23 20:41:01]  I0423 12:39:45.981983    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:01]  I0423 12:39:45.981988    21 net.cpp:137] Memory required for data: 213385600
[2018-04-23 20:41:01]  I0423 12:39:45.981993    21 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
[2018-04-23 20:41:01]  I0423 12:39:45.982002    21 net.cpp:84] Creating Layer Eltwise5_ReLU11_0_split
[2018-04-23 20:41:01]  I0423 12:39:45.982007    21 net.cpp:406] Eltwise5_ReLU11_0_split <- Eltwise5
[2018-04-23 20:41:01]  I0423 12:39:45.982025    21 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
[2018-04-23 20:41:01]  I0423 12:39:45.982043    21 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
[2018-04-23 20:41:01]  I0423 12:39:45.982218    21 net.cpp:122] Setting up Eltwise5_ReLU11_0_split
[2018-04-23 20:41:01]  I0423 12:39:45.982239    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:01]  I0423 12:39:45.982245    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:01]  I0423 12:39:45.982249    21 net.cpp:137] Memory required for data: 217579904
[2018-04-23 20:41:01]  I0423 12:39:45.982254    21 layer_factory.hpp:77] Creating layer Convolution13
[2018-04-23 20:41:01]  I0423 12:39:45.982365    21 net.cpp:84] Creating Layer Convolution13
[2018-04-23 20:41:01]  I0423 12:39:45.982384    21 net.cpp:406] Convolution13 <- Eltwise5_ReLU11_0_split_0
[2018-04-23 20:41:01]  I0423 12:39:45.982401    21 net.cpp:380] Convolution13 -> Convolution13
[2018-04-23 20:41:01]  I0423 12:39:45.985424    21 net.cpp:122] Setting up Convolution13
[2018-04-23 20:41:01]  I0423 12:39:45.985481    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:01]  I0423 12:39:45.985486    21 net.cpp:137] Memory required for data: 219677056
[2018-04-23 20:41:01]  I0423 12:39:45.985502    21 layer_factory.hpp:77] Creating layer BatchNorm13
[2018-04-23 20:41:01]  I0423 12:39:45.985512    21 net.cpp:84] Creating Layer BatchNorm13
[2018-04-23 20:41:01]  I0423 12:39:45.985517    21 net.cpp:406] BatchNorm13 <- Convolution13
[2018-04-23 20:41:01]  I0423 12:39:45.985571    21 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
[2018-04-23 20:41:01]  I0423 12:39:45.986136    21 net.cpp:122] Setting up BatchNorm13
[2018-04-23 20:41:01]  I0423 12:39:45.986161    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:01]  I0423 12:39:45.986166    21 net.cpp:137] Memory required for data: 221774208
[2018-04-23 20:41:01]  I0423 12:39:45.986176    21 layer_factory.hpp:77] Creating layer Scale13
[2018-04-23 20:41:01]  I0423 12:39:45.986183    21 net.cpp:84] Creating Layer Scale13
[2018-04-23 20:41:01]  I0423 12:39:45.986189    21 net.cpp:406] Scale13 <- Convolution13
[2018-04-23 20:41:01]  I0423 12:39:45.986196    21 net.cpp:367] Scale13 -> Convolution13 (in-place)
[2018-04-23 20:41:01]  I0423 12:39:45.986269    21 layer_factory.hpp:77] Creating layer Scale13
[2018-04-23 20:41:01]  I0423 12:39:45.986454    21 net.cpp:122] Setting up Scale13
[2018-04-23 20:41:01]  I0423 12:39:45.986474    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:01]  I0423 12:39:45.986479    21 net.cpp:137] Memory required for data: 223871360
[2018-04-23 20:41:01]  I0423 12:39:45.986486    21 layer_factory.hpp:77] Creating layer ReLU12
[2018-04-23 20:41:01]  I0423 12:39:45.986500    21 net.cpp:84] Creating Layer ReLU12
[2018-04-23 20:41:01]  I0423 12:39:45.986510    21 net.cpp:406] ReLU12 <- Convolution13
[2018-04-23 20:41:01]  I0423 12:39:45.986515    21 net.cpp:367] ReLU12 -> Convolution13 (in-place)
[2018-04-23 20:41:01]  I0423 12:39:45.986958    21 net.cpp:122] Setting up ReLU12
[2018-04-23 20:41:01]  I0423 12:39:45.986989    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:01]  I0423 12:39:45.986994    21 net.cpp:137] Memory required for data: 225968512
[2018-04-23 20:41:01]  I0423 12:39:45.986999    21 layer_factory.hpp:77] Creating layer Convolution14
[2018-04-23 20:41:01]  I0423 12:39:45.987045    21 net.cpp:84] Creating Layer Convolution14
[2018-04-23 20:41:01]  I0423 12:39:45.987051    21 net.cpp:406] Convolution14 <- Convolution13
[2018-04-23 20:41:01]  I0423 12:39:45.987061    21 net.cpp:380] Convolution14 -> Convolution14
[2018-04-23 20:41:01]  I0423 12:39:45.989205    21 net.cpp:122] Setting up Convolution14
[2018-04-23 20:41:01]  I0423 12:39:45.989230    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:01]  I0423 12:39:45.989235    21 net.cpp:137] Memory required for data: 228065664
[2018-04-23 20:41:01]  I0423 12:39:45.989245    21 layer_factory.hpp:77] Creating layer BatchNorm14
[2018-04-23 20:41:01]  I0423 12:39:45.989267    21 net.cpp:84] Creating Layer BatchNorm14
[2018-04-23 20:41:01]  I0423 12:39:45.989274    21 net.cpp:406] BatchNorm14 <- Convolution14
[2018-04-23 20:41:01]  I0423 12:39:45.989286    21 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
[2018-04-23 20:41:01]  I0423 12:39:45.989742    21 net.cpp:122] Setting up BatchNorm14
[2018-04-23 20:41:01]  I0423 12:39:45.989764    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:01]  I0423 12:39:45.989768    21 net.cpp:137] Memory required for data: 230162816
[2018-04-23 20:41:01]  I0423 12:39:45.989778    21 layer_factory.hpp:77] Creating layer Scale14
[2018-04-23 20:41:01]  I0423 12:39:45.989800    21 net.cpp:84] Creating Layer Scale14
[2018-04-23 20:41:02]  I0423 12:39:45.989806    21 net.cpp:406] Scale14 <- Convolution14
[2018-04-23 20:41:02]  I0423 12:39:45.989812    21 net.cpp:367] Scale14 -> Convolution14 (in-place)
[2018-04-23 20:41:02]  I0423 12:39:45.990015    21 layer_factory.hpp:77] Creating layer Scale14
[2018-04-23 20:41:02]  I0423 12:39:45.990320    21 net.cpp:122] Setting up Scale14
[2018-04-23 20:41:02]  I0423 12:39:45.990341    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:02]  I0423 12:39:45.990345    21 net.cpp:137] Memory required for data: 232259968
[2018-04-23 20:41:02]  I0423 12:39:45.990353    21 layer_factory.hpp:77] Creating layer Eltwise6
[2018-04-23 20:41:02]  I0423 12:39:45.990468    21 net.cpp:84] Creating Layer Eltwise6
[2018-04-23 20:41:02]  I0423 12:39:45.990487    21 net.cpp:406] Eltwise6 <- Eltwise5_ReLU11_0_split_1
[2018-04-23 20:41:02]  I0423 12:39:45.990502    21 net.cpp:406] Eltwise6 <- Convolution14
[2018-04-23 20:41:02]  I0423 12:39:45.990510    21 net.cpp:380] Eltwise6 -> Eltwise6
[2018-04-23 20:41:02]  I0423 12:39:45.990593    21 net.cpp:122] Setting up Eltwise6
[2018-04-23 20:41:02]  I0423 12:39:45.990633    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:02]  I0423 12:39:45.990646    21 net.cpp:137] Memory required for data: 234357120
[2018-04-23 20:41:02]  I0423 12:39:45.990651    21 layer_factory.hpp:77] Creating layer ReLU13
[2018-04-23 20:41:02]  I0423 12:39:45.990659    21 net.cpp:84] Creating Layer ReLU13
[2018-04-23 20:41:02]  I0423 12:39:45.990664    21 net.cpp:406] ReLU13 <- Eltwise6
[2018-04-23 20:41:02]  I0423 12:39:45.990758    21 net.cpp:367] ReLU13 -> Eltwise6 (in-place)
[2018-04-23 20:41:02]  I0423 12:39:45.991300    21 net.cpp:122] Setting up ReLU13
[2018-04-23 20:41:02]  I0423 12:39:45.991328    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:02]  I0423 12:39:45.991333    21 net.cpp:137] Memory required for data: 236454272
[2018-04-23 20:41:02]  I0423 12:39:45.991338    21 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
[2018-04-23 20:41:02]  I0423 12:39:45.991358    21 net.cpp:84] Creating Layer Eltwise6_ReLU13_0_split
[2018-04-23 20:41:02]  I0423 12:39:45.991466    21 net.cpp:406] Eltwise6_ReLU13_0_split <- Eltwise6
[2018-04-23 20:41:02]  I0423 12:39:45.991478    21 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
[2018-04-23 20:41:02]  I0423 12:39:45.991554    21 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
[2018-04-23 20:41:02]  I0423 12:39:45.991751    21 net.cpp:122] Setting up Eltwise6_ReLU13_0_split
[2018-04-23 20:41:02]  I0423 12:39:45.991837    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:02]  I0423 12:39:45.991843    21 net.cpp:129] Top shape: 32 64 16 16 (524288)
[2018-04-23 20:41:02]  I0423 12:39:45.991847    21 net.cpp:137] Memory required for data: 240648576
[2018-04-23 20:41:02]  I0423 12:39:45.991852    21 layer_factory.hpp:77] Creating layer Convolution15
[2018-04-23 20:41:02]  I0423 12:39:45.991873    21 net.cpp:84] Creating Layer Convolution15
[2018-04-23 20:41:02]  I0423 12:39:45.991885    21 net.cpp:406] Convolution15 <- Eltwise6_ReLU13_0_split_0
[2018-04-23 20:41:02]  I0423 12:39:45.991895    21 net.cpp:380] Convolution15 -> Convolution15
[2018-04-23 20:41:02]  I0423 12:39:45.994019    21 net.cpp:122] Setting up Convolution15
[2018-04-23 20:41:02]  I0423 12:39:45.994045    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:02]  I0423 12:39:45.994050    21 net.cpp:137] Memory required for data: 241697152
[2018-04-23 20:41:02]  I0423 12:39:45.994060    21 layer_factory.hpp:77] Creating layer BatchNorm15
[2018-04-23 20:41:02]  I0423 12:39:45.994117    21 net.cpp:84] Creating Layer BatchNorm15
[2018-04-23 20:41:02]  I0423 12:39:45.994124    21 net.cpp:406] BatchNorm15 <- Convolution15
[2018-04-23 20:41:02]  I0423 12:39:45.994132    21 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
[2018-04-23 20:41:02]  I0423 12:39:45.994760    21 net.cpp:122] Setting up BatchNorm15
[2018-04-23 20:41:02]  I0423 12:39:45.994777    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:02]  I0423 12:39:45.994781    21 net.cpp:137] Memory required for data: 242745728
[2018-04-23 20:41:02]  I0423 12:39:45.994791    21 layer_factory.hpp:77] Creating layer Scale15
[2018-04-23 20:41:02]  I0423 12:39:45.994798    21 net.cpp:84] Creating Layer Scale15
[2018-04-23 20:41:02]  I0423 12:39:45.994802    21 net.cpp:406] Scale15 <- Convolution15
[2018-04-23 20:41:02]  I0423 12:39:45.994809    21 net.cpp:367] Scale15 -> Convolution15 (in-place)
[2018-04-23 20:41:02]  I0423 12:39:45.994978    21 layer_factory.hpp:77] Creating layer Scale15
[2018-04-23 20:41:02]  I0423 12:39:45.995297    21 net.cpp:122] Setting up Scale15
[2018-04-23 20:41:02]  I0423 12:39:45.995314    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:02]  I0423 12:39:45.995319    21 net.cpp:137] Memory required for data: 243794304
[2018-04-23 20:41:02]  I0423 12:39:45.995326    21 layer_factory.hpp:77] Creating layer Convolution16
[2018-04-23 20:41:02]  I0423 12:39:45.995350    21 net.cpp:84] Creating Layer Convolution16
[2018-04-23 20:41:02]  I0423 12:39:45.995362    21 net.cpp:406] Convolution16 <- Eltwise6_ReLU13_0_split_1
[2018-04-23 20:41:02]  I0423 12:39:45.995370    21 net.cpp:380] Convolution16 -> Convolution16
[2018-04-23 20:41:02]  I0423 12:39:45.997809    21 net.cpp:122] Setting up Convolution16
[2018-04-23 20:41:02]  I0423 12:39:45.997835    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:02]  I0423 12:39:45.997840    21 net.cpp:137] Memory required for data: 244842880
[2018-04-23 20:41:02]  I0423 12:39:45.997849    21 layer_factory.hpp:77] Creating layer BatchNorm16
[2018-04-23 20:41:02]  I0423 12:39:45.997862    21 net.cpp:84] Creating Layer BatchNorm16
[2018-04-23 20:41:02]  I0423 12:39:45.997867    21 net.cpp:406] BatchNorm16 <- Convolution16
[2018-04-23 20:41:02]  I0423 12:39:45.997884    21 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
[2018-04-23 20:41:03]  I0423 12:39:45.998248    21 net.cpp:122] Setting up BatchNorm16
[2018-04-23 20:41:03]  I0423 12:39:45.998266    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:03]  I0423 12:39:45.998270    21 net.cpp:137] Memory required for data: 245891456
[2018-04-23 20:41:03]  I0423 12:39:45.998280    21 layer_factory.hpp:77] Creating layer Scale16
[2018-04-23 20:41:03]  I0423 12:39:45.998296    21 net.cpp:84] Creating Layer Scale16
[2018-04-23 20:41:03]  I0423 12:39:45.998301    21 net.cpp:406] Scale16 <- Convolution16
[2018-04-23 20:41:03]  I0423 12:39:45.998306    21 net.cpp:367] Scale16 -> Convolution16 (in-place)
[2018-04-23 20:41:03]  I0423 12:39:45.998422    21 layer_factory.hpp:77] Creating layer Scale16
[2018-04-23 20:41:03]  I0423 12:39:45.998661    21 net.cpp:122] Setting up Scale16
[2018-04-23 20:41:03]  I0423 12:39:45.998677    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:03]  I0423 12:39:45.998682    21 net.cpp:137] Memory required for data: 246940032
[2018-04-23 20:41:03]  I0423 12:39:45.998689    21 layer_factory.hpp:77] Creating layer ReLU14
[2018-04-23 20:41:03]  I0423 12:39:45.998697    21 net.cpp:84] Creating Layer ReLU14
[2018-04-23 20:41:03]  I0423 12:39:45.998702    21 net.cpp:406] ReLU14 <- Convolution16
[2018-04-23 20:41:03]  I0423 12:39:45.998714    21 net.cpp:367] ReLU14 -> Convolution16 (in-place)
[2018-04-23 20:41:03]  I0423 12:39:45.999053    21 net.cpp:122] Setting up ReLU14
[2018-04-23 20:41:03]  I0423 12:39:45.999073    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:03]  I0423 12:39:45.999078    21 net.cpp:137] Memory required for data: 247988608
[2018-04-23 20:41:03]  I0423 12:39:45.999083    21 layer_factory.hpp:77] Creating layer Convolution17
[2018-04-23 20:41:03]  I0423 12:39:45.999096    21 net.cpp:84] Creating Layer Convolution17
[2018-04-23 20:41:03]  I0423 12:39:45.999104    21 net.cpp:406] Convolution17 <- Convolution16
[2018-04-23 20:41:03]  I0423 12:39:45.999114    21 net.cpp:380] Convolution17 -> Convolution17
[2018-04-23 20:41:03]  I0423 12:39:46.002755    21 net.cpp:122] Setting up Convolution17
[2018-04-23 20:41:03]  I0423 12:39:46.002789    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:03]  I0423 12:39:46.002794    21 net.cpp:137] Memory required for data: 249037184
[2018-04-23 20:41:03]  I0423 12:39:46.002804    21 layer_factory.hpp:77] Creating layer BatchNorm17
[2018-04-23 20:41:03]  I0423 12:39:46.002812    21 net.cpp:84] Creating Layer BatchNorm17
[2018-04-23 20:41:03]  I0423 12:39:46.002825    21 net.cpp:406] BatchNorm17 <- Convolution17
[2018-04-23 20:41:03]  I0423 12:39:46.002832    21 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
[2018-04-23 20:41:03]  I0423 12:39:46.003378    21 net.cpp:122] Setting up BatchNorm17
[2018-04-23 20:41:03]  I0423 12:39:46.003402    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:03]  I0423 12:39:46.003407    21 net.cpp:137] Memory required for data: 250085760
[2018-04-23 20:41:03]  I0423 12:39:46.003417    21 layer_factory.hpp:77] Creating layer Scale17
[2018-04-23 20:41:03]  I0423 12:39:46.003432    21 net.cpp:84] Creating Layer Scale17
[2018-04-23 20:41:03]  I0423 12:39:46.003438    21 net.cpp:406] Scale17 <- Convolution17
[2018-04-23 20:41:03]  I0423 12:39:46.003445    21 net.cpp:367] Scale17 -> Convolution17 (in-place)
[2018-04-23 20:41:03]  I0423 12:39:46.003535    21 layer_factory.hpp:77] Creating layer Scale17
[2018-04-23 20:41:03]  I0423 12:39:46.004221    21 net.cpp:122] Setting up Scale17
[2018-04-23 20:41:03]  I0423 12:39:46.004242    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:03]  I0423 12:39:46.004246    21 net.cpp:137] Memory required for data: 251134336
[2018-04-23 20:41:03]  I0423 12:39:46.004256    21 layer_factory.hpp:77] Creating layer Eltwise7
[2018-04-23 20:41:03]  I0423 12:39:46.004325    21 net.cpp:84] Creating Layer Eltwise7
[2018-04-23 20:41:03]  I0423 12:39:46.004333    21 net.cpp:406] Eltwise7 <- Convolution15
[2018-04-23 20:41:03]  I0423 12:39:46.004338    21 net.cpp:406] Eltwise7 <- Convolution17
[2018-04-23 20:41:03]  I0423 12:39:46.004345    21 net.cpp:380] Eltwise7 -> Eltwise7
[2018-04-23 20:41:03]  I0423 12:39:46.004433    21 net.cpp:122] Setting up Eltwise7
[2018-04-23 20:41:03]  I0423 12:39:46.004443    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:03]  I0423 12:39:46.004447    21 net.cpp:137] Memory required for data: 252182912
[2018-04-23 20:41:03]  I0423 12:39:46.004452    21 layer_factory.hpp:77] Creating layer ReLU15
[2018-04-23 20:41:03]  I0423 12:39:46.004459    21 net.cpp:84] Creating Layer ReLU15
[2018-04-23 20:41:03]  I0423 12:39:46.004465    21 net.cpp:406] ReLU15 <- Eltwise7
[2018-04-23 20:41:03]  I0423 12:39:46.004478    21 net.cpp:367] ReLU15 -> Eltwise7 (in-place)
[2018-04-23 20:41:04]  I0423 12:39:46.004964    21 net.cpp:122] Setting up ReLU15
[2018-04-23 20:41:04]  I0423 12:39:46.004987    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:04]  I0423 12:39:46.004992    21 net.cpp:137] Memory required for data: 253231488
[2018-04-23 20:41:04]  I0423 12:39:46.004997    21 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
[2018-04-23 20:41:04]  I0423 12:39:46.005007    21 net.cpp:84] Creating Layer Eltwise7_ReLU15_0_split
[2018-04-23 20:41:04]  I0423 12:39:46.005012    21 net.cpp:406] Eltwise7_ReLU15_0_split <- Eltwise7
[2018-04-23 20:41:04]  I0423 12:39:46.005025    21 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
[2018-04-23 20:41:04]  I0423 12:39:46.005035    21 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
[2018-04-23 20:41:04]  I0423 12:39:46.005365    21 net.cpp:122] Setting up Eltwise7_ReLU15_0_split
[2018-04-23 20:41:04]  I0423 12:39:46.005389    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:04]  I0423 12:39:46.005406    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:04]  I0423 12:39:46.005409    21 net.cpp:137] Memory required for data: 255328640
[2018-04-23 20:41:04]  I0423 12:39:46.005415    21 layer_factory.hpp:77] Creating layer Convolution18
[2018-04-23 20:41:04]  I0423 12:39:46.005522    21 net.cpp:84] Creating Layer Convolution18
[2018-04-23 20:41:04]  I0423 12:39:46.005561    21 net.cpp:406] Convolution18 <- Eltwise7_ReLU15_0_split_0
[2018-04-23 20:41:04]  I0423 12:39:46.005575    21 net.cpp:380] Convolution18 -> Convolution18
[2018-04-23 20:41:04]  I0423 12:39:46.009308    21 net.cpp:122] Setting up Convolution18
[2018-04-23 20:41:04]  I0423 12:39:46.009376    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:04]  I0423 12:39:46.009385    21 net.cpp:137] Memory required for data: 256377216
[2018-04-23 20:41:04]  I0423 12:39:46.009397    21 layer_factory.hpp:77] Creating layer BatchNorm18
[2018-04-23 20:41:04]  I0423 12:39:46.009407    21 net.cpp:84] Creating Layer BatchNorm18
[2018-04-23 20:41:04]  I0423 12:39:46.009413    21 net.cpp:406] BatchNorm18 <- Convolution18
[2018-04-23 20:41:04]  I0423 12:39:46.009428    21 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
[2018-04-23 20:41:04]  I0423 12:39:46.009924    21 net.cpp:122] Setting up BatchNorm18
[2018-04-23 20:41:04]  I0423 12:39:46.009948    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:04]  I0423 12:39:46.009953    21 net.cpp:137] Memory required for data: 257425792
[2018-04-23 20:41:04]  I0423 12:39:46.009963    21 layer_factory.hpp:77] Creating layer Scale18
[2018-04-23 20:41:04]  I0423 12:39:46.009971    21 net.cpp:84] Creating Layer Scale18
[2018-04-23 20:41:04]  I0423 12:39:46.009979    21 net.cpp:406] Scale18 <- Convolution18
[2018-04-23 20:41:04]  I0423 12:39:46.009984    21 net.cpp:367] Scale18 -> Convolution18 (in-place)
[2018-04-23 20:41:04]  I0423 12:39:46.010069    21 layer_factory.hpp:77] Creating layer Scale18
[2018-04-23 20:41:04]  I0423 12:39:46.010452    21 net.cpp:122] Setting up Scale18
[2018-04-23 20:41:04]  I0423 12:39:46.010473    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:04]  I0423 12:39:46.010478    21 net.cpp:137] Memory required for data: 258474368
[2018-04-23 20:41:04]  I0423 12:39:46.010485    21 layer_factory.hpp:77] Creating layer ReLU16
[2018-04-23 20:41:04]  I0423 12:39:46.010502    21 net.cpp:84] Creating Layer ReLU16
[2018-04-23 20:41:04]  I0423 12:39:46.010509    21 net.cpp:406] ReLU16 <- Convolution18
[2018-04-23 20:41:04]  I0423 12:39:46.010524    21 net.cpp:367] ReLU16 -> Convolution18 (in-place)
[2018-04-23 20:41:04]  I0423 12:39:46.011129    21 net.cpp:122] Setting up ReLU16
[2018-04-23 20:41:04]  I0423 12:39:46.011154    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:04]  I0423 12:39:46.011159    21 net.cpp:137] Memory required for data: 259522944
[2018-04-23 20:41:04]  I0423 12:39:46.011164    21 layer_factory.hpp:77] Creating layer Convolution19
[2018-04-23 20:41:04]  I0423 12:39:46.011185    21 net.cpp:84] Creating Layer Convolution19
[2018-04-23 20:41:04]  I0423 12:39:46.011191    21 net.cpp:406] Convolution19 <- Convolution18
[2018-04-23 20:41:04]  I0423 12:39:46.011206    21 net.cpp:380] Convolution19 -> Convolution19
[2018-04-23 20:41:04]  I0423 12:39:46.017405    21 net.cpp:122] Setting up Convolution19
[2018-04-23 20:41:04]  I0423 12:39:46.017432    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:04]  I0423 12:39:46.017437    21 net.cpp:137] Memory required for data: 260571520
[2018-04-23 20:41:04]  I0423 12:39:46.017447    21 layer_factory.hpp:77] Creating layer BatchNorm19
[2018-04-23 20:41:04]  I0423 12:39:46.017464    21 net.cpp:84] Creating Layer BatchNorm19
[2018-04-23 20:41:04]  I0423 12:39:46.017470    21 net.cpp:406] BatchNorm19 <- Convolution19
[2018-04-23 20:41:04]  I0423 12:39:46.017477    21 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
[2018-04-23 20:41:04]  I0423 12:39:46.017930    21 net.cpp:122] Setting up BatchNorm19
[2018-04-23 20:41:04]  I0423 12:39:46.017951    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:04]  I0423 12:39:46.017956    21 net.cpp:137] Memory required for data: 261620096
[2018-04-23 20:41:04]  I0423 12:39:46.018002    21 layer_factory.hpp:77] Creating layer Scale19
[2018-04-23 20:41:04]  I0423 12:39:46.018012    21 net.cpp:84] Creating Layer Scale19
[2018-04-23 20:41:04]  I0423 12:39:46.018016    21 net.cpp:406] Scale19 <- Convolution19
[2018-04-23 20:41:04]  I0423 12:39:46.018138    21 net.cpp:367] Scale19 -> Convolution19 (in-place)
[2018-04-23 20:41:04]  I0423 12:39:46.018319    21 layer_factory.hpp:77] Creating layer Scale19
[2018-04-23 20:41:04]  I0423 12:39:46.018630    21 net.cpp:122] Setting up Scale19
[2018-04-23 20:41:05]  I0423 12:39:46.018651    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:05]  I0423 12:39:46.018656    21 net.cpp:137] Memory required for data: 262668672
[2018-04-23 20:41:05]  I0423 12:39:46.018664    21 layer_factory.hpp:77] Creating layer Eltwise8
[2018-04-23 20:41:05]  I0423 12:39:46.018770    21 net.cpp:84] Creating Layer Eltwise8
[2018-04-23 20:41:05]  I0423 12:39:46.018780    21 net.cpp:406] Eltwise8 <- Eltwise7_ReLU15_0_split_1
[2018-04-23 20:41:05]  I0423 12:39:46.018788    21 net.cpp:406] Eltwise8 <- Convolution19
[2018-04-23 20:41:05]  I0423 12:39:46.018795    21 net.cpp:380] Eltwise8 -> Eltwise8
[2018-04-23 20:41:05]  I0423 12:39:46.018894    21 net.cpp:122] Setting up Eltwise8
[2018-04-23 20:41:05]  I0423 12:39:46.018915    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:05]  I0423 12:39:46.018919    21 net.cpp:137] Memory required for data: 263717248
[2018-04-23 20:41:05]  I0423 12:39:46.018923    21 layer_factory.hpp:77] Creating layer ReLU17
[2018-04-23 20:41:05]  I0423 12:39:46.019011    21 net.cpp:84] Creating Layer ReLU17
[2018-04-23 20:41:05]  I0423 12:39:46.019028    21 net.cpp:406] ReLU17 <- Eltwise8
[2018-04-23 20:41:05]  I0423 12:39:46.019047    21 net.cpp:367] ReLU17 -> Eltwise8 (in-place)
[2018-04-23 20:41:05]  I0423 12:39:46.019445    21 net.cpp:122] Setting up ReLU17
[2018-04-23 20:41:05]  I0423 12:39:46.019467    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:05]  I0423 12:39:46.019472    21 net.cpp:137] Memory required for data: 264765824
[2018-04-23 20:41:05]  I0423 12:39:46.019476    21 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
[2018-04-23 20:41:05]  I0423 12:39:46.019582    21 net.cpp:84] Creating Layer Eltwise8_ReLU17_0_split
[2018-04-23 20:41:05]  I0423 12:39:46.019600    21 net.cpp:406] Eltwise8_ReLU17_0_split <- Eltwise8
[2018-04-23 20:41:05]  I0423 12:39:46.019620    21 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
[2018-04-23 20:41:05]  I0423 12:39:46.019631    21 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
[2018-04-23 20:41:05]  I0423 12:39:46.019774    21 net.cpp:122] Setting up Eltwise8_ReLU17_0_split
[2018-04-23 20:41:05]  I0423 12:39:46.019795    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:05]  I0423 12:39:46.019801    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:05]  I0423 12:39:46.019804    21 net.cpp:137] Memory required for data: 266862976
[2018-04-23 20:41:05]  I0423 12:39:46.019809    21 layer_factory.hpp:77] Creating layer Convolution20
[2018-04-23 20:41:05]  I0423 12:39:46.019922    21 net.cpp:84] Creating Layer Convolution20
[2018-04-23 20:41:05]  I0423 12:39:46.019935    21 net.cpp:406] Convolution20 <- Eltwise8_ReLU17_0_split_0
[2018-04-23 20:41:05]  I0423 12:39:46.019973    21 net.cpp:380] Convolution20 -> Convolution20
[2018-04-23 20:41:05]  I0423 12:39:46.023196    21 net.cpp:122] Setting up Convolution20
[2018-04-23 20:41:05]  I0423 12:39:46.023231    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:05]  I0423 12:39:46.023236    21 net.cpp:137] Memory required for data: 267911552
[2018-04-23 20:41:05]  I0423 12:39:46.023247    21 layer_factory.hpp:77] Creating layer BatchNorm20
[2018-04-23 20:41:05]  I0423 12:39:46.023257    21 net.cpp:84] Creating Layer BatchNorm20
[2018-04-23 20:41:05]  I0423 12:39:46.023262    21 net.cpp:406] BatchNorm20 <- Convolution20
[2018-04-23 20:41:05]  I0423 12:39:46.023277    21 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
[2018-04-23 20:41:05]  I0423 12:39:46.023774    21 net.cpp:122] Setting up BatchNorm20
[2018-04-23 20:41:05]  I0423 12:39:46.023797    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:05]  I0423 12:39:46.023802    21 net.cpp:137] Memory required for data: 268960128
[2018-04-23 20:41:05]  I0423 12:39:46.023811    21 layer_factory.hpp:77] Creating layer Scale20
[2018-04-23 20:41:05]  I0423 12:39:46.023826    21 net.cpp:84] Creating Layer Scale20
[2018-04-23 20:41:05]  I0423 12:39:46.023838    21 net.cpp:406] Scale20 <- Convolution20
[2018-04-23 20:41:05]  I0423 12:39:46.023846    21 net.cpp:367] Scale20 -> Convolution20 (in-place)
[2018-04-23 20:41:05]  I0423 12:39:46.023948    21 layer_factory.hpp:77] Creating layer Scale20
[2018-04-23 20:41:05]  I0423 12:39:46.024293    21 net.cpp:122] Setting up Scale20
[2018-04-23 20:41:05]  I0423 12:39:46.024314    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:05]  I0423 12:39:46.024319    21 net.cpp:137] Memory required for data: 270008704
[2018-04-23 20:41:05]  I0423 12:39:46.024327    21 layer_factory.hpp:77] Creating layer ReLU18
[2018-04-23 20:41:05]  I0423 12:39:46.024334    21 net.cpp:84] Creating Layer ReLU18
[2018-04-23 20:41:05]  I0423 12:39:46.024339    21 net.cpp:406] ReLU18 <- Convolution20
[2018-04-23 20:41:05]  I0423 12:39:46.024353    21 net.cpp:367] ReLU18 -> Convolution20 (in-place)
[2018-04-23 20:41:05]  I0423 12:39:46.024751    21 net.cpp:122] Setting up ReLU18
[2018-04-23 20:41:05]  I0423 12:39:46.024796    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:05]  I0423 12:39:46.024806    21 net.cpp:137] Memory required for data: 271057280
[2018-04-23 20:41:05]  I0423 12:39:46.024899    21 layer_factory.hpp:77] Creating layer Convolution21
[2018-04-23 20:41:05]  I0423 12:39:46.024953    21 net.cpp:84] Creating Layer Convolution21
[2018-04-23 20:41:06]  I0423 12:39:46.024961    21 net.cpp:406] Convolution21 <- Convolution20
[2018-04-23 20:41:06]  I0423 12:39:46.024977    21 net.cpp:380] Convolution21 -> Convolution21
[2018-04-23 20:41:06]  I0423 12:39:46.028336    21 net.cpp:122] Setting up Convolution21
[2018-04-23 20:41:06]  I0423 12:39:46.028363    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:06]  I0423 12:39:46.028367    21 net.cpp:137] Memory required for data: 272105856
[2018-04-23 20:41:06]  I0423 12:39:46.028376    21 layer_factory.hpp:77] Creating layer BatchNorm21
[2018-04-23 20:41:06]  I0423 12:39:46.028522    21 net.cpp:84] Creating Layer BatchNorm21
[2018-04-23 20:41:06]  I0423 12:39:46.028537    21 net.cpp:406] BatchNorm21 <- Convolution21
[2018-04-23 20:41:06]  I0423 12:39:46.028626    21 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
[2018-04-23 20:41:06]  I0423 12:39:46.029088    21 net.cpp:122] Setting up BatchNorm21
[2018-04-23 20:41:06]  I0423 12:39:46.029106    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:06]  I0423 12:39:46.029110    21 net.cpp:137] Memory required for data: 273154432
[2018-04-23 20:41:06]  I0423 12:39:46.029120    21 layer_factory.hpp:77] Creating layer Scale21
[2018-04-23 20:41:06]  I0423 12:39:46.029135    21 net.cpp:84] Creating Layer Scale21
[2018-04-23 20:41:06]  I0423 12:39:46.029147    21 net.cpp:406] Scale21 <- Convolution21
[2018-04-23 20:41:06]  I0423 12:39:46.029155    21 net.cpp:367] Scale21 -> Convolution21 (in-place)
[2018-04-23 20:41:06]  I0423 12:39:46.029232    21 layer_factory.hpp:77] Creating layer Scale21
[2018-04-23 20:41:06]  I0423 12:39:46.029484    21 net.cpp:122] Setting up Scale21
[2018-04-23 20:41:06]  I0423 12:39:46.029512    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:06]  I0423 12:39:46.029517    21 net.cpp:137] Memory required for data: 274203008
[2018-04-23 20:41:06]  I0423 12:39:46.029525    21 layer_factory.hpp:77] Creating layer Eltwise9
[2018-04-23 20:41:06]  I0423 12:39:46.029539    21 net.cpp:84] Creating Layer Eltwise9
[2018-04-23 20:41:06]  I0423 12:39:46.029552    21 net.cpp:406] Eltwise9 <- Eltwise8_ReLU17_0_split_1
[2018-04-23 20:41:06]  I0423 12:39:46.029558    21 net.cpp:406] Eltwise9 <- Convolution21
[2018-04-23 20:41:06]  I0423 12:39:46.029566    21 net.cpp:380] Eltwise9 -> Eltwise9
[2018-04-23 20:41:06]  I0423 12:39:46.029618    21 net.cpp:122] Setting up Eltwise9
[2018-04-23 20:41:06]  I0423 12:39:46.029634    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:06]  I0423 12:39:46.029639    21 net.cpp:137] Memory required for data: 275251584
[2018-04-23 20:41:06]  I0423 12:39:46.029641    21 layer_factory.hpp:77] Creating layer ReLU19
[2018-04-23 20:41:06]  I0423 12:39:46.029649    21 net.cpp:84] Creating Layer ReLU19
[2018-04-23 20:41:06]  I0423 12:39:46.029654    21 net.cpp:406] ReLU19 <- Eltwise9
[2018-04-23 20:41:06]  I0423 12:39:46.029665    21 net.cpp:367] ReLU19 -> Eltwise9 (in-place)
[2018-04-23 20:41:06]  I0423 12:39:46.030355    21 net.cpp:122] Setting up ReLU19
[2018-04-23 20:41:06]  I0423 12:39:46.030380    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:06]  I0423 12:39:46.030385    21 net.cpp:137] Memory required for data: 276300160
[2018-04-23 20:41:06]  I0423 12:39:46.030388    21 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
[2018-04-23 20:41:06]  I0423 12:39:46.030396    21 net.cpp:84] Creating Layer Eltwise9_ReLU19_0_split
[2018-04-23 20:41:06]  I0423 12:39:46.030401    21 net.cpp:406] Eltwise9_ReLU19_0_split <- Eltwise9
[2018-04-23 20:41:06]  I0423 12:39:46.030417    21 net.cpp:380] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
[2018-04-23 20:41:06]  I0423 12:39:46.030433    21 net.cpp:380] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
[2018-04-23 20:41:06]  I0423 12:39:46.030628    21 net.cpp:122] Setting up Eltwise9_ReLU19_0_split
[2018-04-23 20:41:06]  I0423 12:39:46.030652    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:06]  I0423 12:39:46.030658    21 net.cpp:129] Top shape: 32 128 8 8 (262144)
[2018-04-23 20:41:06]  I0423 12:39:46.030661    21 net.cpp:137] Memory required for data: 278397312
[2018-04-23 20:41:06]  I0423 12:39:46.030665    21 layer_factory.hpp:77] Creating layer Convolution22
[2018-04-23 20:41:06]  I0423 12:39:46.030684    21 net.cpp:84] Creating Layer Convolution22
[2018-04-23 20:41:06]  I0423 12:39:46.030696    21 net.cpp:406] Convolution22 <- Eltwise9_ReLU19_0_split_0
[2018-04-23 20:41:06]  I0423 12:39:46.030804    21 net.cpp:380] Convolution22 -> Convolution22
[2018-04-23 20:41:06]  I0423 12:39:46.033448    21 net.cpp:122] Setting up Convolution22
[2018-04-23 20:41:06]  I0423 12:39:46.033610    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:06]  I0423 12:39:46.033630    21 net.cpp:137] Memory required for data: 278921600
[2018-04-23 20:41:06]  I0423 12:39:46.033640    21 layer_factory.hpp:77] Creating layer BatchNorm22
[2018-04-23 20:41:06]  I0423 12:39:46.033716    21 net.cpp:84] Creating Layer BatchNorm22
[2018-04-23 20:41:06]  I0423 12:39:46.033735    21 net.cpp:406] BatchNorm22 <- Convolution22
[2018-04-23 20:41:06]  I0423 12:39:46.033860    21 net.cpp:367] BatchNorm22 -> Convolution22 (in-place)
[2018-04-23 20:41:06]  I0423 12:39:46.034447    21 net.cpp:122] Setting up BatchNorm22
[2018-04-23 20:41:06]  I0423 12:39:46.034469    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:06]  I0423 12:39:46.034473    21 net.cpp:137] Memory required for data: 279445888
[2018-04-23 20:41:07]  I0423 12:39:46.034483    21 layer_factory.hpp:77] Creating layer Scale22
[2018-04-23 20:41:07]  I0423 12:39:46.034502    21 net.cpp:84] Creating Layer Scale22
[2018-04-23 20:41:07]  I0423 12:39:46.034513    21 net.cpp:406] Scale22 <- Convolution22
[2018-04-23 20:41:07]  I0423 12:39:46.034521    21 net.cpp:367] Scale22 -> Convolution22 (in-place)
[2018-04-23 20:41:07]  I0423 12:39:46.034651    21 layer_factory.hpp:77] Creating layer Scale22
[2018-04-23 20:41:07]  I0423 12:39:46.035030    21 net.cpp:122] Setting up Scale22
[2018-04-23 20:41:07]  I0423 12:39:46.035053    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:07]  I0423 12:39:46.035058    21 net.cpp:137] Memory required for data: 279970176
[2018-04-23 20:41:07]  I0423 12:39:46.035065    21 layer_factory.hpp:77] Creating layer Convolution23
[2018-04-23 20:41:07]  I0423 12:39:46.035121    21 net.cpp:84] Creating Layer Convolution23
[2018-04-23 20:41:07]  I0423 12:39:46.035128    21 net.cpp:406] Convolution23 <- Eltwise9_ReLU19_0_split_1
[2018-04-23 20:41:07]  I0423 12:39:46.035290    21 net.cpp:380] Convolution23 -> Convolution23
[2018-04-23 20:41:07]  I0423 12:39:46.044536    21 net.cpp:122] Setting up Convolution23
[2018-04-23 20:41:07]  I0423 12:39:46.044565    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:07]  I0423 12:39:46.044570    21 net.cpp:137] Memory required for data: 280494464
[2018-04-23 20:41:07]  I0423 12:39:46.044579    21 layer_factory.hpp:77] Creating layer BatchNorm23
[2018-04-23 20:41:07]  I0423 12:39:46.044706    21 net.cpp:84] Creating Layer BatchNorm23
[2018-04-23 20:41:07]  I0423 12:39:46.044718    21 net.cpp:406] BatchNorm23 <- Convolution23
[2018-04-23 20:41:07]  I0423 12:39:46.044731    21 net.cpp:367] BatchNorm23 -> Convolution23 (in-place)
[2018-04-23 20:41:07]  I0423 12:39:46.045228    21 net.cpp:122] Setting up BatchNorm23
[2018-04-23 20:41:07]  I0423 12:39:46.045250    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:07]  I0423 12:39:46.045254    21 net.cpp:137] Memory required for data: 281018752
[2018-04-23 20:41:07]  I0423 12:39:46.045264    21 layer_factory.hpp:77] Creating layer Scale23
[2018-04-23 20:41:07]  I0423 12:39:46.045272    21 net.cpp:84] Creating Layer Scale23
[2018-04-23 20:41:07]  I0423 12:39:46.045279    21 net.cpp:406] Scale23 <- Convolution23
[2018-04-23 20:41:07]  I0423 12:39:46.045292    21 net.cpp:367] Scale23 -> Convolution23 (in-place)
[2018-04-23 20:41:07]  I0423 12:39:46.045449    21 layer_factory.hpp:77] Creating layer Scale23
[2018-04-23 20:41:07]  I0423 12:39:46.045765    21 net.cpp:122] Setting up Scale23
[2018-04-23 20:41:07]  I0423 12:39:46.045783    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:07]  I0423 12:39:46.045789    21 net.cpp:137] Memory required for data: 281543040
[2018-04-23 20:41:07]  I0423 12:39:46.045795    21 layer_factory.hpp:77] Creating layer ReLU20
[2018-04-23 20:41:07]  I0423 12:39:46.045802    21 net.cpp:84] Creating Layer ReLU20
[2018-04-23 20:41:07]  I0423 12:39:46.045809    21 net.cpp:406] ReLU20 <- Convolution23
[2018-04-23 20:41:07]  I0423 12:39:46.045815    21 net.cpp:367] ReLU20 -> Convolution23 (in-place)
[2018-04-23 20:41:07]  I0423 12:39:46.046162    21 net.cpp:122] Setting up ReLU20
[2018-04-23 20:41:07]  I0423 12:39:46.046185    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:07]  I0423 12:39:46.046190    21 net.cpp:137] Memory required for data: 282067328
[2018-04-23 20:41:07]  I0423 12:39:46.046195    21 layer_factory.hpp:77] Creating layer Convolution24
[2018-04-23 20:41:07]  I0423 12:39:46.046208    21 net.cpp:84] Creating Layer Convolution24
[2018-04-23 20:41:07]  I0423 12:39:46.046213    21 net.cpp:406] Convolution24 <- Convolution23
[2018-04-23 20:41:07]  I0423 12:39:46.046222    21 net.cpp:380] Convolution24 -> Convolution24
[2018-04-23 20:41:07]  I0423 12:39:46.055232    21 net.cpp:122] Setting up Convolution24
[2018-04-23 20:41:07]  I0423 12:39:46.055265    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:07]  I0423 12:39:46.055271    21 net.cpp:137] Memory required for data: 282591616
[2018-04-23 20:41:07]  I0423 12:39:46.055281    21 layer_factory.hpp:77] Creating layer BatchNorm24
[2018-04-23 20:41:07]  I0423 12:39:46.055289    21 net.cpp:84] Creating Layer BatchNorm24
[2018-04-23 20:41:07]  I0423 12:39:46.055294    21 net.cpp:406] BatchNorm24 <- Convolution24
[2018-04-23 20:41:07]  I0423 12:39:46.055307    21 net.cpp:367] BatchNorm24 -> Convolution24 (in-place)
[2018-04-23 20:41:07]  I0423 12:39:46.055711    21 net.cpp:122] Setting up BatchNorm24
[2018-04-23 20:41:07]  I0423 12:39:46.055730    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:07]  I0423 12:39:46.055734    21 net.cpp:137] Memory required for data: 283115904
[2018-04-23 20:41:07]  I0423 12:39:46.055743    21 layer_factory.hpp:77] Creating layer Scale24
[2018-04-23 20:41:07]  I0423 12:39:46.055758    21 net.cpp:84] Creating Layer Scale24
[2018-04-23 20:41:07]  I0423 12:39:46.055764    21 net.cpp:406] Scale24 <- Convolution24
[2018-04-23 20:41:07]  I0423 12:39:46.055770    21 net.cpp:367] Scale24 -> Convolution24 (in-place)
[2018-04-23 20:41:07]  I0423 12:39:46.055850    21 layer_factory.hpp:77] Creating layer Scale24
[2018-04-23 20:41:07]  I0423 12:39:46.056078    21 net.cpp:122] Setting up Scale24
[2018-04-23 20:41:08]  I0423 12:39:46.056097    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:08]  I0423 12:39:46.056100    21 net.cpp:137] Memory required for data: 283640192
[2018-04-23 20:41:08]  I0423 12:39:46.056107    21 layer_factory.hpp:77] Creating layer Eltwise10
[2018-04-23 20:41:08]  I0423 12:39:46.056115    21 net.cpp:84] Creating Layer Eltwise10
[2018-04-23 20:41:08]  I0423 12:39:46.056119    21 net.cpp:406] Eltwise10 <- Convolution22
[2018-04-23 20:41:08]  I0423 12:39:46.056124    21 net.cpp:406] Eltwise10 <- Convolution24
[2018-04-23 20:41:08]  I0423 12:39:46.056138    21 net.cpp:380] Eltwise10 -> Eltwise10
[2018-04-23 20:41:08]  I0423 12:39:46.056236    21 net.cpp:122] Setting up Eltwise10
[2018-04-23 20:41:08]  I0423 12:39:46.056257    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:08]  I0423 12:39:46.056262    21 net.cpp:137] Memory required for data: 284164480
[2018-04-23 20:41:08]  I0423 12:39:46.056265    21 layer_factory.hpp:77] Creating layer ReLU21
[2018-04-23 20:41:08]  I0423 12:39:46.056367    21 net.cpp:84] Creating Layer ReLU21
[2018-04-23 20:41:08]  I0423 12:39:46.056386    21 net.cpp:406] ReLU21 <- Eltwise10
[2018-04-23 20:41:08]  I0423 12:39:46.056406    21 net.cpp:367] ReLU21 -> Eltwise10 (in-place)
[2018-04-23 20:41:08]  I0423 12:39:46.056828    21 net.cpp:122] Setting up ReLU21
[2018-04-23 20:41:08]  I0423 12:39:46.056851    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:08]  I0423 12:39:46.056855    21 net.cpp:137] Memory required for data: 284688768
[2018-04-23 20:41:08]  I0423 12:39:46.056860    21 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
[2018-04-23 20:41:08]  I0423 12:39:46.056958    21 net.cpp:84] Creating Layer Eltwise10_ReLU21_0_split
[2018-04-23 20:41:08]  I0423 12:39:46.056977    21 net.cpp:406] Eltwise10_ReLU21_0_split <- Eltwise10
[2018-04-23 20:41:08]  I0423 12:39:46.056994    21 net.cpp:380] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
[2018-04-23 20:41:08]  I0423 12:39:46.057006    21 net.cpp:380] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
[2018-04-23 20:41:08]  I0423 12:39:46.057144    21 net.cpp:122] Setting up Eltwise10_ReLU21_0_split
[2018-04-23 20:41:08]  I0423 12:39:46.057166    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:08]  I0423 12:39:46.057171    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:08]  I0423 12:39:46.057175    21 net.cpp:137] Memory required for data: 285737344
[2018-04-23 20:41:08]  I0423 12:39:46.057179    21 layer_factory.hpp:77] Creating layer Convolution25
[2018-04-23 20:41:08]  I0423 12:39:46.057299    21 net.cpp:84] Creating Layer Convolution25
[2018-04-23 20:41:08]  I0423 12:39:46.057312    21 net.cpp:406] Convolution25 <- Eltwise10_ReLU21_0_split_0
[2018-04-23 20:41:08]  I0423 12:39:46.057416    21 net.cpp:380] Convolution25 -> Convolution25
[2018-04-23 20:41:08]  I0423 12:39:46.069185    21 net.cpp:122] Setting up Convolution25
[2018-04-23 20:41:08]  I0423 12:39:46.069219    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:08]  I0423 12:39:46.069224    21 net.cpp:137] Memory required for data: 286261632
[2018-04-23 20:41:08]  I0423 12:39:46.069236    21 layer_factory.hpp:77] Creating layer BatchNorm25
[2018-04-23 20:41:08]  I0423 12:39:46.069245    21 net.cpp:84] Creating Layer BatchNorm25
[2018-04-23 20:41:08]  I0423 12:39:46.069250    21 net.cpp:406] BatchNorm25 <- Convolution25
[2018-04-23 20:41:08]  I0423 12:39:46.069260    21 net.cpp:367] BatchNorm25 -> Convolution25 (in-place)
[2018-04-23 20:41:08]  I0423 12:39:46.069882    21 net.cpp:122] Setting up BatchNorm25
[2018-04-23 20:41:08]  I0423 12:39:46.069911    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:08]  I0423 12:39:46.069917    21 net.cpp:137] Memory required for data: 286785920
[2018-04-23 20:41:08]  I0423 12:39:46.069929    21 layer_factory.hpp:77] Creating layer Scale25
[2018-04-23 20:41:08]  I0423 12:39:46.069939    21 net.cpp:84] Creating Layer Scale25
[2018-04-23 20:41:08]  I0423 12:39:46.069947    21 net.cpp:406] Scale25 <- Convolution25
[2018-04-23 20:41:08]  I0423 12:39:46.069954    21 net.cpp:367] Scale25 -> Convolution25 (in-place)
[2018-04-23 20:41:08]  I0423 12:39:46.070052    21 layer_factory.hpp:77] Creating layer Scale25
[2018-04-23 20:41:08]  I0423 12:39:46.070272    21 net.cpp:122] Setting up Scale25
[2018-04-23 20:41:08]  I0423 12:39:46.070291    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:08]  I0423 12:39:46.070296    21 net.cpp:137] Memory required for data: 287310208
[2018-04-23 20:41:08]  I0423 12:39:46.070302    21 layer_factory.hpp:77] Creating layer ReLU22
[2018-04-23 20:41:08]  I0423 12:39:46.070317    21 net.cpp:84] Creating Layer ReLU22
[2018-04-23 20:41:08]  I0423 12:39:46.070324    21 net.cpp:406] ReLU22 <- Convolution25
[2018-04-23 20:41:08]  I0423 12:39:46.070330    21 net.cpp:367] ReLU22 -> Convolution25 (in-place)
[2018-04-23 20:41:08]  I0423 12:39:46.070750    21 net.cpp:122] Setting up ReLU22
[2018-04-23 20:41:08]  I0423 12:39:46.070771    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:08]  I0423 12:39:46.070775    21 net.cpp:137] Memory required for data: 287834496
[2018-04-23 20:41:08]  I0423 12:39:46.070780    21 layer_factory.hpp:77] Creating layer Convolution26
[2018-04-23 20:41:08]  I0423 12:39:46.070796    21 net.cpp:84] Creating Layer Convolution26
[2018-04-23 20:41:08]  I0423 12:39:46.070807    21 net.cpp:406] Convolution26 <- Convolution25
[2018-04-23 20:41:08]  I0423 12:39:46.070819    21 net.cpp:380] Convolution26 -> Convolution26
[2018-04-23 20:41:08]  I0423 12:39:46.080339    21 net.cpp:122] Setting up Convolution26
[2018-04-23 20:41:08]  I0423 12:39:46.080368    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:08]  I0423 12:39:46.080373    21 net.cpp:137] Memory required for data: 288358784
[2018-04-23 20:41:08]  I0423 12:39:46.080380    21 layer_factory.hpp:77] Creating layer BatchNorm26
[2018-04-23 20:41:09]  I0423 12:39:46.080397    21 net.cpp:84] Creating Layer BatchNorm26
[2018-04-23 20:41:09]  I0423 12:39:46.080404    21 net.cpp:406] BatchNorm26 <- Convolution26
[2018-04-23 20:41:09]  I0423 12:39:46.080416    21 net.cpp:367] BatchNorm26 -> Convolution26 (in-place)
[2018-04-23 20:41:09]  I0423 12:39:46.080790    21 net.cpp:122] Setting up BatchNorm26
[2018-04-23 20:41:09]  I0423 12:39:46.080808    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:09]  I0423 12:39:46.080813    21 net.cpp:137] Memory required for data: 288883072
[2018-04-23 20:41:09]  I0423 12:39:46.080821    21 layer_factory.hpp:77] Creating layer Scale26
[2018-04-23 20:41:09]  I0423 12:39:46.080830    21 net.cpp:84] Creating Layer Scale26
[2018-04-23 20:41:09]  I0423 12:39:46.080835    21 net.cpp:406] Scale26 <- Convolution26
[2018-04-23 20:41:09]  I0423 12:39:46.080848    21 net.cpp:367] Scale26 -> Convolution26 (in-place)
[2018-04-23 20:41:09]  I0423 12:39:46.080925    21 layer_factory.hpp:77] Creating layer Scale26
[2018-04-23 20:41:09]  I0423 12:39:46.081157    21 net.cpp:122] Setting up Scale26
[2018-04-23 20:41:09]  I0423 12:39:46.081174    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:09]  I0423 12:39:46.081178    21 net.cpp:137] Memory required for data: 289407360
[2018-04-23 20:41:09]  I0423 12:39:46.081185    21 layer_factory.hpp:77] Creating layer Eltwise11
[2018-04-23 20:41:09]  I0423 12:39:46.081193    21 net.cpp:84] Creating Layer Eltwise11
[2018-04-23 20:41:09]  I0423 12:39:46.081198    21 net.cpp:406] Eltwise11 <- Eltwise10_ReLU21_0_split_1
[2018-04-23 20:41:09]  I0423 12:39:46.081204    21 net.cpp:406] Eltwise11 <- Convolution26
[2018-04-23 20:41:09]  I0423 12:39:46.081212    21 net.cpp:380] Eltwise11 -> Eltwise11
[2018-04-23 20:41:09]  I0423 12:39:46.081267    21 net.cpp:122] Setting up Eltwise11
[2018-04-23 20:41:09]  I0423 12:39:46.081288    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:09]  I0423 12:39:46.081293    21 net.cpp:137] Memory required for data: 289931648
[2018-04-23 20:41:09]  I0423 12:39:46.081297    21 layer_factory.hpp:77] Creating layer ReLU23
[2018-04-23 20:41:09]  I0423 12:39:46.081305    21 net.cpp:84] Creating Layer ReLU23
[2018-04-23 20:41:09]  I0423 12:39:46.081310    21 net.cpp:406] ReLU23 <- Eltwise11
[2018-04-23 20:41:09]  I0423 12:39:46.081315    21 net.cpp:367] ReLU23 -> Eltwise11 (in-place)
[2018-04-23 20:41:09]  I0423 12:39:46.081897    21 net.cpp:122] Setting up ReLU23
[2018-04-23 20:41:09]  I0423 12:39:46.081923    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:09]  I0423 12:39:46.081928    21 net.cpp:137] Memory required for data: 290455936
[2018-04-23 20:41:09]  I0423 12:39:46.081931    21 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
[2018-04-23 20:41:09]  I0423 12:39:46.081946    21 net.cpp:84] Creating Layer Eltwise11_ReLU23_0_split
[2018-04-23 20:41:09]  I0423 12:39:46.081954    21 net.cpp:406] Eltwise11_ReLU23_0_split <- Eltwise11
[2018-04-23 20:41:09]  I0423 12:39:46.081962    21 net.cpp:380] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
[2018-04-23 20:41:09]  I0423 12:39:46.081972    21 net.cpp:380] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
[2018-04-23 20:41:09]  I0423 12:39:46.082089    21 net.cpp:122] Setting up Eltwise11_ReLU23_0_split
[2018-04-23 20:41:09]  I0423 12:39:46.082108    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:09]  I0423 12:39:46.082116    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:09]  I0423 12:39:46.082120    21 net.cpp:137] Memory required for data: 291504512
[2018-04-23 20:41:09]  I0423 12:39:46.082125    21 layer_factory.hpp:77] Creating layer Convolution27
[2018-04-23 20:41:09]  I0423 12:39:46.082137    21 net.cpp:84] Creating Layer Convolution27
[2018-04-23 20:41:09]  I0423 12:39:46.082149    21 net.cpp:406] Convolution27 <- Eltwise11_ReLU23_0_split_0
[2018-04-23 20:41:09]  I0423 12:39:46.082160    21 net.cpp:380] Convolution27 -> Convolution27
[2018-04-23 20:41:09]  I0423 12:39:46.091286    21 net.cpp:122] Setting up Convolution27
[2018-04-23 20:41:09]  I0423 12:39:46.091315    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:09]  I0423 12:39:46.091320    21 net.cpp:137] Memory required for data: 292028800
[2018-04-23 20:41:09]  I0423 12:39:46.091328    21 layer_factory.hpp:77] Creating layer BatchNorm27
[2018-04-23 20:41:09]  I0423 12:39:46.091337    21 net.cpp:84] Creating Layer BatchNorm27
[2018-04-23 20:41:09]  I0423 12:39:46.091342    21 net.cpp:406] BatchNorm27 <- Convolution27
[2018-04-23 20:41:09]  I0423 12:39:46.091353    21 net.cpp:367] BatchNorm27 -> Convolution27 (in-place)
[2018-04-23 20:41:09]  I0423 12:39:46.091783    21 net.cpp:122] Setting up BatchNorm27
[2018-04-23 20:41:09]  I0423 12:39:46.091801    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:09]  I0423 12:39:46.091805    21 net.cpp:137] Memory required for data: 292553088
[2018-04-23 20:41:09]  I0423 12:39:46.091827    21 layer_factory.hpp:77] Creating layer Scale27
[2018-04-23 20:41:09]  I0423 12:39:46.091871    21 net.cpp:84] Creating Layer Scale27
[2018-04-23 20:41:09]  I0423 12:39:46.091886    21 net.cpp:406] Scale27 <- Convolution27
[2018-04-23 20:41:09]  I0423 12:39:46.091892    21 net.cpp:367] Scale27 -> Convolution27 (in-place)
[2018-04-23 20:41:09]  I0423 12:39:46.091966    21 layer_factory.hpp:77] Creating layer Scale27
[2018-04-23 20:41:09]  I0423 12:39:46.092180    21 net.cpp:122] Setting up Scale27
[2018-04-23 20:41:10]  I0423 12:39:46.092196    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:10]  I0423 12:39:46.092201    21 net.cpp:137] Memory required for data: 293077376
[2018-04-23 20:41:10]  I0423 12:39:46.092209    21 layer_factory.hpp:77] Creating layer ReLU24
[2018-04-23 20:41:10]  I0423 12:39:46.092229    21 net.cpp:84] Creating Layer ReLU24
[2018-04-23 20:41:10]  I0423 12:39:46.092241    21 net.cpp:406] ReLU24 <- Convolution27
[2018-04-23 20:41:10]  I0423 12:39:46.092247    21 net.cpp:367] ReLU24 -> Convolution27 (in-place)
[2018-04-23 20:41:10]  I0423 12:39:46.092615    21 net.cpp:122] Setting up ReLU24
[2018-04-23 20:41:10]  I0423 12:39:46.092634    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:10]  I0423 12:39:46.092638    21 net.cpp:137] Memory required for data: 293601664
[2018-04-23 20:41:10]  I0423 12:39:46.092643    21 layer_factory.hpp:77] Creating layer Convolution28
[2018-04-23 20:41:10]  I0423 12:39:46.092664    21 net.cpp:84] Creating Layer Convolution28
[2018-04-23 20:41:10]  I0423 12:39:46.092674    21 net.cpp:406] Convolution28 <- Convolution27
[2018-04-23 20:41:10]  I0423 12:39:46.092684    21 net.cpp:380] Convolution28 -> Convolution28
[2018-04-23 20:41:10]  I0423 12:39:46.101929    21 net.cpp:122] Setting up Convolution28
[2018-04-23 20:41:10]  I0423 12:39:46.101958    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:10]  I0423 12:39:46.101963    21 net.cpp:137] Memory required for data: 294125952
[2018-04-23 20:41:10]  I0423 12:39:46.101974    21 layer_factory.hpp:77] Creating layer BatchNorm28
[2018-04-23 20:41:10]  I0423 12:39:46.101990    21 net.cpp:84] Creating Layer BatchNorm28
[2018-04-23 20:41:10]  I0423 12:39:46.101999    21 net.cpp:406] BatchNorm28 <- Convolution28
[2018-04-23 20:41:10]  I0423 12:39:46.102010    21 net.cpp:367] BatchNorm28 -> Convolution28 (in-place)
[2018-04-23 20:41:10]  I0423 12:39:46.102392    21 net.cpp:122] Setting up BatchNorm28
[2018-04-23 20:41:10]  I0423 12:39:46.102411    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:10]  I0423 12:39:46.102414    21 net.cpp:137] Memory required for data: 294650240
[2018-04-23 20:41:10]  I0423 12:39:46.102423    21 layer_factory.hpp:77] Creating layer Scale28
[2018-04-23 20:41:10]  I0423 12:39:46.102432    21 net.cpp:84] Creating Layer Scale28
[2018-04-23 20:41:10]  I0423 12:39:46.102437    21 net.cpp:406] Scale28 <- Convolution28
[2018-04-23 20:41:10]  I0423 12:39:46.102442    21 net.cpp:367] Scale28 -> Convolution28 (in-place)
[2018-04-23 20:41:10]  I0423 12:39:46.102533    21 layer_factory.hpp:77] Creating layer Scale28
[2018-04-23 20:41:10]  I0423 12:39:46.102767    21 net.cpp:122] Setting up Scale28
[2018-04-23 20:41:10]  I0423 12:39:46.102783    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:10]  I0423 12:39:46.102788    21 net.cpp:137] Memory required for data: 295174528
[2018-04-23 20:41:10]  I0423 12:39:46.102795    21 layer_factory.hpp:77] Creating layer Eltwise12
[2018-04-23 20:41:10]  I0423 12:39:46.102802    21 net.cpp:84] Creating Layer Eltwise12
[2018-04-23 20:41:10]  I0423 12:39:46.102807    21 net.cpp:406] Eltwise12 <- Eltwise11_ReLU23_0_split_1
[2018-04-23 20:41:10]  I0423 12:39:46.102814    21 net.cpp:406] Eltwise12 <- Convolution28
[2018-04-23 20:41:10]  I0423 12:39:46.102830    21 net.cpp:380] Eltwise12 -> Eltwise12
[2018-04-23 20:41:10]  I0423 12:39:46.102880    21 net.cpp:122] Setting up Eltwise12
[2018-04-23 20:41:10]  I0423 12:39:46.102890    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:10]  I0423 12:39:46.102893    21 net.cpp:137] Memory required for data: 295698816
[2018-04-23 20:41:10]  I0423 12:39:46.102896    21 layer_factory.hpp:77] Creating layer ReLU25
[2018-04-23 20:41:10]  I0423 12:39:46.102910    21 net.cpp:84] Creating Layer ReLU25
[2018-04-23 20:41:10]  I0423 12:39:46.102916    21 net.cpp:406] ReLU25 <- Eltwise12
[2018-04-23 20:41:10]  I0423 12:39:46.102922    21 net.cpp:367] ReLU25 -> Eltwise12 (in-place)
[2018-04-23 20:41:10]  I0423 12:39:46.103252    21 net.cpp:122] Setting up ReLU25
[2018-04-23 20:41:10]  I0423 12:39:46.103272    21 net.cpp:129] Top shape: 32 256 4 4 (131072)
[2018-04-23 20:41:10]  I0423 12:39:46.103276    21 net.cpp:137] Memory required for data: 296223104
[2018-04-23 20:41:10]  I0423 12:39:46.103281    21 layer_factory.hpp:77] Creating layer Pooling1
[2018-04-23 20:41:10]  I0423 12:39:46.103292    21 net.cpp:84] Creating Layer Pooling1
[2018-04-23 20:41:10]  I0423 12:39:46.103297    21 net.cpp:406] Pooling1 <- Eltwise12
[2018-04-23 20:41:10]  I0423 12:39:46.103304    21 net.cpp:380] Pooling1 -> Pooling1
[2018-04-23 20:41:10]  I0423 12:39:46.103899    21 net.cpp:122] Setting up Pooling1
[2018-04-23 20:41:10]  I0423 12:39:46.103924    21 net.cpp:129] Top shape: 32 256 1 1 (8192)
[2018-04-23 20:41:10]  I0423 12:39:46.103929    21 net.cpp:137] Memory required for data: 296255872
[2018-04-23 20:41:10]  I0423 12:39:46.103934    21 layer_factory.hpp:77] Creating layer InnerProduct1
[2018-04-23 20:41:10]  I0423 12:39:46.103952    21 net.cpp:84] Creating Layer InnerProduct1
[2018-04-23 20:41:10]  I0423 12:39:46.103958    21 net.cpp:406] InnerProduct1 <- Pooling1
[2018-04-23 20:41:10]  I0423 12:39:46.103973    21 net.cpp:380] InnerProduct1 -> InnerProduct1
[2018-04-23 20:41:10]  I0423 12:39:46.104394    21 net.cpp:122] Setting up InnerProduct1
[2018-04-23 20:41:10]  I0423 12:39:46.104419    21 net.cpp:129] Top shape: 32 100 (3200)
[2018-04-23 20:41:10]  I0423 12:39:46.104424    21 net.cpp:137] Memory required for data: 296268672
[2018-04-23 20:41:10]  I0423 12:39:46.104432    21 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
[2018-04-23 20:41:11]  I0423 12:39:46.104440    21 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
[2018-04-23 20:41:11]  I0423 12:39:46.104445    21 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
[2018-04-23 20:41:11]  I0423 12:39:46.104452    21 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
[2018-04-23 20:41:11]  I0423 12:39:46.104460    21 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
[2018-04-23 20:41:11]  I0423 12:39:46.104557    21 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
[2018-04-23 20:41:11]  I0423 12:39:46.104568    21 net.cpp:129] Top shape: 32 100 (3200)
[2018-04-23 20:41:11]  I0423 12:39:46.104571    21 net.cpp:129] Top shape: 32 100 (3200)
[2018-04-23 20:41:11]  I0423 12:39:46.104574    21 net.cpp:137] Memory required for data: 296294272
[2018-04-23 20:41:11]  I0423 12:39:46.104578    21 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
[2018-04-23 20:41:11]  I0423 12:39:46.104593    21 net.cpp:84] Creating Layer SoftmaxWithLoss1
[2018-04-23 20:41:11]  I0423 12:39:46.104604    21 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
[2018-04-23 20:41:11]  I0423 12:39:46.104610    21 net.cpp:406] SoftmaxWithLoss1 <- label_data_1_split_0
[2018-04-23 20:41:11]  I0423 12:39:46.104617    21 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
[2018-04-23 20:41:11]  I0423 12:39:46.104629    21 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
[2018-04-23 20:41:11]  I0423 12:39:46.105180    21 net.cpp:122] Setting up SoftmaxWithLoss1
[2018-04-23 20:41:11]  I0423 12:39:46.105198    21 net.cpp:129] Top shape: (1)
[2018-04-23 20:41:11]  I0423 12:39:46.105202    21 net.cpp:132]     with loss weight 1
[2018-04-23 20:41:11]  I0423 12:39:46.105214    21 net.cpp:137] Memory required for data: 296294276
[2018-04-23 20:41:11]  I0423 12:39:46.105219    21 layer_factory.hpp:77] Creating layer Accuracy_test
[2018-04-23 20:41:11]  I0423 12:39:46.105235    21 net.cpp:84] Creating Layer Accuracy_test
[2018-04-23 20:41:11]  I0423 12:39:46.105242    21 net.cpp:406] Accuracy_test <- InnerProduct1_InnerProduct1_0_split_1
[2018-04-23 20:41:11]  I0423 12:39:46.105248    21 net.cpp:406] Accuracy_test <- label_data_1_split_1
[2018-04-23 20:41:11]  I0423 12:39:46.105255    21 net.cpp:380] Accuracy_test -> Accuracy_test
[2018-04-23 20:41:11]  I0423 12:39:46.105269    21 net.cpp:122] Setting up Accuracy_test
[2018-04-23 20:41:11]  I0423 12:39:46.105275    21 net.cpp:129] Top shape: (1)
[2018-04-23 20:41:11]  I0423 12:39:46.105279    21 net.cpp:137] Memory required for data: 296294280
[2018-04-23 20:41:11]  I0423 12:39:46.105283    21 net.cpp:200] Accuracy_test does not need backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105288    21 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105293    21 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105298    21 net.cpp:198] InnerProduct1 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105303    21 net.cpp:198] Pooling1 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105306    21 net.cpp:198] ReLU25 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105310    21 net.cpp:198] Eltwise12 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105315    21 net.cpp:198] Scale28 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105319    21 net.cpp:198] BatchNorm28 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105322    21 net.cpp:198] Convolution28 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105327    21 net.cpp:198] ReLU24 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105331    21 net.cpp:198] Scale27 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105335    21 net.cpp:198] BatchNorm27 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105374    21 net.cpp:198] Convolution27 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105381    21 net.cpp:198] Eltwise11_ReLU23_0_split needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105396    21 net.cpp:198] ReLU23 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105399    21 net.cpp:198] Eltwise11 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105404    21 net.cpp:198] Scale26 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105408    21 net.cpp:198] BatchNorm26 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105412    21 net.cpp:198] Convolution26 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105417    21 net.cpp:198] ReLU22 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105422    21 net.cpp:198] Scale25 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105425    21 net.cpp:198] BatchNorm25 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105430    21 net.cpp:198] Convolution25 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105434    21 net.cpp:198] Eltwise10_ReLU21_0_split needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105439    21 net.cpp:198] ReLU21 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105443    21 net.cpp:198] Eltwise10 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105448    21 net.cpp:198] Scale24 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105453    21 net.cpp:198] BatchNorm24 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105456    21 net.cpp:198] Convolution24 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105460    21 net.cpp:198] ReLU20 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105464    21 net.cpp:198] Scale23 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105468    21 net.cpp:198] BatchNorm23 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105473    21 net.cpp:198] Convolution23 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105478    21 net.cpp:198] Scale22 needs backward computation.
[2018-04-23 20:41:11]  I0423 12:39:46.105481    21 net.cpp:198] BatchNorm22 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105485    21 net.cpp:198] Convolution22 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105489    21 net.cpp:198] Eltwise9_ReLU19_0_split needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105506    21 net.cpp:198] ReLU19 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105510    21 net.cpp:198] Eltwise9 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105515    21 net.cpp:198] Scale21 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105520    21 net.cpp:198] BatchNorm21 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105523    21 net.cpp:198] Convolution21 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105530    21 net.cpp:198] ReLU18 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105533    21 net.cpp:198] Scale20 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105536    21 net.cpp:198] BatchNorm20 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105540    21 net.cpp:198] Convolution20 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105545    21 net.cpp:198] Eltwise8_ReLU17_0_split needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105549    21 net.cpp:198] ReLU17 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105553    21 net.cpp:198] Eltwise8 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105558    21 net.cpp:198] Scale19 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105563    21 net.cpp:198] BatchNorm19 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105567    21 net.cpp:198] Convolution19 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105571    21 net.cpp:198] ReLU16 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105574    21 net.cpp:198] Scale18 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105578    21 net.cpp:198] BatchNorm18 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105582    21 net.cpp:198] Convolution18 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105587    21 net.cpp:198] Eltwise7_ReLU15_0_split needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105592    21 net.cpp:198] ReLU15 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105597    21 net.cpp:198] Eltwise7 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105600    21 net.cpp:198] Scale17 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105604    21 net.cpp:198] BatchNorm17 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105608    21 net.cpp:198] Convolution17 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105621    21 net.cpp:198] ReLU14 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105626    21 net.cpp:198] Scale16 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105630    21 net.cpp:198] BatchNorm16 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105634    21 net.cpp:198] Convolution16 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105639    21 net.cpp:198] Scale15 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105643    21 net.cpp:198] BatchNorm15 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105648    21 net.cpp:198] Convolution15 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105651    21 net.cpp:198] Eltwise6_ReLU13_0_split needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105657    21 net.cpp:198] ReLU13 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105661    21 net.cpp:198] Eltwise6 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105666    21 net.cpp:198] Scale14 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105670    21 net.cpp:198] BatchNorm14 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105675    21 net.cpp:198] Convolution14 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105679    21 net.cpp:198] ReLU12 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105684    21 net.cpp:198] Scale13 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105687    21 net.cpp:198] BatchNorm13 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105692    21 net.cpp:198] Convolution13 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105697    21 net.cpp:198] Eltwise5_ReLU11_0_split needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105701    21 net.cpp:198] ReLU11 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105705    21 net.cpp:198] Eltwise5 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105711    21 net.cpp:198] Scale12 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105715    21 net.cpp:198] BatchNorm12 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105720    21 net.cpp:198] Convolution12 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105726    21 net.cpp:198] ReLU10 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105729    21 net.cpp:198] Scale11 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105733    21 net.cpp:198] BatchNorm11 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105737    21 net.cpp:198] Convolution11 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105742    21 net.cpp:198] Eltwise4_ReLU9_0_split needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105747    21 net.cpp:198] ReLU9 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105751    21 net.cpp:198] Eltwise4 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105757    21 net.cpp:198] Scale10 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105762    21 net.cpp:198] BatchNorm10 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105765    21 net.cpp:198] Convolution10 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105770    21 net.cpp:198] ReLU8 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105774    21 net.cpp:198] Scale9 needs backward computation.
[2018-04-23 20:41:12]  I0423 12:39:46.105782    21 net.cpp:198] BatchNorm9 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105794    21 net.cpp:198] Convolution9 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105803    21 net.cpp:198] Scale8 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105829    21 net.cpp:198] BatchNorm8 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105834    21 net.cpp:198] Convolution8 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105839    21 net.cpp:198] Eltwise3_ReLU7_0_split needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105842    21 net.cpp:198] ReLU7 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105846    21 net.cpp:198] Eltwise3 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105851    21 net.cpp:198] Scale7 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105864    21 net.cpp:198] BatchNorm7 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105873    21 net.cpp:198] Convolution7 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105883    21 net.cpp:198] ReLU6 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105887    21 net.cpp:198] Scale6 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105891    21 net.cpp:198] BatchNorm6 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105895    21 net.cpp:198] Convolution6 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105900    21 net.cpp:198] Eltwise2_ReLU5_0_split needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105903    21 net.cpp:198] ReLU5 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105907    21 net.cpp:198] Eltwise2 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105912    21 net.cpp:198] Scale5 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105918    21 net.cpp:198] BatchNorm5 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105922    21 net.cpp:198] Convolution5 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105926    21 net.cpp:198] ReLU4 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105931    21 net.cpp:198] Scale4 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105934    21 net.cpp:198] BatchNorm4 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105938    21 net.cpp:198] Convolution4 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105942    21 net.cpp:198] Eltwise1_ReLU3_0_split needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105947    21 net.cpp:198] ReLU3 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105952    21 net.cpp:198] Eltwise1 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105960    21 net.cpp:198] Scale3 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105965    21 net.cpp:198] BatchNorm3 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105969    21 net.cpp:198] Convolution3 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105973    21 net.cpp:198] ReLU2 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105978    21 net.cpp:198] Scale2 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105983    21 net.cpp:198] BatchNorm2 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105986    21 net.cpp:198] Convolution2 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105990    21 net.cpp:198] Convolution1_ReLU1_0_split needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105995    21 net.cpp:198] ReLU1 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.105999    21 net.cpp:198] Scale1 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.106003    21 net.cpp:198] BatchNorm1 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.106006    21 net.cpp:198] Convolution1 needs backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.106012    21 net.cpp:200] label_data_1_split does not need backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.106019    21 net.cpp:200] data does not need backward computation.
[2018-04-23 20:41:13]  I0423 12:39:46.106021    21 net.cpp:242] This network produces output Accuracy_test
[2018-04-23 20:41:13]  I0423 12:39:46.106026    21 net.cpp:242] This network produces output SoftmaxWithLoss1
[2018-04-23 20:41:13]  I0423 12:39:46.106134    21 net.cpp:255] Network initialization done.
[2018-04-23 20:41:13]  I0423 12:39:46.106555    21 solver.cpp:56] Solver scaffolding done.
[2018-04-23 20:41:13]  I0423 12:39:46.139297    21 solver.cpp:330] Iteration 0, Testing net (#0)
[2018-04-23 20:41:13]  I0423 12:39:46.146752    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:41:13]  I0423 12:39:46.229785    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:41:13]  I0423 12:39:50.239670    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0108173
[2018-04-23 20:41:13]  I0423 12:39:50.239820    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.60517 (* 1 = 4.60517 loss)
[2018-04-23 20:41:13]  I0423 12:39:50.354424    21 solver.cpp:218] Iteration 0 (0 iter/s, 4.21534s/100 iters), loss = 5.14598
[2018-04-23 20:41:13]  I0423 12:39:50.354480    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.015625
[2018-04-23 20:41:13]  I0423 12:39:50.354506    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 5.14598 (* 1 = 5.14598 loss)
[2018-04-23 20:41:13]  I0423 12:39:50.354538    21 sgd_solver.cpp:105] Iteration 0, lr = 0.1
[2018-04-23 20:41:13]  I0423 12:39:57.110541    21 solver.cpp:330] Iteration 100, Testing net (#0)
[2018-04-23 20:41:13]  I0423 12:39:57.113445    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:41:13]  I0423 12:40:01.134232    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0138221
[2018-04-23 20:41:13]  I0423 12:40:01.135171    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 15.9059 (* 1 = 15.9059 loss)
[2018-04-23 20:41:14]  I0423 12:40:01.203716    21 solver.cpp:218] Iteration 100 (9.21762 iter/s, 10.8488s/100 iters), loss = 4.52819
[2018-04-23 20:41:14]  I0423 12:40:01.203893    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.015625
[2018-04-23 20:41:14]  I0423 12:40:01.203969    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 4.52819 (* 1 = 4.52819 loss)
[2018-04-23 20:41:14]  I0423 12:40:01.204031    21 sgd_solver.cpp:105] Iteration 100, lr = 0.1
[2018-04-23 20:41:14]  I0423 12:40:07.944806    21 solver.cpp:330] Iteration 200, Testing net (#0)
[2018-04-23 20:41:14]  I0423 12:40:07.946607    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:41:14]  I0423 12:40:12.176674    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0120192
[2018-04-23 20:41:14]  I0423 12:40:12.177059    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 5.37356 (* 1 = 5.37356 loss)
[2018-04-23 20:41:14]  I0423 12:40:12.246258    21 solver.cpp:218] Iteration 200 (9.05638 iter/s, 11.0419s/100 iters), loss = 4.28154
[2018-04-23 20:41:14]  I0423 12:40:12.246299    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.015625
[2018-04-23 20:41:14]  I0423 12:40:12.246311    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 4.28154 (* 1 = 4.28154 loss)
[2018-04-23 20:41:14]  I0423 12:40:12.246332    21 sgd_solver.cpp:105] Iteration 200, lr = 0.1
[2018-04-23 20:41:14]  I0423 12:40:18.999497    21 solver.cpp:330] Iteration 300, Testing net (#0)
[2018-04-23 20:41:14]  I0423 12:40:19.002738    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:41:14]  I0423 12:40:23.082005    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0144231
[2018-04-23 20:41:14]  I0423 12:40:23.082289    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 5.41734 (* 1 = 5.41734 loss)
[2018-04-23 20:41:14]  I0423 12:40:23.148145    21 solver.cpp:218] Iteration 300 (9.17317 iter/s, 10.9014s/100 iters), loss = 3.75218
[2018-04-23 20:41:14]  I0423 12:40:23.148237    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.109375
[2018-04-23 20:41:14]  I0423 12:40:23.148291    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 3.75218 (* 1 = 3.75218 loss)
[2018-04-23 20:41:14]  I0423 12:40:23.148381    21 sgd_solver.cpp:105] Iteration 300, lr = 0.1
[2018-04-23 20:41:14]  I0423 12:40:29.852988    21 solver.cpp:330] Iteration 400, Testing net (#0)
[2018-04-23 20:41:14]  I0423 12:40:29.854315    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:41:14]  I0423 12:40:33.807278    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0208333
[2018-04-23 20:41:14]  I0423 12:40:33.807571    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 7.26392 (* 1 = 7.26392 loss)
[2018-04-23 20:41:14]  I0423 12:40:33.875999    21 solver.cpp:218] Iteration 400 (9.32203 iter/s, 10.7273s/100 iters), loss = 2.83498
[2018-04-23 20:41:14]  I0423 12:40:33.876049    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.21875
[2018-04-23 20:41:14]  I0423 12:40:33.876063    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 2.83498 (* 1 = 2.83498 loss)
[2018-04-23 20:41:14]  I0423 12:40:33.876077    21 sgd_solver.cpp:105] Iteration 400, lr = 0.1
[2018-04-23 20:41:14]  I0423 12:40:40.528442    21 solver.cpp:330] Iteration 500, Testing net (#0)
[2018-04-23 20:41:14]  I0423 12:40:40.528918    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:41:14]  I0423 12:40:44.538944    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0426683
[2018-04-23 20:41:14]  I0423 12:40:44.539055    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 6.60632 (* 1 = 6.60632 loss)
[2018-04-23 20:41:14]  I0423 12:40:44.615810    21 solver.cpp:218] Iteration 500 (9.31163 iter/s, 10.7393s/100 iters), loss = 1.96046
[2018-04-23 20:41:14]  I0423 12:40:44.615885    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.546875
[2018-04-23 20:41:14]  I0423 12:40:44.615921    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 1.96046 (* 1 = 1.96046 loss)
[2018-04-23 20:41:14]  I0423 12:40:44.615952    21 sgd_solver.cpp:105] Iteration 500, lr = 0.1
[2018-04-23 20:41:14]  I0423 12:40:51.315479    21 solver.cpp:330] Iteration 600, Testing net (#0)
[2018-04-23 20:41:14]  I0423 12:40:51.317024    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:41:14]  I0423 12:40:53.655391    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:41:14]  I0423 12:40:55.503582    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0378606
[2018-04-23 20:41:14]  I0423 12:40:55.504259    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 6.60003 (* 1 = 6.60003 loss)
[2018-04-23 20:41:14]  I0423 12:40:55.574154    21 solver.cpp:218] Iteration 600 (9.12593 iter/s, 10.9578s/100 iters), loss = 2.30599
[2018-04-23 20:41:14]  I0423 12:40:55.574206    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.40625
[2018-04-23 20:41:14]  I0423 12:40:55.574259    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 2.30599 (* 1 = 2.30599 loss)
[2018-04-23 20:41:14]  I0423 12:40:55.574313    21 sgd_solver.cpp:105] Iteration 600, lr = 0.1
[2018-04-23 20:41:14]  I0423 12:41:02.379204    21 solver.cpp:330] Iteration 700, Testing net (#0)
[2018-04-23 20:41:14]  I0423 12:41:02.382306    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:41:14]  I0423 12:41:06.682646    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0282452
[2018-04-23 20:41:14]  I0423 12:41:06.682929    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 6.31032 (* 1 = 6.31032 loss)
[2018-04-23 20:41:14]  I0423 12:41:06.752768    21 solver.cpp:218] Iteration 700 (8.9461 iter/s, 11.1781s/100 iters), loss = 1.36607
[2018-04-23 20:41:14]  I0423 12:41:06.753111    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.6875
[2018-04-23 20:41:14]  I0423 12:41:06.753188    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 1.36607 (* 1 = 1.36607 loss)
[2018-04-23 20:41:14]  I0423 12:41:06.753255    21 sgd_solver.cpp:105] Iteration 700, lr = 0.1
[2018-04-23 20:41:14]  I0423 12:41:13.589272    21 solver.cpp:330] Iteration 800, Testing net (#0)
[2018-04-23 20:41:14]  I0423 12:41:13.592908    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:41:17]  I0423 12:41:17.767858    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0506811
[2018-04-23 20:41:17]  I0423 12:41:17.768138    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.5072 (* 1 = 4.5072 loss)
[2018-04-23 20:41:17]  I0423 12:41:17.835352    21 solver.cpp:218] Iteration 800 (9.0239 iter/s, 11.0817s/100 iters), loss = 1.37853
[2018-04-23 20:41:17]  I0423 12:41:17.835407    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.5625
[2018-04-23 20:41:17]  I0423 12:41:17.835429    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 1.37853 (* 1 = 1.37853 loss)
[2018-04-23 20:41:17]  I0423 12:41:17.835525    21 sgd_solver.cpp:105] Iteration 800, lr = 0.1
[2018-04-23 20:41:24]  I0423 12:41:24.733112    21 solver.cpp:330] Iteration 900, Testing net (#0)
[2018-04-23 20:41:24]  I0423 12:41:24.734182    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:41:29]  I0423 12:41:29.010864    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.025641
[2018-04-23 20:41:29]  I0423 12:41:29.011005    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.76983 (* 1 = 4.76983 loss)
[2018-04-23 20:41:29]  I0423 12:41:29.078444    21 solver.cpp:218] Iteration 900 (8.89485 iter/s, 11.2425s/100 iters), loss = 1.39143
[2018-04-23 20:41:29]  I0423 12:41:29.078487    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.65625
[2018-04-23 20:41:29]  I0423 12:41:29.078502    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 1.39143 (* 1 = 1.39143 loss)
[2018-04-23 20:41:29]  I0423 12:41:29.078522    21 sgd_solver.cpp:105] Iteration 900, lr = 0.1
[2018-04-23 20:41:35]  I0423 12:41:35.980525    21 solver.cpp:330] Iteration 1000, Testing net (#0)
[2018-04-23 20:41:35]  I0423 12:41:35.981549    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:41:40]  I0423 12:41:40.078320    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0410657
[2018-04-23 20:41:40]  I0423 12:41:40.078816    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.46225 (* 1 = 4.46225 loss)
[2018-04-23 20:41:40]  I0423 12:41:40.143137    21 solver.cpp:218] Iteration 1000 (9.03826 iter/s, 11.0641s/100 iters), loss = 1.21928
[2018-04-23 20:41:40]  I0423 12:41:40.143187    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.703125
[2018-04-23 20:41:40]  I0423 12:41:40.143223    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 1.21928 (* 1 = 1.21928 loss)
[2018-04-23 20:41:40]  I0423 12:41:40.143254    21 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
[2018-04-23 20:41:46]  I0423 12:41:46.865784    21 solver.cpp:330] Iteration 1100, Testing net (#0)
[2018-04-23 20:41:46]  I0423 12:41:46.868214    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:41:50]  I0423 12:41:50.989097    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0272436
[2018-04-23 20:41:51]  I0423 12:41:50.990233    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.57308 (* 1 = 4.57308 loss)
[2018-04-23 20:41:51]  I0423 12:41:51.056949    21 solver.cpp:218] Iteration 1100 (9.16321 iter/s, 10.9132s/100 iters), loss = 1.12284
[2018-04-23 20:41:51]  I0423 12:41:51.056996    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.75
[2018-04-23 20:41:51]  I0423 12:41:51.057060    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 1.12284 (* 1 = 1.12284 loss)
[2018-04-23 20:41:51]  I0423 12:41:51.057090    21 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
[2018-04-23 20:41:57]  I0423 12:41:57.784776    21 solver.cpp:330] Iteration 1200, Testing net (#0)
[2018-04-23 20:41:57]  I0423 12:41:57.785773    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:42:01]  I0423 12:42:01.948855    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0228365
[2018-04-23 20:42:02]  I0423 12:42:01.949772    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.68678 (* 1 = 4.68678 loss)
[2018-04-23 20:42:02]  I0423 12:42:02.018038    21 solver.cpp:218] Iteration 1200 (9.12368 iter/s, 10.9605s/100 iters), loss = 0.819355
[2018-04-23 20:42:02]  I0423 12:42:02.018199    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.796875
[2018-04-23 20:42:02]  I0423 12:42:02.018272    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.819355 (* 1 = 0.819355 loss)
[2018-04-23 20:42:02]  I0423 12:42:02.018527    21 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
[2018-04-23 20:42:08]  I0423 12:42:08.859176    21 solver.cpp:330] Iteration 1300, Testing net (#0)
[2018-04-23 20:42:08]  I0423 12:42:08.861155    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:42:09]  I0423 12:42:09.312640    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:42:12]  I0423 12:42:12.660303    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0186298
[2018-04-23 20:42:12]  I0423 12:42:12.661139    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.6601 (* 1 = 4.6601 loss)
[2018-04-23 20:42:12]  I0423 12:42:12.728973    21 solver.cpp:218] Iteration 1300 (9.33686 iter/s, 10.7102s/100 iters), loss = 1.06402
[2018-04-23 20:42:12]  I0423 12:42:12.729115    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.75
[2018-04-23 20:42:12]  I0423 12:42:12.729195    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 1.06402 (* 1 = 1.06402 loss)
[2018-04-23 20:42:12]  I0423 12:42:12.729281    21 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
[2018-04-23 20:42:19]  I0423 12:42:19.532405    21 solver.cpp:330] Iteration 1400, Testing net (#0)
[2018-04-23 20:42:19]  I0423 12:42:19.533205    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:42:23]  I0423 12:42:23.436255    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0140224
[2018-04-23 20:42:23]  I0423 12:42:23.436484    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.90558 (* 1 = 4.90558 loss)
[2018-04-23 20:42:23]  I0423 12:42:23.504062    21 solver.cpp:218] Iteration 1400 (9.28124 iter/s, 10.7744s/100 iters), loss = 0.658932
[2018-04-23 20:42:23]  I0423 12:42:23.504155    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.828125
[2018-04-23 20:42:23]  I0423 12:42:23.504220    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.658932 (* 1 = 0.658932 loss)
[2018-04-23 20:42:23]  I0423 12:42:23.504318    21 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
[2018-04-23 20:42:30]  I0423 12:42:30.212613    21 solver.cpp:330] Iteration 1500, Testing net (#0)
[2018-04-23 20:42:30]  I0423 12:42:30.213147    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:42:34]  I0423 12:42:34.270838    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0108173
[2018-04-23 20:42:34]  I0423 12:42:34.272801    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.84126 (* 1 = 4.84126 loss)
[2018-04-23 20:42:34]  I0423 12:42:34.343509    21 solver.cpp:218] Iteration 1500 (9.22625 iter/s, 10.8386s/100 iters), loss = 0.58804
[2018-04-23 20:42:34]  I0423 12:42:34.343627    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.890625
[2018-04-23 20:42:34]  I0423 12:42:34.343704    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.58804 (* 1 = 0.58804 loss)
[2018-04-23 20:42:34]  I0423 12:42:34.343972    21 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
[2018-04-23 20:42:41]  I0423 12:42:41.214581    21 solver.cpp:330] Iteration 1600, Testing net (#0)
[2018-04-23 20:42:41]  I0423 12:42:41.216789    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:42:45]  I0423 12:42:45.179651    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0258413
[2018-04-23 20:42:45]  I0423 12:42:45.180069    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.84048 (* 1 = 4.84048 loss)
[2018-04-23 20:42:45]  I0423 12:42:45.250324    21 solver.cpp:218] Iteration 1600 (9.16912 iter/s, 10.9062s/100 iters), loss = 0.740914
[2018-04-23 20:42:45]  I0423 12:42:45.250370    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.78125
[2018-04-23 20:42:45]  I0423 12:42:45.250393    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.740914 (* 1 = 0.740914 loss)
[2018-04-23 20:42:45]  I0423 12:42:45.250418    21 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
[2018-04-23 20:42:52]  I0423 12:42:52.159600    21 solver.cpp:330] Iteration 1700, Testing net (#0)
[2018-04-23 20:42:52]  I0423 12:42:52.161499    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:42:56]  I0423 12:42:56.092957    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0170272
[2018-04-23 20:42:56]  I0423 12:42:56.093293    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 5.05788 (* 1 = 5.05788 loss)
[2018-04-23 20:42:56]  I0423 12:42:56.162063    21 solver.cpp:218] Iteration 1700 (9.16492 iter/s, 10.9112s/100 iters), loss = 0.625263
[2018-04-23 20:42:56]  I0423 12:42:56.162118    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.859375
[2018-04-23 20:42:56]  I0423 12:42:56.162132    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.625263 (* 1 = 0.625263 loss)
[2018-04-23 20:42:56]  I0423 12:42:56.162384    21 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
[2018-04-23 20:43:02]  I0423 12:43:02.965798    21 solver.cpp:330] Iteration 1800, Testing net (#0)
[2018-04-23 20:43:03]  I0423 12:43:02.966576    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:43:06]  I0423 12:43:06.819129    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0120192
[2018-04-23 20:43:06]  I0423 12:43:06.819384    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 5.31621 (* 1 = 5.31621 loss)
[2018-04-23 20:43:06]  I0423 12:43:06.888687    21 solver.cpp:218] Iteration 1800 (9.32309 iter/s, 10.7261s/100 iters), loss = 0.800197
[2018-04-23 20:43:06]  I0423 12:43:06.888731    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.734375
[2018-04-23 20:43:06]  I0423 12:43:06.888752    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.800197 (* 1 = 0.800197 loss)
[2018-04-23 20:43:06]  I0423 12:43:06.888777    21 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
[2018-04-23 20:43:13]  I0423 12:43:13.696264    21 solver.cpp:330] Iteration 1900, Testing net (#0)
[2018-04-23 20:43:13]  I0423 12:43:13.696848    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:43:16]  I0423 12:43:16.458935    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:43:17]  I0423 12:43:17.678113    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0130208
[2018-04-23 20:43:17]  I0423 12:43:17.678231    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.9198 (* 1 = 4.9198 loss)
[2018-04-23 20:43:17]  I0423 12:43:17.747473    21 solver.cpp:218] Iteration 1900 (9.2096 iter/s, 10.8582s/100 iters), loss = 0.569096
[2018-04-23 20:43:17]  I0423 12:43:17.747529    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.8125
[2018-04-23 20:43:17]  I0423 12:43:17.747542    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.569096 (* 1 = 0.569096 loss)
[2018-04-23 20:43:17]  I0423 12:43:17.747565    21 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
[2018-04-23 20:43:24]  I0423 12:43:24.478291    21 solver.cpp:447] Snapshotting to binary proto file /data/output/myres20__iter_2000.caffemodel
[2018-04-23 20:43:24]  I0423 12:43:24.734308    21 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /data/output/myres20__iter_2000.solverstate
[2018-04-23 20:43:24]  I0423 12:43:24.816607    21 solver.cpp:330] Iteration 2000, Testing net (#0)
[2018-04-23 20:43:24]  I0423 12:43:24.816948    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:43:28]  I0423 12:43:28.637239    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0130208
[2018-04-23 20:43:28]  I0423 12:43:28.637522    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 5.03336 (* 1 = 5.03336 loss)
[2018-04-23 20:43:28]  I0423 12:43:28.703348    21 solver.cpp:218] Iteration 2000 (9.12813 iter/s, 10.9551s/100 iters), loss = 0.844493
[2018-04-23 20:43:28]  I0423 12:43:28.703389    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.8125
[2018-04-23 20:43:28]  I0423 12:43:28.703408    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.844493 (* 1 = 0.844493 loss)
[2018-04-23 20:43:28]  I0423 12:43:28.703433    21 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
[2018-04-23 20:43:35]  I0423 12:43:35.403651    21 solver.cpp:330] Iteration 2100, Testing net (#0)
[2018-04-23 20:43:35]  I0423 12:43:35.405668    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:43:39]  I0423 12:43:39.251827    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.010617
[2018-04-23 20:43:39]  I0423 12:43:39.251976    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.94507 (* 1 = 4.94507 loss)
[2018-04-23 20:43:39]  I0423 12:43:39.318641    21 solver.cpp:218] Iteration 2100 (9.42101 iter/s, 10.6146s/100 iters), loss = 0.555634
[2018-04-23 20:43:39]  I0423 12:43:39.318706    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.84375
[2018-04-23 20:43:39]  I0423 12:43:39.318722    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.555634 (* 1 = 0.555634 loss)
[2018-04-23 20:43:39]  I0423 12:43:39.318747    21 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
[2018-04-23 20:43:46]  I0423 12:43:46.144646    21 solver.cpp:330] Iteration 2200, Testing net (#0)
[2018-04-23 20:43:46]  I0423 12:43:46.145313    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:43:50]  I0423 12:43:50.203040    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.010016
[2018-04-23 20:43:50]  I0423 12:43:50.203514    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 5.03838 (* 1 = 5.03838 loss)
[2018-04-23 20:43:50]  I0423 12:43:50.297389    21 solver.cpp:218] Iteration 2200 (9.10931 iter/s, 10.9778s/100 iters), loss = 0.51264
[2018-04-23 20:43:50]  I0423 12:43:50.297458    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.875
[2018-04-23 20:43:50]  I0423 12:43:50.297507    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.51264 (* 1 = 0.51264 loss)
[2018-04-23 20:43:50]  I0423 12:43:50.297737    21 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
[2018-04-23 20:43:57]  I0423 12:43:57.051756    21 solver.cpp:330] Iteration 2300, Testing net (#0)
[2018-04-23 20:43:57]  I0423 12:43:57.053310    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:44:01]  I0423 12:44:01.179368    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0104167
[2018-04-23 20:44:01]  I0423 12:44:01.179996    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 5.18412 (* 1 = 5.18412 loss)
[2018-04-23 20:44:01]  I0423 12:44:01.247622    21 solver.cpp:218] Iteration 2300 (9.13287 iter/s, 10.9495s/100 iters), loss = 0.546615
[2018-04-23 20:44:01]  I0423 12:44:01.247680    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.796875
[2018-04-23 20:44:01]  I0423 12:44:01.247694    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.546615 (* 1 = 0.546615 loss)
[2018-04-23 20:44:01]  I0423 12:44:01.247719    21 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
[2018-04-23 20:44:08]  I0423 12:44:08.027621    21 solver.cpp:330] Iteration 2400, Testing net (#0)
[2018-04-23 20:44:08]  I0423 12:44:08.031625    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:44:12]  I0423 12:44:12.137442    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0126202
[2018-04-23 20:44:12]  I0423 12:44:12.137820    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 5.02609 (* 1 = 5.02609 loss)
[2018-04-23 20:44:12]  I0423 12:44:12.208045    21 solver.cpp:218] Iteration 2400 (9.12438 iter/s, 10.9596s/100 iters), loss = 0.640124
[2018-04-23 20:44:12]  I0423 12:44:12.208092    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.828125
[2018-04-23 20:44:12]  I0423 12:44:12.208142    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.640124 (* 1 = 0.640124 loss)
[2018-04-23 20:44:12]  I0423 12:44:12.208356    21 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
[2018-04-23 20:44:18]  I0423 12:44:18.980279    21 solver.cpp:330] Iteration 2500, Testing net (#0)
[2018-04-23 20:44:18]  I0423 12:44:18.983832    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:44:23]  I0423 12:44:23.065126    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0118189
[2018-04-23 20:44:23]  I0423 12:44:23.065554    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.93083 (* 1 = 4.93083 loss)
[2018-04-23 20:44:23]  I0423 12:44:23.149633    21 solver.cpp:218] Iteration 2500 (9.14005 iter/s, 10.9409s/100 iters), loss = 0.786675
[2018-04-23 20:44:23]  I0423 12:44:23.149690    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.828125
[2018-04-23 20:44:23]  I0423 12:44:23.149745    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.786675 (* 1 = 0.786675 loss)
[2018-04-23 20:44:23]  I0423 12:44:23.150007    21 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
[2018-04-23 20:44:29]  I0423 12:44:29.883195    21 solver.cpp:330] Iteration 2600, Testing net (#0)
[2018-04-23 20:44:29]  I0423 12:44:29.887076    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:44:30]  I0423 12:44:30.993808    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:44:34]  I0423 12:44:34.061556    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0136218
[2018-04-23 20:44:34]  I0423 12:44:34.062434    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.99121 (* 1 = 4.99121 loss)
[2018-04-23 20:44:34]  I0423 12:44:34.131546    21 solver.cpp:218] Iteration 2600 (9.10648 iter/s, 10.9812s/100 iters), loss = 0.66245
[2018-04-23 20:44:34]  I0423 12:44:34.131615    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.796875
[2018-04-23 20:44:34]  I0423 12:44:34.131628    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.66245 (* 1 = 0.66245 loss)
[2018-04-23 20:44:34]  I0423 12:44:34.131709    21 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
[2018-04-23 20:44:40]  I0423 12:44:40.920114    21 solver.cpp:330] Iteration 2700, Testing net (#0)
[2018-04-23 20:44:40]  I0423 12:44:40.925312    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:44:44]  I0423 12:44:44.941251    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0166266
[2018-04-23 20:44:44]  I0423 12:44:44.941648    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 5.05849 (* 1 = 5.05849 loss)
[2018-04-23 20:44:45]  I0423 12:44:45.012606    21 solver.cpp:218] Iteration 2700 (9.19089 iter/s, 10.8803s/100 iters), loss = 0.651658
[2018-04-23 20:44:45]  I0423 12:44:45.012698    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.875
[2018-04-23 20:44:45]  I0423 12:44:45.012933    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.651658 (* 1 = 0.651658 loss)
[2018-04-23 20:44:45]  I0423 12:44:45.013196    21 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
[2018-04-23 20:44:51]  I0423 12:44:51.816264    21 solver.cpp:330] Iteration 2800, Testing net (#0)
[2018-04-23 20:44:51]  I0423 12:44:51.816910    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:44:55]  I0423 12:44:55.746043    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0130208
[2018-04-23 20:44:55]  I0423 12:44:55.746181    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 5.07133 (* 1 = 5.07133 loss)
[2018-04-23 20:44:55]  I0423 12:44:55.843327    21 solver.cpp:218] Iteration 2800 (9.23362 iter/s, 10.83s/100 iters), loss = 0.456982
[2018-04-23 20:44:55]  I0423 12:44:55.843403    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.859375
[2018-04-23 20:44:55]  I0423 12:44:55.843420    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.456982 (* 1 = 0.456982 loss)
[2018-04-23 20:44:55]  I0423 12:44:55.843439    21 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
[2018-04-23 20:45:02]  I0423 12:45:02.567080    21 solver.cpp:330] Iteration 2900, Testing net (#0)
[2018-04-23 20:45:02]  I0423 12:45:02.569933    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:45:06]  I0423 12:45:06.466845    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0214343
[2018-04-23 20:45:06]  I0423 12:45:06.467280    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 5.24622 (* 1 = 5.24622 loss)
[2018-04-23 20:45:06]  I0423 12:45:06.533123    21 solver.cpp:218] Iteration 2900 (9.35532 iter/s, 10.6891s/100 iters), loss = 0.375759
[2018-04-23 20:45:06]  I0423 12:45:06.533172    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.921875
[2018-04-23 20:45:06]  I0423 12:45:06.533223    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.375759 (* 1 = 0.375759 loss)
[2018-04-23 20:45:06]  I0423 12:45:06.533424    21 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
[2018-04-23 20:45:13]  I0423 12:45:13.252033    21 solver.cpp:330] Iteration 3000, Testing net (#0)
[2018-04-23 20:45:13]  I0423 12:45:13.253737    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:45:17]  I0423 12:45:17.280360    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.015024
[2018-04-23 20:45:17]  I0423 12:45:17.280844    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.95463 (* 1 = 4.95463 loss)
[2018-04-23 20:45:17]  I0423 12:45:17.347522    21 solver.cpp:218] Iteration 3000 (9.24756 iter/s, 10.8137s/100 iters), loss = 0.68074
[2018-04-23 20:45:17]  I0423 12:45:17.347571    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.859375
[2018-04-23 20:45:17]  I0423 12:45:17.347584    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.68074 (* 1 = 0.68074 loss)
[2018-04-23 20:45:17]  I0423 12:45:17.347597    21 sgd_solver.cpp:46] MultiStep Status: Iteration 3000, step = 1
[2018-04-23 20:45:17]  I0423 12:45:17.347609    21 sgd_solver.cpp:105] Iteration 3000, lr = 0.01
[2018-04-23 20:45:24]  I0423 12:45:24.041267    21 solver.cpp:330] Iteration 3100, Testing net (#0)
[2018-04-23 20:45:24]  I0423 12:45:24.043498    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:45:28]  I0423 12:45:28.003293    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0196314
[2018-04-23 20:45:28]  I0423 12:45:28.003588    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.87613 (* 1 = 4.87613 loss)
[2018-04-23 20:45:28]  I0423 12:45:28.069945    21 solver.cpp:218] Iteration 3100 (9.32693 iter/s, 10.7216s/100 iters), loss = 0.300487
[2018-04-23 20:45:28]  I0423 12:45:28.069990    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.921875
[2018-04-23 20:45:28]  I0423 12:45:28.070039    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.300487 (* 1 = 0.300487 loss)
[2018-04-23 20:45:28]  I0423 12:45:28.070235    21 sgd_solver.cpp:105] Iteration 3100, lr = 0.01
[2018-04-23 20:45:34]  I0423 12:45:34.744284    21 solver.cpp:330] Iteration 3200, Testing net (#0)
[2018-04-23 20:45:34]  I0423 12:45:34.744876    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:45:38]  I0423 12:45:38.195363    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:45:38]  I0423 12:45:38.865909    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0218349
[2018-04-23 20:45:38]  I0423 12:45:38.866037    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.88843 (* 1 = 4.88843 loss)
[2018-04-23 20:45:38]  I0423 12:45:38.930991    21 solver.cpp:218] Iteration 3200 (9.20787 iter/s, 10.8603s/100 iters), loss = 0.195553
[2018-04-23 20:45:38]  I0423 12:45:38.931044    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:45:39]  I0423 12:45:38.931057    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.195553 (* 1 = 0.195553 loss)
[2018-04-23 20:45:39]  I0423 12:45:38.931078    21 sgd_solver.cpp:105] Iteration 3200, lr = 0.01
[2018-04-23 20:45:45]  I0423 12:45:45.655525    21 solver.cpp:330] Iteration 3300, Testing net (#0)
[2018-04-23 20:45:45]  I0423 12:45:45.657383    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:45:49]  I0423 12:45:49.696910    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0196314
[2018-04-23 20:45:49]  I0423 12:45:49.697293    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.91763 (* 1 = 4.91763 loss)
[2018-04-23 20:45:49]  I0423 12:45:49.769501    21 solver.cpp:218] Iteration 3300 (9.22701 iter/s, 10.8377s/100 iters), loss = 0.120453
[2018-04-23 20:45:49]  I0423 12:45:49.769634    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.96875
[2018-04-23 20:45:49]  I0423 12:45:49.769882    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.120453 (* 1 = 0.120453 loss)
[2018-04-23 20:45:49]  I0423 12:45:49.769964    21 sgd_solver.cpp:105] Iteration 3300, lr = 0.01
[2018-04-23 20:45:56]  I0423 12:45:56.501107    21 solver.cpp:330] Iteration 3400, Testing net (#0)
[2018-04-23 20:45:56]  I0423 12:45:56.501855    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:46:00]  I0423 12:46:00.451879    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.016226
[2018-04-23 20:46:00]  I0423 12:46:00.452143    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.9512 (* 1 = 4.9512 loss)
[2018-04-23 20:46:00]  I0423 12:46:00.518048    21 solver.cpp:218] Iteration 3400 (9.3043 iter/s, 10.7477s/100 iters), loss = 0.125026
[2018-04-23 20:46:00]  I0423 12:46:00.518213    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.96875
[2018-04-23 20:46:00]  I0423 12:46:00.518262    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.125026 (* 1 = 0.125026 loss)
[2018-04-23 20:46:00]  I0423 12:46:00.518311    21 sgd_solver.cpp:105] Iteration 3400, lr = 0.01
[2018-04-23 20:46:07]  I0423 12:46:07.190259    21 solver.cpp:330] Iteration 3500, Testing net (#0)
[2018-04-23 20:46:07]  I0423 12:46:07.190992    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:46:11]  I0423 12:46:11.046043    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0248397
[2018-04-23 20:46:11]  I0423 12:46:11.046308    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.89353 (* 1 = 4.89353 loss)
[2018-04-23 20:46:11]  I0423 12:46:11.111054    21 solver.cpp:218] Iteration 3500 (9.44093 iter/s, 10.5922s/100 iters), loss = 0.0767874
[2018-04-23 20:46:11]  I0423 12:46:11.111104    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:46:11]  I0423 12:46:11.111121    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0767874 (* 1 = 0.0767874 loss)
[2018-04-23 20:46:11]  I0423 12:46:11.111146    21 sgd_solver.cpp:105] Iteration 3500, lr = 0.01
[2018-04-23 20:46:17]  I0423 12:46:17.783056    21 solver.cpp:330] Iteration 3600, Testing net (#0)
[2018-04-23 20:46:17]  I0423 12:46:17.783620    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:46:21]  I0423 12:46:21.908690    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0196314
[2018-04-23 20:46:21]  I0423 12:46:21.908840    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.94842 (* 1 = 4.94842 loss)
[2018-04-23 20:46:21]  I0423 12:46:21.974759    21 solver.cpp:218] Iteration 3600 (9.20559 iter/s, 10.863s/100 iters), loss = 0.0847339
[2018-04-23 20:46:21]  I0423 12:46:21.974805    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:46:21]  I0423 12:46:21.974819    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.084734 (* 1 = 0.084734 loss)
[2018-04-23 20:46:21]  I0423 12:46:21.974833    21 sgd_solver.cpp:105] Iteration 3600, lr = 0.01
[2018-04-23 20:46:28]  I0423 12:46:28.643630    21 solver.cpp:330] Iteration 3700, Testing net (#0)
[2018-04-23 20:46:28]  I0423 12:46:28.644018    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:46:32]  I0423 12:46:32.502162    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0288462
[2018-04-23 20:46:32]  I0423 12:46:32.502281    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.86438 (* 1 = 4.86438 loss)
[2018-04-23 20:46:32]  I0423 12:46:32.568225    21 solver.cpp:218] Iteration 3700 (9.4404 iter/s, 10.5928s/100 iters), loss = 0.229104
[2018-04-23 20:46:32]  I0423 12:46:32.568276    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.953125
[2018-04-23 20:46:32]  I0423 12:46:32.568290    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.229104 (* 1 = 0.229104 loss)
[2018-04-23 20:46:32]  I0423 12:46:32.568303    21 sgd_solver.cpp:105] Iteration 3700, lr = 0.01
[2018-04-23 20:46:39]  I0423 12:46:39.224843    21 solver.cpp:330] Iteration 3800, Testing net (#0)
[2018-04-23 20:46:39]  I0423 12:46:39.226260    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:46:43]  I0423 12:46:43.093650    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0428686
[2018-04-23 20:46:43]  I0423 12:46:43.093902    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.70273 (* 1 = 4.70273 loss)
[2018-04-23 20:46:43]  I0423 12:46:43.160640    21 solver.cpp:218] Iteration 3800 (9.44134 iter/s, 10.5917s/100 iters), loss = 0.0962915
[2018-04-23 20:46:43]  I0423 12:46:43.160687    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:46:43]  I0423 12:46:43.160707    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0962915 (* 1 = 0.0962915 loss)
[2018-04-23 20:46:43]  I0423 12:46:43.160723    21 sgd_solver.cpp:105] Iteration 3800, lr = 0.01
[2018-04-23 20:46:49]  I0423 12:46:49.800307    21 solver.cpp:330] Iteration 3900, Testing net (#0)
[2018-04-23 20:46:49]  I0423 12:46:49.801529    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:46:51]  I0423 12:46:51.385383    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:46:53]  I0423 12:46:53.630977    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0390625
[2018-04-23 20:46:53]  I0423 12:46:53.631052    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.7299 (* 1 = 4.7299 loss)
[2018-04-23 20:46:53]  I0423 12:46:53.695437    21 solver.cpp:218] Iteration 3900 (9.49296 iter/s, 10.5341s/100 iters), loss = 0.157542
[2018-04-23 20:46:53]  I0423 12:46:53.695485    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.96875
[2018-04-23 20:46:53]  I0423 12:46:53.695498    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.157542 (* 1 = 0.157542 loss)
[2018-04-23 20:46:53]  I0423 12:46:53.695518    21 sgd_solver.cpp:105] Iteration 3900, lr = 0.01
[2018-04-23 20:47:00]  I0423 12:47:00.403348    21 solver.cpp:447] Snapshotting to binary proto file /data/output/myres20__iter_4000.caffemodel
[2018-04-23 20:47:00]  I0423 12:47:00.579116    21 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /data/output/myres20__iter_4000.solverstate
[2018-04-23 20:47:00]  I0423 12:47:00.654664    21 solver.cpp:330] Iteration 4000, Testing net (#0)
[2018-04-23 20:47:00]  I0423 12:47:00.655694    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:47:04]  I0423 12:47:04.647663    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0426683
[2018-04-23 20:47:04]  I0423 12:47:04.647799    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.66175 (* 1 = 4.66175 loss)
[2018-04-23 20:47:04]  I0423 12:47:04.713484    21 solver.cpp:218] Iteration 4000 (9.07659 iter/s, 11.0174s/100 iters), loss = 0.0822438
[2018-04-23 20:47:04]  I0423 12:47:04.713532    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:47:04]  I0423 12:47:04.713544    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0822439 (* 1 = 0.0822439 loss)
[2018-04-23 20:47:04]  I0423 12:47:04.713599    21 sgd_solver.cpp:105] Iteration 4000, lr = 0.01
[2018-04-23 20:47:11]  I0423 12:47:11.429162    21 solver.cpp:330] Iteration 4100, Testing net (#0)
[2018-04-23 20:47:11]  I0423 12:47:11.431972    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:47:15]  I0423 12:47:15.353905    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0448718
[2018-04-23 20:47:15]  I0423 12:47:15.354214    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.66541 (* 1 = 4.66541 loss)
[2018-04-23 20:47:15]  I0423 12:47:15.420884    21 solver.cpp:218] Iteration 4100 (9.33992 iter/s, 10.7067s/100 iters), loss = 0.371782
[2018-04-23 20:47:15]  I0423 12:47:15.420930    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.9375
[2018-04-23 20:47:15]  I0423 12:47:15.420949    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.371782 (* 1 = 0.371782 loss)
[2018-04-23 20:47:15]  I0423 12:47:15.421248    21 sgd_solver.cpp:105] Iteration 4100, lr = 0.01
[2018-04-23 20:47:22]  I0423 12:47:22.123456    21 solver.cpp:330] Iteration 4200, Testing net (#0)
[2018-04-23 20:47:22]  I0423 12:47:22.124413    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:47:26]  I0423 12:47:26.058017    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0552885
[2018-04-23 20:47:26]  I0423 12:47:26.058097    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.54803 (* 1 = 4.54803 loss)
[2018-04-23 20:47:26]  I0423 12:47:26.122975    21 solver.cpp:218] Iteration 4200 (9.34454 iter/s, 10.7014s/100 iters), loss = 0.127918
[2018-04-23 20:47:26]  I0423 12:47:26.123042    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.96875
[2018-04-23 20:47:26]  I0423 12:47:26.123060    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.127918 (* 1 = 0.127918 loss)
[2018-04-23 20:47:26]  I0423 12:47:26.123085    21 sgd_solver.cpp:105] Iteration 4200, lr = 0.01
[2018-04-23 20:47:32]  I0423 12:47:32.841624    21 solver.cpp:330] Iteration 4300, Testing net (#0)
[2018-04-23 20:47:32]  I0423 12:47:32.843392    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:47:36]  I0423 12:47:36.916347    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0598958
[2018-04-23 20:47:36]  I0423 12:47:36.916649    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.48787 (* 1 = 4.48787 loss)
[2018-04-23 20:47:36]  I0423 12:47:36.983692    21 solver.cpp:218] Iteration 4300 (9.20806 iter/s, 10.86s/100 iters), loss = 0.141218
[2018-04-23 20:47:36]  I0423 12:47:36.983737    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.953125
[2018-04-23 20:47:37]  I0423 12:47:36.983754    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.141218 (* 1 = 0.141218 loss)
[2018-04-23 20:47:37]  I0423 12:47:36.983945    21 sgd_solver.cpp:105] Iteration 4300, lr = 0.01
[2018-04-23 20:47:43]  I0423 12:47:43.754315    21 solver.cpp:330] Iteration 4400, Testing net (#0)
[2018-04-23 20:47:43]  I0423 12:47:43.757850    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:47:47]  I0423 12:47:47.865824    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0761218
[2018-04-23 20:47:47]  I0423 12:47:47.866693    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.24352 (* 1 = 4.24352 loss)
[2018-04-23 20:47:47]  I0423 12:47:47.937634    21 solver.cpp:218] Iteration 4400 (9.12967 iter/s, 10.9533s/100 iters), loss = 0.0316092
[2018-04-23 20:47:47]  I0423 12:47:47.937717    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:47:47]  I0423 12:47:47.937731    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0316092 (* 1 = 0.0316092 loss)
[2018-04-23 20:47:47]  I0423 12:47:47.937816    21 sgd_solver.cpp:105] Iteration 4400, lr = 0.01
[2018-04-23 20:47:54]  I0423 12:47:54.680347    21 solver.cpp:330] Iteration 4500, Testing net (#0)
[2018-04-23 20:47:54]  I0423 12:47:54.683019    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:47:58]  I0423 12:47:58.474689    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:47:58]  I0423 12:47:58.551584    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.0917468
[2018-04-23 20:47:58]  I0423 12:47:58.551821    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 4.07129 (* 1 = 4.07129 loss)
[2018-04-23 20:47:58]  I0423 12:47:58.616659    21 solver.cpp:218] Iteration 4500 (9.36473 iter/s, 10.6784s/100 iters), loss = 0.0935827
[2018-04-23 20:47:58]  I0423 12:47:58.616704    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:47:58]  I0423 12:47:58.616724    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0935828 (* 1 = 0.0935828 loss)
[2018-04-23 20:47:58]  I0423 12:47:58.616746    21 sgd_solver.cpp:105] Iteration 4500, lr = 0.01
[2018-04-23 20:48:05]  I0423 12:48:05.311285    21 solver.cpp:330] Iteration 4600, Testing net (#0)
[2018-04-23 20:48:05]  I0423 12:48:05.311911    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:48:09]  I0423 12:48:09.328174    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.101162
[2018-04-23 20:48:09]  I0423 12:48:09.328255    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 3.994 (* 1 = 3.994 loss)
[2018-04-23 20:48:09]  I0423 12:48:09.393558    21 solver.cpp:218] Iteration 4600 (9.27964 iter/s, 10.7763s/100 iters), loss = 0.039271
[2018-04-23 20:48:09]  I0423 12:48:09.393604    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:48:09]  I0423 12:48:09.393617    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.039271 (* 1 = 0.039271 loss)
[2018-04-23 20:48:09]  I0423 12:48:09.393643    21 sgd_solver.cpp:105] Iteration 4600, lr = 0.01
[2018-04-23 20:48:16]  I0423 12:48:16.085088    21 solver.cpp:330] Iteration 4700, Testing net (#0)
[2018-04-23 20:48:16]  I0423 12:48:16.085474    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:48:20]  I0423 12:48:20.120937    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.113181
[2018-04-23 20:48:20]  I0423 12:48:20.121078    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 3.93449 (* 1 = 3.93449 loss)
[2018-04-23 20:48:20]  I0423 12:48:20.187610    21 solver.cpp:218] Iteration 4700 (9.26489 iter/s, 10.7934s/100 iters), loss = 0.0286934
[2018-04-23 20:48:20]  I0423 12:48:20.187670    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:48:20]  I0423 12:48:20.187685    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0286934 (* 1 = 0.0286934 loss)
[2018-04-23 20:48:20]  I0423 12:48:20.187705    21 sgd_solver.cpp:105] Iteration 4700, lr = 0.01
[2018-04-23 20:48:26]  I0423 12:48:26.908109    21 solver.cpp:330] Iteration 4800, Testing net (#0)
[2018-04-23 20:48:26]  I0423 12:48:26.908862    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:48:30]  I0423 12:48:30.782066    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.11278
[2018-04-23 20:48:30]  I0423 12:48:30.782315    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 3.92048 (* 1 = 3.92048 loss)
[2018-04-23 20:48:30]  I0423 12:48:30.848491    21 solver.cpp:218] Iteration 4800 (9.38063 iter/s, 10.6603s/100 iters), loss = 0.0694596
[2018-04-23 20:48:30]  I0423 12:48:30.848536    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:48:30]  I0423 12:48:30.848548    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0694597 (* 1 = 0.0694597 loss)
[2018-04-23 20:48:30]  I0423 12:48:30.848728    21 sgd_solver.cpp:105] Iteration 4800, lr = 0.01
[2018-04-23 20:48:37]  I0423 12:48:37.571674    21 solver.cpp:330] Iteration 4900, Testing net (#0)
[2018-04-23 20:48:37]  I0423 12:48:37.572207    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:48:41]  I0423 12:48:41.460294    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.15645
[2018-04-23 20:48:41]  I0423 12:48:41.460383    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 3.67335 (* 1 = 3.67335 loss)
[2018-04-23 20:48:41]  I0423 12:48:41.525498    21 solver.cpp:218] Iteration 4900 (9.3664 iter/s, 10.6765s/100 iters), loss = 0.0760483
[2018-04-23 20:48:41]  I0423 12:48:41.525549    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:48:41]  I0423 12:48:41.525568    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0760483 (* 1 = 0.0760483 loss)
[2018-04-23 20:48:41]  I0423 12:48:41.525593    21 sgd_solver.cpp:105] Iteration 4900, lr = 0.01
[2018-04-23 20:48:48]  I0423 12:48:48.228739    21 solver.cpp:330] Iteration 5000, Testing net (#0)
[2018-04-23 20:48:48]  I0423 12:48:48.229051    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:48:52]  I0423 12:48:52.136637    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.190505
[2018-04-23 20:48:52]  I0423 12:48:52.136710    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 3.44029 (* 1 = 3.44029 loss)
[2018-04-23 20:48:52]  I0423 12:48:52.201993    21 solver.cpp:218] Iteration 5000 (9.36674 iter/s, 10.6761s/100 iters), loss = 0.0379694
[2018-04-23 20:48:52]  I0423 12:48:52.202039    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:48:52]  I0423 12:48:52.202052    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0379694 (* 1 = 0.0379694 loss)
[2018-04-23 20:48:52]  I0423 12:48:52.202065    21 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 2
[2018-04-23 20:48:52]  I0423 12:48:52.202118    21 sgd_solver.cpp:105] Iteration 5000, lr = 0.001
[2018-04-23 20:48:58]  I0423 12:48:58.894356    21 solver.cpp:330] Iteration 5100, Testing net (#0)
[2018-04-23 20:48:58]  I0423 12:48:58.896591    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:49:02]  I0423 12:49:02.779439    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.292468
[2018-04-23 20:49:02]  I0423 12:49:02.779562    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.95973 (* 1 = 2.95973 loss)
[2018-04-23 20:49:02]  I0423 12:49:02.847436    21 solver.cpp:218] Iteration 5100 (9.39408 iter/s, 10.645s/100 iters), loss = 0.0297898
[2018-04-23 20:49:02]  I0423 12:49:02.847512    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:49:02]  I0423 12:49:02.847527    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0297898 (* 1 = 0.0297898 loss)
[2018-04-23 20:49:02]  I0423 12:49:02.847582    21 sgd_solver.cpp:105] Iteration 5100, lr = 0.001
[2018-04-23 20:49:09]  I0423 12:49:09.613734    21 solver.cpp:330] Iteration 5200, Testing net (#0)
[2018-04-23 20:49:09]  I0423 12:49:09.614223    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:49:11]  I0423 12:49:11.829648    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:49:13]  I0423 12:49:13.661159    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.391827
[2018-04-23 20:49:13]  I0423 12:49:13.661276    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.48611 (* 1 = 2.48611 loss)
[2018-04-23 20:49:13]  I0423 12:49:13.726950    21 solver.cpp:218] Iteration 5200 (9.19195 iter/s, 10.8791s/100 iters), loss = 0.0262863
[2018-04-23 20:49:13]  I0423 12:49:13.726994    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:49:13]  I0423 12:49:13.727010    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0262863 (* 1 = 0.0262863 loss)
[2018-04-23 20:49:13]  I0423 12:49:13.727077    21 sgd_solver.cpp:105] Iteration 5200, lr = 0.001
[2018-04-23 20:49:20]  I0423 12:49:20.424563    21 solver.cpp:330] Iteration 5300, Testing net (#0)
[2018-04-23 20:49:20]  I0423 12:49:20.425873    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:49:24]  I0423 12:49:24.352558    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.497997
[2018-04-23 20:49:24]  I0423 12:49:24.352846    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.05907 (* 1 = 2.05907 loss)
[2018-04-23 20:49:24]  I0423 12:49:24.418045    21 solver.cpp:218] Iteration 5300 (9.35388 iter/s, 10.6908s/100 iters), loss = 0.0218132
[2018-04-23 20:49:24]  I0423 12:49:24.418112    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:49:24]  I0423 12:49:24.418131    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0218132 (* 1 = 0.0218132 loss)
[2018-04-23 20:49:24]  I0423 12:49:24.418184    21 sgd_solver.cpp:105] Iteration 5300, lr = 0.001
[2018-04-23 20:49:31]  I0423 12:49:31.111362    21 solver.cpp:330] Iteration 5400, Testing net (#0)
[2018-04-23 20:49:31]  I0423 12:49:31.112349    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:49:35]  I0423 12:49:35.099043    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.579527
[2018-04-23 20:49:35]  I0423 12:49:35.099231    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.71778 (* 1 = 1.71778 loss)
[2018-04-23 20:49:35]  I0423 12:49:35.164252    21 solver.cpp:218] Iteration 5400 (9.30593 iter/s, 10.7458s/100 iters), loss = 0.025322
[2018-04-23 20:49:35]  I0423 12:49:35.164295    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:49:35]  I0423 12:49:35.164307    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0253221 (* 1 = 0.0253221 loss)
[2018-04-23 20:49:35]  I0423 12:49:35.164321    21 sgd_solver.cpp:105] Iteration 5400, lr = 0.001
[2018-04-23 20:49:41]  I0423 12:49:41.935988    21 solver.cpp:330] Iteration 5500, Testing net (#0)
[2018-04-23 20:49:41]  I0423 12:49:41.937026    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:49:45]  I0423 12:49:45.836421    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.663862
[2018-04-23 20:49:45]  I0423 12:49:45.836964    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.41037 (* 1 = 1.41037 loss)
[2018-04-23 20:49:45]  I0423 12:49:45.903190    21 solver.cpp:218] Iteration 5500 (9.31221 iter/s, 10.7386s/100 iters), loss = 0.0174414
[2018-04-23 20:49:45]  I0423 12:49:45.903242    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:49:45]  I0423 12:49:45.903255    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0174415 (* 1 = 0.0174415 loss)
[2018-04-23 20:49:45]  I0423 12:49:45.903275    21 sgd_solver.cpp:105] Iteration 5500, lr = 0.001
[2018-04-23 20:49:52]  I0423 12:49:52.572839    21 solver.cpp:330] Iteration 5600, Testing net (#0)
[2018-04-23 20:49:52]  I0423 12:49:52.576005    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:49:56]  I0423 12:49:56.470676    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.722556
[2018-04-23 20:49:56]  I0423 12:49:56.470999    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.16288 (* 1 = 1.16288 loss)
[2018-04-23 20:49:56]  I0423 12:49:56.536823    21 solver.cpp:218] Iteration 5600 (9.40444 iter/s, 10.6333s/100 iters), loss = 0.0318117
[2018-04-23 20:49:56]  I0423 12:49:56.536867    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:49:56]  I0423 12:49:56.536957    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0318117 (* 1 = 0.0318117 loss)
[2018-04-23 20:49:56]  I0423 12:49:56.536991    21 sgd_solver.cpp:105] Iteration 5600, lr = 0.001
[2018-04-23 20:50:03]  I0423 12:50:03.196300    21 solver.cpp:330] Iteration 5700, Testing net (#0)
[2018-04-23 20:50:03]  I0423 12:50:03.197502    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:50:07]  I0423 12:50:07.093080    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.768429
[2018-04-23 20:50:07]  I0423 12:50:07.093226    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.976901 (* 1 = 0.976901 loss)
[2018-04-23 20:50:07]  I0423 12:50:07.159770    21 solver.cpp:218] Iteration 5700 (9.4139 iter/s, 10.6226s/100 iters), loss = 0.0177807
[2018-04-23 20:50:07]  I0423 12:50:07.159824    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:50:07]  I0423 12:50:07.159839    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0177807 (* 1 = 0.0177807 loss)
[2018-04-23 20:50:07]  I0423 12:50:07.159927    21 sgd_solver.cpp:105] Iteration 5700, lr = 0.001
[2018-04-23 20:50:13]  I0423 12:50:13.865764    21 solver.cpp:330] Iteration 5800, Testing net (#0)
[2018-04-23 20:50:13]  I0423 12:50:13.866621    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:50:17]  I0423 12:50:17.741166    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.79988
[2018-04-23 20:50:17]  I0423 12:50:17.741307    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.858266 (* 1 = 0.858266 loss)
[2018-04-23 20:50:17]  I0423 12:50:17.808213    21 solver.cpp:218] Iteration 5800 (9.39137 iter/s, 10.6481s/100 iters), loss = 0.0184509
[2018-04-23 20:50:17]  I0423 12:50:17.808255    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:50:17]  I0423 12:50:17.808269    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.018451 (* 1 = 0.018451 loss)
[2018-04-23 20:50:17]  I0423 12:50:17.808425    21 sgd_solver.cpp:105] Iteration 5800, lr = 0.001
[2018-04-23 20:50:24]  I0423 12:50:24.467964    21 solver.cpp:330] Iteration 5900, Testing net (#0)
[2018-04-23 20:50:24]  I0423 12:50:24.469099    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:50:24]  I0423 12:50:24.993618    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:50:28]  I0423 12:50:28.498180    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.825721
[2018-04-23 20:50:28]  I0423 12:50:28.498268    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.733847 (* 1 = 0.733847 loss)
[2018-04-23 20:50:28]  I0423 12:50:28.564476    21 solver.cpp:218] Iteration 5900 (9.29723 iter/s, 10.7559s/100 iters), loss = 0.0553012
[2018-04-23 20:50:28]  I0423 12:50:28.564532    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:50:28]  I0423 12:50:28.564544    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0553013 (* 1 = 0.0553013 loss)
[2018-04-23 20:50:28]  I0423 12:50:28.564617    21 sgd_solver.cpp:105] Iteration 5900, lr = 0.001
[2018-04-23 20:50:35]  I0423 12:50:35.306900    21 solver.cpp:447] Snapshotting to binary proto file /data/output/myres20__iter_6000.caffemodel
[2018-04-23 20:50:35]  I0423 12:50:35.479337    21 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /data/output/myres20__iter_6000.solverstate
[2018-04-23 20:50:35]  I0423 12:50:35.571638    21 solver.cpp:330] Iteration 6000, Testing net (#0)
[2018-04-23 20:50:35]  I0423 12:50:35.572108    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:50:39]  I0423 12:50:39.426784    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.850962
[2018-04-23 20:50:39]  I0423 12:50:39.427332    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.63803 (* 1 = 0.63803 loss)
[2018-04-23 20:50:39]  I0423 12:50:39.493293    21 solver.cpp:218] Iteration 6000 (9.15045 iter/s, 10.9284s/100 iters), loss = 0.0159279
[2018-04-23 20:50:39]  I0423 12:50:39.493335    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:50:39]  I0423 12:50:39.493362    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.015928 (* 1 = 0.015928 loss)
[2018-04-23 20:50:39]  I0423 12:50:39.493379    21 sgd_solver.cpp:105] Iteration 6000, lr = 0.001
[2018-04-23 20:50:46]  I0423 12:50:46.147500    21 solver.cpp:330] Iteration 6100, Testing net (#0)
[2018-04-23 20:50:46]  I0423 12:50:46.148298    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:50:49]  I0423 12:50:49.994546    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.858173
[2018-04-23 20:50:50]  I0423 12:50:49.994608    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.591962 (* 1 = 0.591962 loss)
[2018-04-23 20:50:50]  I0423 12:50:50.059701    21 solver.cpp:218] Iteration 6100 (9.46429 iter/s, 10.566s/100 iters), loss = 0.0503314
[2018-04-23 20:50:50]  I0423 12:50:50.059746    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:50:50]  I0423 12:50:50.059761    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0503315 (* 1 = 0.0503315 loss)
[2018-04-23 20:50:50]  I0423 12:50:50.059958    21 sgd_solver.cpp:105] Iteration 6100, lr = 0.001
[2018-04-23 20:50:56]  I0423 12:50:56.744105    21 solver.cpp:330] Iteration 6200, Testing net (#0)
[2018-04-23 20:50:56]  I0423 12:50:56.744596    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:51:00]  I0423 12:51:00.559723    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.8748
[2018-04-23 20:51:00]  I0423 12:51:00.559823    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.522186 (* 1 = 0.522186 loss)
[2018-04-23 20:51:00]  I0423 12:51:00.624256    21 solver.cpp:218] Iteration 6200 (9.46595 iter/s, 10.5642s/100 iters), loss = 0.0111876
[2018-04-23 20:51:00]  I0423 12:51:00.624301    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:51:00]  I0423 12:51:00.624330    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0111876 (* 1 = 0.0111876 loss)
[2018-04-23 20:51:00]  I0423 12:51:00.624357    21 sgd_solver.cpp:105] Iteration 6200, lr = 0.001
[2018-04-23 20:51:07]  I0423 12:51:07.290390    21 solver.cpp:330] Iteration 6300, Testing net (#0)
[2018-04-23 20:51:07]  I0423 12:51:07.290834    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:51:11]  I0423 12:51:11.092170    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.88121
[2018-04-23 20:51:11]  I0423 12:51:11.092244    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.495124 (* 1 = 0.495124 loss)
[2018-04-23 20:51:11]  I0423 12:51:11.157388    21 solver.cpp:218] Iteration 6300 (9.49402 iter/s, 10.5329s/100 iters), loss = 0.0118913
[2018-04-23 20:51:11]  I0423 12:51:11.157434    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:51:11]  I0423 12:51:11.157449    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0118913 (* 1 = 0.0118913 loss)
[2018-04-23 20:51:11]  I0423 12:51:11.157464    21 sgd_solver.cpp:105] Iteration 6300, lr = 0.001
[2018-04-23 20:51:17]  I0423 12:51:17.816200    21 solver.cpp:330] Iteration 6400, Testing net (#0)
[2018-04-23 20:51:17]  I0423 12:51:17.816684    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:51:21]  I0423 12:51:21.584691    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.886418
[2018-04-23 20:51:21]  I0423 12:51:21.584753    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.459763 (* 1 = 0.459763 loss)
[2018-04-23 20:51:21]  I0423 12:51:21.649668    21 solver.cpp:218] Iteration 6400 (9.53094 iter/s, 10.4921s/100 iters), loss = 0.0154278
[2018-04-23 20:51:21]  I0423 12:51:21.649713    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:51:21]  I0423 12:51:21.649729    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0154279 (* 1 = 0.0154279 loss)
[2018-04-23 20:51:21]  I0423 12:51:21.649749    21 sgd_solver.cpp:105] Iteration 6400, lr = 0.001
[2018-04-23 20:51:28]  I0423 12:51:28.257856    21 solver.cpp:330] Iteration 6500, Testing net (#0)
[2018-04-23 20:51:28]  I0423 12:51:28.258237    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:51:31]  I0423 12:51:31.097044    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:51:32]  I0423 12:51:32.202569    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.892027
[2018-04-23 20:51:32]  I0423 12:51:32.202636    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.434677 (* 1 = 0.434677 loss)
[2018-04-23 20:51:32]  I0423 12:51:32.268146    21 solver.cpp:218] Iteration 6500 (9.41768 iter/s, 10.6183s/100 iters), loss = 0.0341474
[2018-04-23 20:51:32]  I0423 12:51:32.268187    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:51:32]  I0423 12:51:32.268199    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0341475 (* 1 = 0.0341475 loss)
[2018-04-23 20:51:32]  I0423 12:51:32.268220    21 sgd_solver.cpp:105] Iteration 6500, lr = 0.001
[2018-04-23 20:51:38]  I0423 12:51:38.953485    21 solver.cpp:330] Iteration 6600, Testing net (#0)
[2018-04-23 20:51:38]  I0423 12:51:38.953846    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:51:42]  I0423 12:51:42.806180    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.902043
[2018-04-23 20:51:42]  I0423 12:51:42.806285    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.407372 (* 1 = 0.407372 loss)
[2018-04-23 20:51:42]  I0423 12:51:42.869985    21 solver.cpp:218] Iteration 6600 (9.43247 iter/s, 10.6017s/100 iters), loss = 0.0453981
[2018-04-23 20:51:42]  I0423 12:51:42.870031    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:51:42]  I0423 12:51:42.870045    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0453981 (* 1 = 0.0453981 loss)
[2018-04-23 20:51:42]  I0423 12:51:42.870069    21 sgd_solver.cpp:105] Iteration 6600, lr = 0.001
[2018-04-23 20:51:49]  I0423 12:51:49.560664    21 solver.cpp:330] Iteration 6700, Testing net (#0)
[2018-04-23 20:51:49]  I0423 12:51:49.562258    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:51:53]  I0423 12:51:53.595749    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.904247
[2018-04-23 20:51:53]  I0423 12:51:53.596725    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.389024 (* 1 = 0.389024 loss)
[2018-04-23 20:51:53]  I0423 12:51:53.664808    21 solver.cpp:218] Iteration 6700 (9.26386 iter/s, 10.7946s/100 iters), loss = 0.0544766
[2018-04-23 20:51:53]  I0423 12:51:53.664911    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:51:53]  I0423 12:51:53.665004    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0544766 (* 1 = 0.0544766 loss)
[2018-04-23 20:51:53]  I0423 12:51:53.665093    21 sgd_solver.cpp:105] Iteration 6700, lr = 0.001
[2018-04-23 20:52:00]  I0423 12:52:00.491031    21 solver.cpp:330] Iteration 6800, Testing net (#0)
[2018-04-23 20:52:00]  I0423 12:52:00.492915    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:52:04]  I0423 12:52:04.759815    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.90645
[2018-04-23 20:52:04]  I0423 12:52:04.760222    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.388571 (* 1 = 0.388571 loss)
[2018-04-23 20:52:04]  I0423 12:52:04.835139    21 solver.cpp:218] Iteration 6800 (8.95248 iter/s, 11.1701s/100 iters), loss = 0.0525351
[2018-04-23 20:52:04]  I0423 12:52:04.835181    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:52:04]  I0423 12:52:04.835199    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0525351 (* 1 = 0.0525351 loss)
[2018-04-23 20:52:04]  I0423 12:52:04.835391    21 sgd_solver.cpp:105] Iteration 6800, lr = 0.001
[2018-04-23 20:52:11]  I0423 12:52:11.659731    21 solver.cpp:330] Iteration 6900, Testing net (#0)
[2018-04-23 20:52:11]  I0423 12:52:11.664355    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:52:15]  I0423 12:52:15.801112    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.909655
[2018-04-23 20:52:15]  I0423 12:52:15.801286    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.363227 (* 1 = 0.363227 loss)
[2018-04-23 20:52:15]  I0423 12:52:15.867525    21 solver.cpp:218] Iteration 6900 (9.06439 iter/s, 11.0322s/100 iters), loss = 0.0589671
[2018-04-23 20:52:15]  I0423 12:52:15.867575    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:52:15]  I0423 12:52:15.867588    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0589671 (* 1 = 0.0589671 loss)
[2018-04-23 20:52:15]  I0423 12:52:15.867611    21 sgd_solver.cpp:105] Iteration 6900, lr = 0.001
[2018-04-23 20:52:22]  I0423 12:52:22.582227    21 solver.cpp:330] Iteration 7000, Testing net (#0)
[2018-04-23 20:52:22]  I0423 12:52:22.586434    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:52:26]  I0423 12:52:26.485527    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.91226
[2018-04-23 20:52:26]  I0423 12:52:26.485607    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.363623 (* 1 = 0.363623 loss)
[2018-04-23 20:52:26]  I0423 12:52:26.552359    21 solver.cpp:218] Iteration 7000 (9.35925 iter/s, 10.6846s/100 iters), loss = 0.0230539
[2018-04-23 20:52:26]  I0423 12:52:26.552410    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:52:26]  I0423 12:52:26.552428    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0230539 (* 1 = 0.0230539 loss)
[2018-04-23 20:52:26]  I0423 12:52:26.552443    21 sgd_solver.cpp:46] MultiStep Status: Iteration 7000, step = 3
[2018-04-23 20:52:26]  I0423 12:52:26.552470    21 sgd_solver.cpp:105] Iteration 7000, lr = 0.0001
[2018-04-23 20:52:33]  I0423 12:52:33.247064    21 solver.cpp:330] Iteration 7100, Testing net (#0)
[2018-04-23 20:52:33]  I0423 12:52:33.247915    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:52:37]  I0423 12:52:37.288904    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.916466
[2018-04-23 20:52:37]  I0423 12:52:37.289172    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.344522 (* 1 = 0.344522 loss)
[2018-04-23 20:52:37]  I0423 12:52:37.356597    21 solver.cpp:218] Iteration 7100 (9.25582 iter/s, 10.804s/100 iters), loss = 0.0297896
[2018-04-23 20:52:37]  I0423 12:52:37.356643    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:52:37]  I0423 12:52:37.356657    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0297896 (* 1 = 0.0297896 loss)
[2018-04-23 20:52:37]  I0423 12:52:37.356745    21 sgd_solver.cpp:105] Iteration 7100, lr = 0.0001
[2018-04-23 20:52:44]  I0423 12:52:44.219316    21 solver.cpp:330] Iteration 7200, Testing net (#0)
[2018-04-23 20:52:44]  I0423 12:52:44.223714    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:52:45]  I0423 12:52:45.362033    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:52:48]  I0423 12:52:48.408679    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.914864
[2018-04-23 20:52:48]  I0423 12:52:48.409466    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.35058 (* 1 = 0.35058 loss)
[2018-04-23 20:52:48]  I0423 12:52:48.479346    21 solver.cpp:218] Iteration 7200 (8.99078 iter/s, 11.1225s/100 iters), loss = 0.0203696
[2018-04-23 20:52:48]  I0423 12:52:48.479394    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:52:48]  I0423 12:52:48.479445    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0203696 (* 1 = 0.0203696 loss)
[2018-04-23 20:52:48]  I0423 12:52:48.479722    21 sgd_solver.cpp:105] Iteration 7200, lr = 0.0001
[2018-04-23 20:52:55]  I0423 12:52:55.284682    21 solver.cpp:330] Iteration 7300, Testing net (#0)
[2018-04-23 20:52:55]  I0423 12:52:55.287783    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:52:59]  I0423 12:52:59.381958    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.918069
[2018-04-23 20:52:59]  I0423 12:52:59.382567    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.335316 (* 1 = 0.335316 loss)
[2018-04-23 20:52:59]  I0423 12:52:59.448859    21 solver.cpp:218] Iteration 7300 (9.11638 iter/s, 10.9693s/100 iters), loss = 0.0299373
[2018-04-23 20:52:59]  I0423 12:52:59.448909    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:52:59]  I0423 12:52:59.448923    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0299373 (* 1 = 0.0299373 loss)
[2018-04-23 20:52:59]  I0423 12:52:59.448938    21 sgd_solver.cpp:105] Iteration 7300, lr = 0.0001
[2018-04-23 20:53:06]  I0423 12:53:06.182889    21 solver.cpp:330] Iteration 7400, Testing net (#0)
[2018-04-23 20:53:06]  I0423 12:53:06.185787    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:53:10]  I0423 12:53:10.257715    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.918269
[2018-04-23 20:53:10]  I0423 12:53:10.258428    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.337447 (* 1 = 0.337447 loss)
[2018-04-23 20:53:10]  I0423 12:53:10.330283    21 solver.cpp:218] Iteration 7400 (9.19019 iter/s, 10.8812s/100 iters), loss = 0.0248154
[2018-04-23 20:53:10]  I0423 12:53:10.330338    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:53:10]  I0423 12:53:10.330416    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0248154 (* 1 = 0.0248154 loss)
[2018-04-23 20:53:10]  I0423 12:53:10.330552    21 sgd_solver.cpp:105] Iteration 7400, lr = 0.0001
[2018-04-23 20:53:17]  I0423 12:53:17.134455    21 solver.cpp:330] Iteration 7500, Testing net (#0)
[2018-04-23 20:53:17]  I0423 12:53:17.138204    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:53:21]  I0423 12:53:21.374191    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.922877
[2018-04-23 20:53:21]  I0423 12:53:21.375177    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.321621 (* 1 = 0.321621 loss)
[2018-04-23 20:53:21]  I0423 12:53:21.447501    21 solver.cpp:218] Iteration 7500 (8.99512 iter/s, 11.1171s/100 iters), loss = 0.0219155
[2018-04-23 20:53:21]  I0423 12:53:21.447556    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:53:21]  I0423 12:53:21.447571    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0219155 (* 1 = 0.0219155 loss)
[2018-04-23 20:53:21]  I0423 12:53:21.447774    21 sgd_solver.cpp:105] Iteration 7500, lr = 0.0001
[2018-04-23 20:53:28]  I0423 12:53:28.227210    21 solver.cpp:330] Iteration 7600, Testing net (#0)
[2018-04-23 20:53:28]  I0423 12:53:28.231315    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:53:32]  I0423 12:53:32.381436    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.91847
[2018-04-23 20:53:32]  I0423 12:53:32.381567    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.329887 (* 1 = 0.329887 loss)
[2018-04-23 20:53:32]  I0423 12:53:32.449555    21 solver.cpp:218] Iteration 7600 (9.08928 iter/s, 11.002s/100 iters), loss = 0.0148177
[2018-04-23 20:53:32]  I0423 12:53:32.449610    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:53:32]  I0423 12:53:32.449663    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0148177 (* 1 = 0.0148177 loss)
[2018-04-23 20:53:32]  I0423 12:53:32.449776    21 sgd_solver.cpp:105] Iteration 7600, lr = 0.0001
[2018-04-23 20:53:39]  I0423 12:53:39.258177    21 solver.cpp:330] Iteration 7700, Testing net (#0)
[2018-04-23 20:53:39]  I0423 12:53:39.260076    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:53:43]  I0423 12:53:43.200398    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.921474
[2018-04-23 20:53:43]  I0423 12:53:43.201300    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.317907 (* 1 = 0.317907 loss)
[2018-04-23 20:53:43]  I0423 12:53:43.268610    21 solver.cpp:218] Iteration 7700 (9.24304 iter/s, 10.819s/100 iters), loss = 0.0213491
[2018-04-23 20:53:43]  I0423 12:53:43.268674    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:53:43]  I0423 12:53:43.268724    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0213491 (* 1 = 0.0213491 loss)
[2018-04-23 20:53:43]  I0423 12:53:43.268961    21 sgd_solver.cpp:105] Iteration 7700, lr = 0.0001
[2018-04-23 20:53:49]  I0423 12:53:49.967115    21 solver.cpp:330] Iteration 7800, Testing net (#0)
[2018-04-23 20:53:49]  I0423 12:53:49.968693    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:53:53]  I0423 12:53:53.152215    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:53:53]  I0423 12:53:53.840960    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.91847
[2018-04-23 20:53:53]  I0423 12:53:53.841116    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.329317 (* 1 = 0.329317 loss)
[2018-04-23 20:53:53]  I0423 12:53:53.908385    21 solver.cpp:218] Iteration 7800 (9.3988 iter/s, 10.6397s/100 iters), loss = 0.0205704
[2018-04-23 20:53:53]  I0423 12:53:53.908434    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:53:53]  I0423 12:53:53.908453    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0205704 (* 1 = 0.0205704 loss)
[2018-04-23 20:53:53]  I0423 12:53:53.908619    21 sgd_solver.cpp:105] Iteration 7800, lr = 0.0001
[2018-04-23 20:54:00]  I0423 12:54:00.657073    21 solver.cpp:330] Iteration 7900, Testing net (#0)
[2018-04-23 20:54:00]  I0423 12:54:00.658002    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:54:04]  I0423 12:54:04.709424    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.918269
[2018-04-23 20:54:04]  I0423 12:54:04.709959    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.32084 (* 1 = 0.32084 loss)
[2018-04-23 20:54:04]  I0423 12:54:04.775331    21 solver.cpp:218] Iteration 7900 (9.20224 iter/s, 10.8669s/100 iters), loss = 0.0282775
[2018-04-23 20:54:04]  I0423 12:54:04.775427    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:54:04]  I0423 12:54:04.775588    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0282775 (* 1 = 0.0282775 loss)
[2018-04-23 20:54:04]  I0423 12:54:04.775764    21 sgd_solver.cpp:105] Iteration 7900, lr = 0.0001
[2018-04-23 20:54:11]  I0423 12:54:11.439748    21 solver.cpp:447] Snapshotting to binary proto file /data/output/myres20__iter_8000.caffemodel
[2018-04-23 20:54:11]  I0423 12:54:11.631661    21 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /data/output/myres20__iter_8000.solverstate
[2018-04-23 20:54:11]  I0423 12:54:11.715983    21 solver.cpp:330] Iteration 8000, Testing net (#0)
[2018-04-23 20:54:11]  I0423 12:54:11.716563    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:54:15]  I0423 12:54:15.649205    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.922877
[2018-04-23 20:54:15]  I0423 12:54:15.649592    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.311829 (* 1 = 0.311829 loss)
[2018-04-23 20:54:15]  I0423 12:54:15.714126    21 solver.cpp:218] Iteration 8000 (9.14178 iter/s, 10.9388s/100 iters), loss = 0.0673509
[2018-04-23 20:54:15]  I0423 12:54:15.714231    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:54:15]  I0423 12:54:15.714355    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0673509 (* 1 = 0.0673509 loss)
[2018-04-23 20:54:15]  I0423 12:54:15.714427    21 sgd_solver.cpp:105] Iteration 8000, lr = 0.0001
[2018-04-23 20:54:22]  I0423 12:54:22.407830    21 solver.cpp:330] Iteration 8100, Testing net (#0)
[2018-04-23 20:54:22]  I0423 12:54:22.410200    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:54:26]  I0423 12:54:26.266688    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.919671
[2018-04-23 20:54:26]  I0423 12:54:26.267146    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.320901 (* 1 = 0.320901 loss)
[2018-04-23 20:54:26]  I0423 12:54:26.337381    21 solver.cpp:218] Iteration 8100 (9.41335 iter/s, 10.6232s/100 iters), loss = 0.0335546
[2018-04-23 20:54:26]  I0423 12:54:26.337600    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:54:26]  I0423 12:54:26.337683    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0335546 (* 1 = 0.0335546 loss)
[2018-04-23 20:54:26]  I0423 12:54:26.337762    21 sgd_solver.cpp:105] Iteration 8100, lr = 0.0001
[2018-04-23 20:54:33]  I0423 12:54:33.039932    21 solver.cpp:330] Iteration 8200, Testing net (#0)
[2018-04-23 20:54:33]  I0423 12:54:33.040591    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:54:36]  I0423 12:54:36.839718    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.920473
[2018-04-23 20:54:36]  I0423 12:54:36.840088    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.317832 (* 1 = 0.317832 loss)
[2018-04-23 20:54:36]  I0423 12:54:36.906785    21 solver.cpp:218] Iteration 8200 (9.46143 iter/s, 10.5692s/100 iters), loss = 0.0592213
[2018-04-23 20:54:36]  I0423 12:54:36.906982    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:54:36]  I0423 12:54:36.907147    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0592213 (* 1 = 0.0592213 loss)
[2018-04-23 20:54:37]  I0423 12:54:36.907330    21 sgd_solver.cpp:105] Iteration 8200, lr = 0.0001
[2018-04-23 20:54:43]  I0423 12:54:43.556005    21 solver.cpp:330] Iteration 8300, Testing net (#0)
[2018-04-23 20:54:43]  I0423 12:54:43.556452    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:54:47]  I0423 12:54:47.351341    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.924079
[2018-04-23 20:54:47]  I0423 12:54:47.351411    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.308598 (* 1 = 0.308598 loss)
[2018-04-23 20:54:47]  I0423 12:54:47.415580    21 solver.cpp:218] Iteration 8300 (9.51599 iter/s, 10.5086s/100 iters), loss = 0.049166
[2018-04-23 20:54:47]  I0423 12:54:47.415637    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:54:47]  I0423 12:54:47.415650    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0491659 (* 1 = 0.0491659 loss)
[2018-04-23 20:54:47]  I0423 12:54:47.415675    21 sgd_solver.cpp:105] Iteration 8300, lr = 0.0001
[2018-04-23 20:54:54]  I0423 12:54:54.165382    21 solver.cpp:330] Iteration 8400, Testing net (#0)
[2018-04-23 20:54:54]  I0423 12:54:54.166046    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:54:58]  I0423 12:54:58.167716    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.919872
[2018-04-23 20:54:58]  I0423 12:54:58.167826    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.31593 (* 1 = 0.31593 loss)
[2018-04-23 20:54:58]  I0423 12:54:58.232152    21 solver.cpp:218] Iteration 8400 (9.24512 iter/s, 10.8165s/100 iters), loss = 0.0433119
[2018-04-23 20:54:58]  I0423 12:54:58.232200    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:54:58]  I0423 12:54:58.232214    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0433119 (* 1 = 0.0433119 loss)
[2018-04-23 20:54:58]  I0423 12:54:58.232406    21 sgd_solver.cpp:105] Iteration 8400, lr = 0.0001
[2018-04-23 20:55:04]  I0423 12:55:04.923439    21 solver.cpp:330] Iteration 8500, Testing net (#0)
[2018-04-23 20:55:04]  I0423 12:55:04.923945    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:55:06]  I0423 12:55:06.502951    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:55:08]  I0423 12:55:08.845890    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.922276
[2018-04-23 20:55:08]  I0423 12:55:08.845988    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.314957 (* 1 = 0.314957 loss)
[2018-04-23 20:55:08]  I0423 12:55:08.911249    21 solver.cpp:218] Iteration 8500 (9.36414 iter/s, 10.679s/100 iters), loss = 0.031044
[2018-04-23 20:55:08]  I0423 12:55:08.911300    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:55:08]  I0423 12:55:08.911314    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.031044 (* 1 = 0.031044 loss)
[2018-04-23 20:55:08]  I0423 12:55:08.911334    21 sgd_solver.cpp:105] Iteration 8500, lr = 0.0001
[2018-04-23 20:55:15]  I0423 12:55:15.660770    21 solver.cpp:330] Iteration 8600, Testing net (#0)
[2018-04-23 20:55:15]  I0423 12:55:15.661178    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:55:19]  I0423 12:55:19.717656    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.919872
[2018-04-23 20:55:19]  I0423 12:55:19.717877    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.321533 (* 1 = 0.321533 loss)
[2018-04-23 20:55:19]  I0423 12:55:19.782634    21 solver.cpp:218] Iteration 8600 (9.19853 iter/s, 10.8713s/100 iters), loss = 0.06287
[2018-04-23 20:55:19]  I0423 12:55:19.782691    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:55:19]  I0423 12:55:19.782709    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.06287 (* 1 = 0.06287 loss)
[2018-04-23 20:55:19]  I0423 12:55:19.782729    21 sgd_solver.cpp:105] Iteration 8600, lr = 0.0001
[2018-04-23 20:55:26]  I0423 12:55:26.485922    21 solver.cpp:330] Iteration 8700, Testing net (#0)
[2018-04-23 20:55:26]  I0423 12:55:26.486649    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:55:30]  I0423 12:55:30.406458    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.923878
[2018-04-23 20:55:30]  I0423 12:55:30.406584    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.299543 (* 1 = 0.299543 loss)
[2018-04-23 20:55:30]  I0423 12:55:30.471658    21 solver.cpp:218] Iteration 8700 (9.35547 iter/s, 10.6889s/100 iters), loss = 0.0222893
[2018-04-23 20:55:30]  I0423 12:55:30.471704    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:55:30]  I0423 12:55:30.471719    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0222893 (* 1 = 0.0222893 loss)
[2018-04-23 20:55:30]  I0423 12:55:30.471736    21 sgd_solver.cpp:105] Iteration 8700, lr = 0.0001
[2018-04-23 20:55:37]  I0423 12:55:37.265544    21 solver.cpp:330] Iteration 8800, Testing net (#0)
[2018-04-23 20:55:37]  I0423 12:55:37.269404    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:55:41]  I0423 12:55:41.223678    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.922075
[2018-04-23 20:55:41]  I0423 12:55:41.223940    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.31606 (* 1 = 0.31606 loss)
[2018-04-23 20:55:41]  I0423 12:55:41.291931    21 solver.cpp:218] Iteration 8800 (9.24199 iter/s, 10.8202s/100 iters), loss = 0.114569
[2018-04-23 20:55:41]  I0423 12:55:41.291976    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.96875
[2018-04-23 20:55:41]  I0423 12:55:41.291990    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.114569 (* 1 = 0.114569 loss)
[2018-04-23 20:55:41]  I0423 12:55:41.292062    21 sgd_solver.cpp:105] Iteration 8800, lr = 0.0001
[2018-04-23 20:55:48]  I0423 12:55:48.211227    21 solver.cpp:330] Iteration 8900, Testing net (#0)
[2018-04-23 20:55:48]  I0423 12:55:48.212507    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:55:52]  I0423 12:55:52.446609    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.921474
[2018-04-23 20:55:52]  I0423 12:55:52.446908    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.313029 (* 1 = 0.313029 loss)
[2018-04-23 20:55:52]  I0423 12:55:52.522552    21 solver.cpp:218] Iteration 8900 (8.9043 iter/s, 11.2305s/100 iters), loss = 0.0482741
[2018-04-23 20:55:52]  I0423 12:55:52.522605    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:55:52]  I0423 12:55:52.522656    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0482741 (* 1 = 0.0482741 loss)
[2018-04-23 20:55:52]  I0423 12:55:52.522860    21 sgd_solver.cpp:105] Iteration 8900, lr = 0.0001
[2018-04-23 20:55:59]  I0423 12:55:59.488494    21 solver.cpp:330] Iteration 9000, Testing net (#0)
[2018-04-23 20:55:59]  I0423 12:55:59.491585    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:56:03]  I0423 12:56:03.565945    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.922676
[2018-04-23 20:56:03]  I0423 12:56:03.566224    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.32086 (* 1 = 0.32086 loss)
[2018-04-23 20:56:03]  I0423 12:56:03.637778    21 solver.cpp:218] Iteration 9000 (8.99662 iter/s, 11.1153s/100 iters), loss = 0.0217313
[2018-04-23 20:56:03]  I0423 12:56:03.637825    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:56:03]  I0423 12:56:03.637874    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0217313 (* 1 = 0.0217313 loss)
[2018-04-23 20:56:03]  I0423 12:56:03.637939    21 sgd_solver.cpp:105] Iteration 9000, lr = 0.0001
[2018-04-23 20:56:10]  I0423 12:56:10.636147    21 solver.cpp:330] Iteration 9100, Testing net (#0)
[2018-04-23 20:56:10]  I0423 12:56:10.637903    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:56:14]  I0423 12:56:14.577077    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:56:14]  I0423 12:56:14.786461    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.921875
[2018-04-23 20:56:14]  I0423 12:56:14.786653    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.30875 (* 1 = 0.30875 loss)
[2018-04-23 20:56:14]  I0423 12:56:14.862402    21 solver.cpp:218] Iteration 9100 (8.90901 iter/s, 11.2246s/100 iters), loss = 0.0357971
[2018-04-23 20:56:14]  I0423 12:56:14.862444    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:56:14]  I0423 12:56:14.862458    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0357971 (* 1 = 0.0357971 loss)
[2018-04-23 20:56:14]  I0423 12:56:14.862534    21 sgd_solver.cpp:105] Iteration 9100, lr = 0.0001
[2018-04-23 20:56:21]  I0423 12:56:21.870537    21 solver.cpp:330] Iteration 9200, Testing net (#0)
[2018-04-23 20:56:21]  I0423 12:56:21.871099    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:56:26]  I0423 12:56:26.117545    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.92488
[2018-04-23 20:56:26]  I0423 12:56:26.118122    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.30937 (* 1 = 0.30937 loss)
[2018-04-23 20:56:26]  I0423 12:56:26.186939    21 solver.cpp:218] Iteration 9200 (8.83046 iter/s, 11.3244s/100 iters), loss = 0.0579442
[2018-04-23 20:56:26]  I0423 12:56:26.186992    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:56:26]  I0423 12:56:26.187012    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0579443 (* 1 = 0.0579443 loss)
[2018-04-23 20:56:26]  I0423 12:56:26.187027    21 sgd_solver.cpp:105] Iteration 9200, lr = 0.0001
[2018-04-23 20:56:33]  I0423 12:56:33.111219    21 solver.cpp:330] Iteration 9300, Testing net (#0)
[2018-04-23 20:56:33]  I0423 12:56:33.111791    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:56:37]  I0423 12:56:37.002548    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.921074
[2018-04-23 20:56:37]  I0423 12:56:37.002813    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.31796 (* 1 = 0.31796 loss)
[2018-04-23 20:56:37]  I0423 12:56:37.068819    21 solver.cpp:218] Iteration 9300 (9.18968 iter/s, 10.8818s/100 iters), loss = 0.0313267
[2018-04-23 20:56:37]  I0423 12:56:37.068876    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:56:37]  I0423 12:56:37.068889    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0313267 (* 1 = 0.0313267 loss)
[2018-04-23 20:56:37]  I0423 12:56:37.068909    21 sgd_solver.cpp:105] Iteration 9300, lr = 0.0001
[2018-04-23 20:56:43]  I0423 12:56:43.876698    21 solver.cpp:330] Iteration 9400, Testing net (#0)
[2018-04-23 20:56:43]  I0423 12:56:43.877068    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:56:47]  I0423 12:56:47.885917    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.922276
[2018-04-23 20:56:47]  I0423 12:56:47.886087    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.314826 (* 1 = 0.314826 loss)
[2018-04-23 20:56:47]  I0423 12:56:47.956354    21 solver.cpp:218] Iteration 9400 (9.18491 iter/s, 10.8874s/100 iters), loss = 0.0151765
[2018-04-23 20:56:47]  I0423 12:56:47.956413    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:56:47]  I0423 12:56:47.956430    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0151765 (* 1 = 0.0151765 loss)
[2018-04-23 20:56:48]  I0423 12:56:47.956472    21 sgd_solver.cpp:105] Iteration 9400, lr = 0.0001
[2018-04-23 20:56:54]  I0423 12:56:54.635466    21 solver.cpp:330] Iteration 9500, Testing net (#0)
[2018-04-23 20:56:54]  I0423 12:56:54.636023    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:56:58]  I0423 12:56:58.690815    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.920673
[2018-04-23 20:56:58]  I0423 12:56:58.690913    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.31278 (* 1 = 0.31278 loss)
[2018-04-23 20:56:58]  I0423 12:56:58.756784    21 solver.cpp:218] Iteration 9500 (9.25901 iter/s, 10.8003s/100 iters), loss = 0.0197381
[2018-04-23 20:56:58]  I0423 12:56:58.756826    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:56:58]  I0423 12:56:58.756839    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0197382 (* 1 = 0.0197382 loss)
[2018-04-23 20:56:58]  I0423 12:56:58.756872    21 sgd_solver.cpp:105] Iteration 9500, lr = 0.0001
[2018-04-23 20:57:05]  I0423 12:57:05.471864    21 solver.cpp:330] Iteration 9600, Testing net (#0)
[2018-04-23 20:57:05]  I0423 12:57:05.472780    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:57:09]  I0423 12:57:09.579167    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.925881
[2018-04-23 20:57:09]  I0423 12:57:09.580058    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.291425 (* 1 = 0.291425 loss)
[2018-04-23 20:57:09]  I0423 12:57:09.645977    21 solver.cpp:218] Iteration 9600 (9.18354 iter/s, 10.8891s/100 iters), loss = 0.0163317
[2018-04-23 20:57:09]  I0423 12:57:09.646025    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:57:09]  I0423 12:57:09.646087    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0163318 (* 1 = 0.0163318 loss)
[2018-04-23 20:57:09]  I0423 12:57:09.646103    21 sgd_solver.cpp:105] Iteration 9600, lr = 0.0001
[2018-04-23 20:57:16]  I0423 12:57:16.322069    21 solver.cpp:330] Iteration 9700, Testing net (#0)
[2018-04-23 20:57:16]  I0423 12:57:16.322434    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:57:20]  I0423 12:57:20.302397    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.919671
[2018-04-23 20:57:20]  I0423 12:57:20.303321    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.33136 (* 1 = 0.33136 loss)
[2018-04-23 20:57:20]  I0423 12:57:20.368624    21 solver.cpp:218] Iteration 9700 (9.32619 iter/s, 10.7225s/100 iters), loss = 0.0545618
[2018-04-23 20:57:20]  I0423 12:57:20.368734    21 solver.cpp:237]     Train net output #0: Accuracy_train = 0.984375
[2018-04-23 20:57:20]  I0423 12:57:20.368903    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0545619 (* 1 = 0.0545619 loss)
[2018-04-23 20:57:20]  I0423 12:57:20.369561    21 sgd_solver.cpp:105] Iteration 9700, lr = 0.0001
[2018-04-23 20:57:27]  I0423 12:57:27.019389    21 solver.cpp:330] Iteration 9800, Testing net (#0)
[2018-04-23 20:57:27]  I0423 12:57:27.020447    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:57:29]  I0423 12:57:29.066524    21 blocking_queue.cpp:49] Waiting for data
[2018-04-23 20:57:31]  I0423 12:57:31.068538    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.921474
[2018-04-23 20:57:31]  I0423 12:57:31.068835    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.307957 (* 1 = 0.307957 loss)
[2018-04-23 20:57:31]  I0423 12:57:31.133075    21 solver.cpp:218] Iteration 9800 (9.29005 iter/s, 10.7642s/100 iters), loss = 0.0266265
[2018-04-23 20:57:31]  I0423 12:57:31.133209    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:57:31]  I0423 12:57:31.133258    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0266265 (* 1 = 0.0266265 loss)
[2018-04-23 20:57:31]  I0423 12:57:31.133365    21 sgd_solver.cpp:105] Iteration 9800, lr = 0.0001
[2018-04-23 20:57:37]  I0423 12:57:37.831249    21 solver.cpp:330] Iteration 9900, Testing net (#0)
[2018-04-23 20:57:37]  I0423 12:57:37.831890    21 net.cpp:676] Ignoring source layer Accuracy_train
[2018-04-23 20:57:41]  I0423 12:57:41.810487    21 solver.cpp:397]     Test net output #0: Accuracy_test = 0.921274
[2018-04-23 20:57:41]  I0423 12:57:41.810829    21 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.315136 (* 1 = 0.315136 loss)
[2018-04-23 20:57:41]  I0423 12:57:41.878080    21 solver.cpp:218] Iteration 9900 (9.30688 iter/s, 10.7447s/100 iters), loss = 0.0342061
[2018-04-23 20:57:41]  I0423 12:57:41.878123    21 solver.cpp:237]     Train net output #0: Accuracy_train = 1
[2018-04-23 20:57:41]  I0423 12:57:41.878135    21 solver.cpp:237]     Train net output #1: SoftmaxWithLoss1 = 0.0342062 (* 1 = 0.0342062 loss)
[2018-04-23 20:57:41]  I0423 12:57:41.878161    21 sgd_solver.cpp:105] Iteration 9900, lr = 0.0001
[2018-04-23 20:57:48]  I0423 12:57:48.635789    21 solver.cpp:447] Snapshotting to binary proto file /data/output/myres20__iter_10000.caffemodel
[2018-04-23 20:57:48]  I0423 12:57:48.843767    21 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /data/output/myres20__iter_10000.solverstate
[2018-04-23 20:57:50]  upload files to caffe_tinymind/
